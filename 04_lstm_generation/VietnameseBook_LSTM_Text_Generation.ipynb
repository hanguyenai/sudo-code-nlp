{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd1076a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pyvi) (1.7.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (3.5.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tqdm>=2.0->sklearn-crfsuite->pyvi) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021df6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset copied to: 04_lstm_generation/10000-vietnamese-books\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import kagglehub\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"iambestfeeder/10000-vietnamese-books\")\n",
    "\n",
    "# Move or copy to /content\n",
    "target_dir = \"04_lstm_generation/10000-vietnamese-books\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "shutil.copytree(path, target_dir, dirs_exist_ok=True)\n",
    "\n",
    "print(\"✅ Dataset copied to:\", target_dir)\n",
    "!ls -lh $target_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import regex\n",
    "import pandas as pd\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns\n",
    "RE_EMAIL = re.compile(r\"([\\w0-9_\\.-]+)(@)([\\d\\w\\.-]+)(\\.)([\\w\\.]{2,6})\")\n",
    "RE_URL = re.compile(r\"https?:\\/\\/(?!.*:\\/\\/)\\S+\")\n",
    "RE_PHONE = re.compile(r\"(09|01[2|6|8|9])+([0-9]{8})\\b\")\n",
    "RE_MENTION = re.compile(r\"@.+?:\")\n",
    "RE_NUMBER = re.compile(r\"\\d+\\.?\\d*\")\n",
    "RE_DATETIME = '\\d{1,2}\\s?[/-]\\s?\\d{1,2}\\s?[/-]\\s?\\d{4}'\n",
    "RE_HTML_TAG = re.compile(r'<[^>]+>')\n",
    "RE_CLEAR_1 = regex.compile(r\"[^_<>\\s\\p{Latin}]\")  # requires 'regex' library\n",
    "RE_CLEAR_2 = re.compile(r\"__+\")\n",
    "RE_CLEAR_3 = re.compile(r\"\\s+\")\n",
    "\n",
    "KEEP_DIACRITICS = True # Keep Vietnamese diacritics when lower-casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f95d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_common_token(txt):\n",
    "  txt = re.sub(RE_EMAIL, ' ', txt)\n",
    "  txt = re.sub(RE_URL, ' ', txt)\n",
    "  txt = re.sub(RE_MENTION, ' ', txt)\n",
    "  txt = re.sub(RE_DATETIME, ' ', txt)\n",
    "  txt = re.sub(RE_NUMBER, ' ', txt)\n",
    "  return txt\n",
    "\n",
    "def remove_emoji(txt):\n",
    "    txt = re.sub(r':v', '', txt)\n",
    "    txt = re.sub(r':D', '', txt)\n",
    "    txt = re.sub(r':3', '', txt)\n",
    "    txt = re.sub(r':\\(', '', txt)\n",
    "    txt = re.sub(r':\\)', '', txt)\n",
    "    txt = re.sub(r'=\\)*', '', txt)\n",
    "    return txt\n",
    "\n",
    "def remove_html_tag(txt):\n",
    "    return re.sub(RE_HTML_TAG, ' ', txt)\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = (\n",
    "        'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|'\n",
    "        'è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|'\n",
    "        'ì|í|ỉ|ĩ|ị|'\n",
    "        'ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|'\n",
    "        'ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|'\n",
    "        'ỳ|ý|ỷ|ỹ|ỵ|'\n",
    "        'À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|'\n",
    "        'È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|'\n",
    "        'Ì|Í|Ỉ|Ĩ|Ị|'\n",
    "        'Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|'\n",
    "        'Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|'\n",
    "        'Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "    ).split('|')\n",
    "\n",
    "    charutf8 = (\n",
    "        'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|'\n",
    "        'è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|'\n",
    "        'ì|í|ỉ|ĩ|ị|'\n",
    "        'ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|'\n",
    "        'ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|'\n",
    "        'ỳ|ý|ỷ|ỹ|ỵ|'\n",
    "        'À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|'\n",
    "        'È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|'\n",
    "        'Ì|Í|Ỉ|Ĩ|Ị|'\n",
    "        'Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|'\n",
    "        'Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|'\n",
    "        'Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "    ).split('|')\n",
    "\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "def convertwindown1525toutf8(txt):\n",
    "    \"\"\"\n",
    "    Convert Vietnamese text from Windows-1252/VNI encoding errors\n",
    "    to proper UTF-8 Unicode.\n",
    "    \"\"\"\n",
    "    return re.sub(\n",
    "        '|'.join(dicchar.keys()),  # regex khớp toàn bộ các ký tự lỗi\n",
    "        lambda x: dicchar[x.group()],\n",
    "        txt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "        \n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    \"\"\"\n",
    "    Normalize tone marks in a single Vietnamese word.\n",
    "    Ensures tone marks are placed on the correct vowel\n",
    "    according to Vietnamese orthography.\n",
    "    Example: 'hoà' -> 'hoà', 'thỏai' -> 'thoải', 'úy' -> 'úy'\n",
    "    \"\"\"\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0              # tone mark type (sắc, huyền, hỏi, ngã, nặng)\n",
    "    nguyen_am_index = []     # indexes of vowels in the word\n",
    "    qu_or_gi = False         # special case flag for \"qu\" and \"gi\"\n",
    "\n",
    "    # Scan each character\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "\n",
    "        # Special case \"qu\": treat 'u' after 'q' as a consonant\n",
    "        if x == 9 and index > 0 and chars[index - 1] == 'q':\n",
    "            chars[index] = 'u'\n",
    "            qu_or_gi = True\n",
    "\n",
    "        # Special case \"gi\": treat 'i' after 'g' as a consonant\n",
    "        elif x == 5 and index > 0 and chars[index - 1] == 'g':\n",
    "            chars[index] = 'i'\n",
    "            qu_or_gi = True\n",
    "\n",
    "        # If this vowel has a tone mark → record it\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]  # reset to base vowel\n",
    "\n",
    "        # Record vowel index unless it's part of \"qu\"/\"gi\"\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "\n",
    "    # Case: no valid vowels\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                # \"gi\", \"qu\" cases with 2 letters\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                # longer word with \"gi\"/\"qu\"\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    # fallback: put tone on i or u\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    # Case: if the word contains ê or ơ → always put the tone there\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    # Case: word has 2 vowels\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            # If last vowel is at the end → tone goes to the first vowel\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "        else:\n",
    "            # Otherwise → tone goes to the second vowel\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "\n",
    "    # Case: word has 3 vowels (e.g., \"khuyên\")\n",
    "    else:\n",
    "        # Place tone on the middle vowel\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        words[index] = chuan_hoa_dau_tu_tieng_viet(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e21b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt, tokenize=True):\n",
    "    # 1. Remove HTML tags\n",
    "    txt = remove_html_tag(txt)\n",
    "\n",
    "    # 2. Remove HTML entities like &nbsp; &amp; ...\n",
    "    txt = re.sub('&.{3,4};', ' ', txt)\n",
    "\n",
    "    # 3. Tokenization (Vietnamese word segmentation)\n",
    "    if tokenize:\n",
    "        txt = ViTokenizer.tokenize(txt)\n",
    "\n",
    "    # 4. Lowercase\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # 5. Remove common tokens (emails, URLs, mentions, dates, numbers, phones)\n",
    "    txt = replace_common_token(txt)\n",
    "\n",
    "    # 6. Remove emojis and emoticons\n",
    "    txt = remove_emoji(txt)\n",
    "\n",
    "    # 7. Remove unwanted characters\n",
    "    txt = RE_CLEAR_1.sub(\" \", txt)  # non-Latin chars\n",
    "    txt = RE_CLEAR_2.sub(\" \", txt)  # collapse multiple underscores\n",
    "    txt = RE_CLEAR_3.sub(\" \", txt)  # collapse multiple spaces\n",
    "\n",
    "    # 8. Strip leading/trailing whitespace\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7dbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "data_path = \"/content/10000-vietnamese-books/output\"\n",
    "data = []\n",
    "\n",
    "# Wrap file iteration with tqdm\n",
    "for file_name in tqdm(os.listdir(data_path), desc=\"Reading files\"):\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Add inner tqdm for large files (optional)\n",
    "        for line in f:\n",
    "            sentences = line.strip().split(\"\\n\")\n",
    "            for sentence in sentences:\n",
    "                data.append(preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f14aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
