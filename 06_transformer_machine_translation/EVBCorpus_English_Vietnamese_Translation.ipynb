{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanguyenai/sudo-code-nlp/blob/main/06_transformer_machine_translation/EVBCorpus_English_Vietnamese_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OoHhVOe0lQ8"
      },
      "source": [
        "# 1 Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DtVc1jn0yRG",
        "outputId": "ff9ff55e-c0d4-49b8-a0b4-5c4b4806eb5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting torchtext\n",
            "  Using cached torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting rarfile\n",
            "  Using cached rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Using cached portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.3)\n",
            "Using cached sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "Using cached torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "Using cached rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Using cached portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: rarfile, portalocker, colorama, sacrebleu, torchtext\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 rarfile-4.2 sacrebleu-2.5.1 torchtext-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu sentencepiece torchtext rarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT1sRK5P0Ayy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "import rarfile\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsor3--k10Ks",
        "outputId": "c7f2f9a1-074c-49a2-903a-76565d431569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Special tokens\n",
        "PAD = '<pad>'\n",
        "UNK = '<unk>'\n",
        "BOS = '<sos>'\n",
        "EOS = '<eos>'\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wFwRgYABDbh",
        "outputId": "0e053d70-f288-4972-830e-9e1b5a8f56ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EVBCorpus'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Total 35 (delta 0), reused 0 (delta 0), pack-reused 35 (from 1)\u001b[K\n",
            "Receiving objects: 100% (35/35), 35.37 MiB | 45.85 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/qhungngo/EVBCorpus.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClVlsL2L6Ld8"
      },
      "outputs": [],
      "source": [
        "def parse_sgml_file(sgml_path):\n",
        "    \"\"\"Parse a single SGML file and extract English-Vietnamese sentence pairs\"\"\"\n",
        "    try:\n",
        "        # Read file content\n",
        "        with open(sgml_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Parse with BeautifulSoup (handles SGML/XML-like formats)\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        pairs = []\n",
        "\n",
        "        # Find all sentence pairs\n",
        "        for spair in soup.find_all('spair'):\n",
        "            en_text = \"\"\n",
        "            vi_text = \"\"\n",
        "\n",
        "            # Get all <s> tags in this spair\n",
        "            s_tags = spair.find_all('s')\n",
        "\n",
        "            for s in s_tags:\n",
        "                s_id = s.get('id', '')\n",
        "                text = s.get_text(strip=True)\n",
        "\n",
        "                # Identify English vs Vietnamese by id prefix\n",
        "                if s_id.startswith('en'):\n",
        "                    en_text = text\n",
        "                elif s_id.startswith('vn'):\n",
        "                    vi_text = text\n",
        "\n",
        "            # Only add if both sentences exist and are non-empty\n",
        "            if en_text and vi_text:\n",
        "                pairs.append({\n",
        "                    'english': en_text,\n",
        "                    'vietnamese': vi_text\n",
        "                })\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Error parsing {os.path.basename(sgml_path)}: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cceqdx78KvPv"
      },
      "outputs": [],
      "source": [
        "def load_evbcorpus_to_dataframe(extract_dir):\n",
        "    \"\"\"Load all SGML files from EVBCorpus and create pandas DataFrame\"\"\"\n",
        "    print(\"\\n📖 Parsing SGML files...\")\n",
        "\n",
        "    # Find all SGML/XML files\n",
        "    sgml_files = []\n",
        "    for root, dirs, files in os.walk(extract_dir):\n",
        "        for f in files:\n",
        "            if f.endswith('.xml') or f.endswith('.sgml'):\n",
        "                sgml_files.append(os.path.join(root, f))\n",
        "\n",
        "    if not sgml_files:\n",
        "        print(\"⚠️  No SGML/XML files found!\")\n",
        "        print(f\"Searched in: {extract_dir}\")\n",
        "        print(\"\\nTrying to find all files...\")\n",
        "        all_files = []\n",
        "        for root, dirs, files in os.walk(extract_dir):\n",
        "            all_files.extend([os.path.join(root, f) for f in files[:5]])\n",
        "        print(f\"Found files: {all_files[:10]}\")\n",
        "        raise FileNotFoundError(\"No SGML/XML files found in extracted directory\")\n",
        "\n",
        "    print(f\"Found {len(sgml_files)} SGML/XML files\")\n",
        "\n",
        "    # Parse all SGML files\n",
        "    all_pairs = []\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    for sgml_file in tqdm(sgml_files, desc=\"Parsing SGML\"):\n",
        "        pairs = parse_sgml_file(sgml_file)\n",
        "        all_pairs.extend(pairs)\n",
        "\n",
        "    if not all_pairs:\n",
        "        raise ValueError(\"No sentence pairs found! Check SGML file format.\")\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(all_pairs)\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    print(f\"\\n✓ Loaded {len(df):,} sentence pairs\")\n",
        "    print(f\"  Unique pairs: {len(df):,}\")\n",
        "    print(f\"\\nDataFrame Info:\")\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Show statistics\n",
        "    print(f\"\\nSentence Length Statistics:\")\n",
        "    df['en_words'] = df['english'].str.split().str.len()\n",
        "    df['vi_words'] = df['vietnamese'].str.split().str.len()\n",
        "    print(f\"  English words: mean={df['en_words'].mean():.1f}, max={df['en_words'].max()}\")\n",
        "    print(f\"  Vietnamese words: mean={df['vi_words'].mean():.1f}, max={df['vi_words'].max()}\")\n",
        "\n",
        "    # Drop temporary columns\n",
        "    df = df.drop(['en_words', 'vi_words'], axis=1)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBiVuqFKLvSF"
      },
      "outputs": [],
      "source": [
        "rar_path = \"/content/EVBCorpus/EVBCorpus_EVBNews_v2.0.rar\"\n",
        "out_dir = \"/content/evbcorpus_data/EVBCorpus_v2\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "with rarfile.RarFile(rar_path) as rf:\n",
        "    # rf.printdir()  # xem list file\n",
        "    rf.extractall(path=out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq8HgnFIKzC4"
      },
      "outputs": [],
      "source": [
        "# Download and parse corpus\n",
        "print(\"=\" * 80)\n",
        "print(\"EVBCorpus v2.0 - SGML Parser\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "extract_dir = out_dir\n",
        "corpus_df = load_evbcorpus_to_dataframe(extract_dir)\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\n📊 Sample data from DataFrame:\")\n",
        "print(\"=\" * 80)\n",
        "display_df = corpus_df.head(5).copy()\n",
        "display_df['english'] = display_df['english'].str[:60] + '...'\n",
        "display_df['vietnamese'] = display_df['vietnamese'].str[:60] + '...'\n",
        "print(display_df.to_string(index=True))\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQbOADRhNJ29"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(w, max_length=None):\n",
        "    \"\"\"Preprocess a sentence (TensorFlow/Keras style)\"\"\"\n",
        "    w = w.lower().strip()\n",
        "\n",
        "    # Add space around punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # Replace multiple spaces with single space\n",
        "    w = re.sub(r'\\s+', ' ', w)\n",
        "    w = w.strip()\n",
        "\n",
        "    # Truncate to max_length if specified\n",
        "    if max_length:\n",
        "        w = \" \".join(w.split()[:max_length])\n",
        "\n",
        "    # Add start and end tokens\n",
        "    w = '{} {} {}'.format(BOS, w, EOS)\n",
        "\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMA8TOumNLQh"
      },
      "outputs": [],
      "source": [
        "def display_samples(inp_lines, targ_lines, num_of_pairs=5):\n",
        "    \"\"\"Display sample data pairs\"\"\"\n",
        "    pairs = list(zip(inp_lines[:num_of_pairs], targ_lines[:num_of_pairs]))\n",
        "\n",
        "    print('=' * 70)\n",
        "    print('SAMPLE DATA')\n",
        "    print('=' * 70)\n",
        "\n",
        "    for i, (inp, targ) in enumerate(pairs):\n",
        "        print(f'\\n--> Sample {i + 1}:')\n",
        "        print(f'    Input:  {inp}')\n",
        "        print(f'    Target: {targ}')\n",
        "\n",
        "    print('\\n' + '=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkHVqFTuNOmO"
      },
      "outputs": [],
      "source": [
        "def load_data_from_dataframe(df, max_samples=None, max_length=100):\n",
        "    \"\"\"Load and preprocess data from pandas DataFrame\"\"\"\n",
        "    print(f\"\\n📖 Loading data from DataFrame...\")\n",
        "    print(f\"  Total pairs: {len(df):,}\")\n",
        "\n",
        "    # Get English and Vietnamese sentences\n",
        "    en_sentences = df['english'].tolist()\n",
        "    vi_sentences = df['vietnamese'].tolist()\n",
        "\n",
        "    print(f\"\\n🧹 Preprocessing sentences...\")\n",
        "    en_preprocessed = []\n",
        "    vi_preprocessed = []\n",
        "\n",
        "    for en, vi in tqdm(zip(en_sentences, vi_sentences), total=len(en_sentences), desc=\"Processing\"):\n",
        "        en_prep = preprocess_sentence(en, max_length=max_length)\n",
        "        vi_prep = preprocess_sentence(vi, max_length=max_length)\n",
        "\n",
        "        # Filter out empty sentences\n",
        "        if len(en_prep.split()) > 2 and len(vi_prep.split()) > 2:  # More than BOS+EOS\n",
        "            en_preprocessed.append(en_prep)\n",
        "            vi_preprocessed.append(vi_prep)\n",
        "\n",
        "    print(f\"✓ After preprocessing: {len(en_preprocessed):,} sentences\")\n",
        "\n",
        "    if max_samples:\n",
        "        en_preprocessed = en_preprocessed[:max_samples]\n",
        "        vi_preprocessed = vi_preprocessed[:max_samples]\n",
        "        print(f\"✓ Using {max_samples:,} samples\")\n",
        "\n",
        "    # Display samples\n",
        "    display_samples(en_preprocessed, vi_preprocessed, num_of_pairs=3)\n",
        "\n",
        "    return en_preprocessed, vi_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1lEeM9X6v30"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    \"\"\"Tokenizer similar to TensorFlow/Keras Tokenizer\"\"\"\n",
        "    def __init__(self, num_words=None, oov_token='<unk>'):\n",
        "        self.num_words = num_words\n",
        "        self.oov_token = oov_token\n",
        "\n",
        "        self.word_index = {}  # word -> index mapping\n",
        "        self.index_word = {}  # index -> word mapping\n",
        "        self.word_counts = Counter()\n",
        "\n",
        "        # Initialize with special tokens\n",
        "        self.word_index = {\n",
        "            PAD: 0,\n",
        "            BOS: 1,\n",
        "            EOS: 2,\n",
        "            UNK: 3\n",
        "        }\n",
        "        self.index_word = {v: k for k, v in self.word_index.items()}\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        print(f\"🔧 Building vocabulary...\")\n",
        "\n",
        "        # Count words\n",
        "        for text in tqdm(texts, desc=\"Counting words\"):\n",
        "            words = text.split()\n",
        "            self.word_counts.update(words)\n",
        "\n",
        "        # Build word_index based on frequency\n",
        "        sorted_words = sorted(self.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Start from index 4 (after special tokens)\n",
        "        idx = 4\n",
        "        for word, count in sorted_words:\n",
        "            if word not in self.word_index:\n",
        "                if self.num_words is None or idx < self.num_words:\n",
        "                    self.word_index[word] = idx\n",
        "                    self.index_word[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "        vocab_size = len(self.word_index)\n",
        "        print(f\"✓ Vocabulary size: {vocab_size:,}\")\n",
        "        print(f\"  Total unique words: {len(self.word_counts):,}\")\n",
        "        if self.num_words:\n",
        "            print(f\"  Kept top {self.num_words:,} words\")\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        \"\"\"Convert texts to sequences of integers\"\"\"\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            words = text.split()\n",
        "            sequence = []\n",
        "            for word in words:\n",
        "                if word in self.word_index:\n",
        "                    sequence.append(self.word_index[word])\n",
        "                else:\n",
        "                    sequence.append(self.word_index[UNK])\n",
        "            sequences.append(sequence)\n",
        "        return sequences\n",
        "\n",
        "    def sequences_to_texts(self, sequences):\n",
        "        \"\"\"Convert sequences of integers back to texts\"\"\"\n",
        "        texts = []\n",
        "        for sequence in sequences:\n",
        "            words = []\n",
        "            for idx in sequence:\n",
        "                if idx in self.index_word:\n",
        "                    word = self.index_word[idx]\n",
        "                    # Skip special tokens except for visualization\n",
        "                    if word not in [PAD, BOS, EOS]:\n",
        "                        words.append(word)\n",
        "                else:\n",
        "                    words.append(UNK)\n",
        "            texts.append(' '.join(words))\n",
        "        return texts\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        \"\"\"Get vocabulary size\"\"\"\n",
        "        return len(self.word_index)\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save tokenizer\"\"\"\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'word_index': self.word_index,\n",
        "                'index_word': self.index_word,\n",
        "                'word_counts': self.word_counts,\n",
        "                'num_words': self.num_words,\n",
        "                'oov_token': self.oov_token\n",
        "            }, f)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        \"\"\"Load tokenizer\"\"\"\n",
        "        with open(filepath, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.word_index = data['word_index']\n",
        "            self.index_word = data['index_word']\n",
        "            self.word_counts = data['word_counts']\n",
        "            self.num_words = data['num_words']\n",
        "            self.oov_token = data['oov_token']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2kjrgxN71xc"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"Dataset for translation task\"\"\"\n",
        "    def __init__(self, src_sequences, trg_sequences, max_len=None):\n",
        "        self.src_sequences = src_sequences\n",
        "        self.trg_sequences = trg_sequences\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_sequences[idx]\n",
        "        trg = self.trg_sequences[idx]\n",
        "\n",
        "        # Truncate if too long\n",
        "        if self.max_len:\n",
        "            src = src[:self.max_len]\n",
        "            trg = trg[:self.max_len]\n",
        "\n",
        "        src = torch.tensor(src, dtype=torch.long)\n",
        "        trg = torch.tensor(trg, dtype=torch.long)\n",
        "        return src, trg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etGSHqXpc2M-"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader with padding\"\"\"\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "\n",
        "    # Pad sequences\n",
        "    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    return src_batch, trg_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6-c9uxiOXiT"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for Transformer\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # FIX: Use torch.arange instead of torch.range (deprecated)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding and apply dropout\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eC40b5UO_CN"
      },
      "outputs": [],
      "source": [
        "class TransformerTranslator(nn.Module):\n",
        "    \"\"\"Transformer model for translation\"\"\"\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=512, nhead=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048,\n",
        "                 dropout=0.1, max_len=200):\n",
        "        super(TransformerTranslator, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len, dropout=dropout)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Generate mask for target sequence\"\"\"\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        # Create masks\n",
        "        trg_mask = self.generate_square_subsequent_mask(trg.size(1)).to(src.device)\n",
        "        src_padding_mask = (src == 0)\n",
        "        trg_padding_mask = (trg == 0)\n",
        "\n",
        "        # Embeddings with scaling\n",
        "        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n",
        "        trg_emb = self.pos_encoder(self.trg_embedding(trg) * math.sqrt(self.d_model))\n",
        "\n",
        "        # Transformer\n",
        "        output = self.transformer(\n",
        "            src_emb, trg_emb,\n",
        "            tgt_mask=trg_mask,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=trg_padding_mask\n",
        "        )\n",
        "\n",
        "        return self.fc_out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_3TiwXGY0n5"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, clip=1.0):\n",
        "  \"\"\"Train for one epoch\"\"\"\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "  for src, trg in progress_bar:\n",
        "    src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(src, trg[:, :-1])\n",
        "\n",
        "    # Reshape for loss calculation\n",
        "    output = output.reshape(-1, output.shape[-1])\n",
        "    trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "    loss = criterion(output, trg)\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "  return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crgy3IrqZi-5"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg[:, :-1])\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLM1t15PZkch"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, src_tokenizer, trg_tokenizer, max_len=50):\n",
        "    \"\"\"Translate a single sentence\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess and tokenize source sentence\n",
        "    preprocessed = preprocess_sentence(sentence, max_length=max_len)\n",
        "    src_sequence = src_tokenizer.texts_to_sequences([preprocessed])[0]\n",
        "    src_tensor = torch.tensor(src_sequence).unsqueeze(0).to(device)\n",
        "\n",
        "    # Start with BOS token\n",
        "    trg_indices = [trg_tokenizer.word_index[BOS]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            trg_tensor = torch.tensor(trg_indices).unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(src_tensor, trg_tensor)\n",
        "            next_token = output.argmax(dim=-1)[:, -1].item()\n",
        "\n",
        "            trg_indices.append(next_token)\n",
        "\n",
        "            # Stop if EOS token\n",
        "            if next_token == trg_tokenizer.word_index[EOS]:\n",
        "                break\n",
        "\n",
        "    # Decode to text\n",
        "    translation = trg_tokenizer.sequences_to_texts([trg_indices])[0]\n",
        "    return translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGOX51v3Zo3X"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu(model, test_texts, src_tokenizer, trg_tokenizer, sample_size=None):\n",
        "    \"\"\"Calculate BLEU score on test data\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    src_texts, trg_texts = test_texts\n",
        "\n",
        "    if sample_size:\n",
        "        indices = random.sample(range(len(src_texts)), min(sample_size, len(src_texts)))\n",
        "        src_texts = [src_texts[i] for i in indices]\n",
        "        trg_texts = [trg_texts[i] for i in indices]\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(\"Generating translations for BLEU calculation...\")\n",
        "    for src, trg in tqdm(zip(src_texts, trg_texts), total=len(src_texts)):\n",
        "        # Translate (src is already preprocessed with BOS/EOS)\n",
        "        # Remove BOS/EOS for input to translate_sentence\n",
        "        src_clean = src.replace(BOS, '').replace(EOS, '').strip()\n",
        "        pred = translate_sentence(model, src_clean, src_tokenizer, trg_tokenizer)\n",
        "\n",
        "        # Clean reference (remove BOS/EOS)\n",
        "        trg_clean = trg.replace(BOS, '').replace(EOS, '').strip()\n",
        "\n",
        "        predictions.append(pred)\n",
        "        references.append(trg_clean)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
        "\n",
        "    return bleu.score, predictions, references"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sApPKs6nZr23"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"TRANSFORMER MODEL FOR ENGLISH-VIETNAMESE TRANSLATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Model configuration\n",
        "config = {\n",
        "    'd_model': 256,\n",
        "    'nhead': 8,\n",
        "    'num_encoder_layers': 3,\n",
        "    'num_decoder_layers': 3,\n",
        "    'dim_feedforward': 512,\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 32,\n",
        "    'num_epochs': 50,\n",
        "    'learning_rate': 0.0001,\n",
        "    'max_len': 150,\n",
        "    'max_samples': None,  # None for full dataset, or number like 5000 for testing\n",
        "    'max_sentence_length': 100  # Max words per sentence\n",
        "}\n",
        "\n",
        "print(\"\\n📋 Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaTw14kgZ5F3"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load data from DataFrame\n",
        "en_data, vi_data = load_data_from_dataframe(\n",
        "    corpus_df,\n",
        "    max_samples=config['max_samples'],\n",
        "    max_length=config['max_sentence_length']\n",
        ")\n",
        "\n",
        "# Split data: 80% train, 10% val, 10% test\n",
        "train_size = int(0.8 * len(en_data))\n",
        "val_size = int(0.1 * len(en_data))\n",
        "\n",
        "train_en = en_data[:train_size]\n",
        "train_vi = vi_data[:train_size]\n",
        "val_en = en_data[train_size:train_size+val_size]\n",
        "val_vi = vi_data[train_size:train_size+val_size]\n",
        "test_en = en_data[train_size+val_size:]\n",
        "test_vi = vi_data[train_size+val_size:]\n",
        "\n",
        "print(f\"\\n📊 Data split:\")\n",
        "print(f\"  Train: {len(train_en):,} sentences\")\n",
        "print(f\"  Val:   {len(val_en):,} sentences\")\n",
        "print(f\"  Test:  {len(test_en):,} sentences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhEeEAdKZ7SC"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BUILDING VOCABULARIES (Keras/TensorFlow style)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Initialize tokenizers\n",
        "en_tokenizer = Tokenizer(num_words=50000)  # Keep top 50K words\n",
        "vi_tokenizer = Tokenizer(num_words=50000)\n",
        "\n",
        "# Fit on training data\n",
        "print(\"\\n🔤 Fitting English tokenizer...\")\n",
        "en_tokenizer.fit_on_texts(train_en)\n",
        "\n",
        "print(\"🔤 Fitting Vietnamese tokenizer...\")\n",
        "vi_tokenizer.fit_on_texts(train_vi)\n",
        "\n",
        "print(f\"\\n✓ English vocab size: {en_tokenizer.get_vocab_size():,}\")\n",
        "print(f\"✓ Vietnamese vocab size: {vi_tokenizer.get_vocab_size():,}\")\n",
        "\n",
        "# Show tokenization examples\n",
        "print(\"\\n📝 Tokenization examples:\")\n",
        "sample_en = train_en[0]\n",
        "sample_vi = train_vi[0]\n",
        "\n",
        "en_seq = en_tokenizer.texts_to_sequences([sample_en])[0]\n",
        "vi_seq = vi_tokenizer.texts_to_sequences([sample_vi])[0]\n",
        "\n",
        "print(f\"\\nEnglish text: {sample_en}\")\n",
        "print(f\"Sequence:     {en_seq[:20]}...\")  # Show first 20 tokens\n",
        "print(f\"Decoded back: {en_tokenizer.sequences_to_texts([en_seq])[0]}\")\n",
        "\n",
        "print(f\"\\nVietnamese text: {sample_vi}\")\n",
        "print(f\"Sequence:        {vi_seq[:20]}...\")\n",
        "print(f\"Decoded back:    {vi_tokenizer.sequences_to_texts([vi_seq])[0]}\")\n",
        "\n",
        "# Convert all texts to sequences\n",
        "print(\"\\n🔄 Converting texts to sequences...\")\n",
        "train_en_seq = en_tokenizer.texts_to_sequences(train_en)\n",
        "train_vi_seq = vi_tokenizer.texts_to_sequences(train_vi)\n",
        "val_en_seq = en_tokenizer.texts_to_sequences(val_en)\n",
        "val_vi_seq = vi_tokenizer.texts_to_sequences(val_vi)\n",
        "test_en_seq = en_tokenizer.texts_to_sequences(test_en)\n",
        "test_vi_seq = vi_tokenizer.texts_to_sequences(test_vi)\n",
        "\n",
        "print(f\"✓ Converted all texts to sequences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWAv2wmWaYoy"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING DATASETS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create datasets from sequences with max_len truncation\n",
        "train_dataset = TranslationDataset(train_en_seq, train_vi_seq, max_len=config['max_len'])\n",
        "val_dataset = TranslationDataset(val_en_seq, val_vi_seq, max_len=config['max_len'])\n",
        "test_dataset = TranslationDataset(test_en_seq, test_vi_seq, max_len=config['max_len'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],\n",
        "                         shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'],\n",
        "                       collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'],\n",
        "                        collate_fn=collate_fn)\n",
        "\n",
        "print(f\"✓ Train batches: {len(train_loader)}\")\n",
        "print(f\"✓ Val batches:   {len(val_loader)}\")\n",
        "print(f\"✓ Test batches:  {len(test_loader)}\")\n",
        "print(f\"✓ Max sequence length: {config['max_len']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G95MbOnabMS"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INITIALIZING MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model = TransformerTranslator(\n",
        "    src_vocab_size=en_tokenizer.get_vocab_size(),\n",
        "    trg_vocab_size=vi_tokenizer.get_vocab_size(),\n",
        "    d_model=config['d_model'],\n",
        "    nhead=config['nhead'],\n",
        "    num_encoder_layers=config['num_encoder_layers'],\n",
        "    num_decoder_layers=config['num_decoder_layers'],\n",
        "    dim_feedforward=config['dim_feedforward'],\n",
        "    dropout=config['dropout'],\n",
        "    max_len=config['max_len']\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n🤖 Model Architecture:\")\n",
        "print(f\"  Total parameters:     {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Model size:           ~{total_params * 4 / 1e6:.2f} MB\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "print(f\"✓ Loss function: CrossEntropyLoss\")\n",
        "print(f\"✓ Optimizer: Adam (lr={config['learning_rate']})\")\n",
        "print(f\"✓ Scheduler: ReduceLROnPlateau\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orOgGL1kaeAb"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(config['num_epochs']):\n",
        "    epoch_start = time.time()\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "\n",
        "    # Validate\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    # Record losses\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n📊 Results:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train PPL: {math.exp(train_loss):.2f}\")\n",
        "    print(f\"  Val Loss:   {val_loss:.4f} | Val PPL:   {math.exp(val_loss):.2f}\")\n",
        "    print(f\"  Time:       {epoch_time:.2f}s\")\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'config': config\n",
        "        }, 'best_model.pt')\n",
        "        print(\"  ✓ Best model saved!\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"✓ Training completed in {total_time/60:.2f} minutes\")\n",
        "print(f\"  Average time per epoch: {total_time/config['num_epochs']/60:.2f} minutes\")\n",
        "\n",
        "# ============================================================================\n",
        "# Cell 19: Load Best Model\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 6: LOADING BEST MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "checkpoint = torch.load('best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(f\"✓ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "print(f\"  Val Loss: {checkpoint['val_loss']:.4f}\")\n",
        "print(f\"  Val PPL:  {math.exp(checkpoint['val_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKSB4B2RboK_"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EVALUATING ON TEST SET\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "test_loss = evaluate(model, test_loader, criterion)\n",
        "test_ppl = math.exp(test_loss)\n",
        "\n",
        "print(f\"\\n📊 Test Results:\")\n",
        "print(f\"  Test Loss:       {test_loss:.4f}\")\n",
        "print(f\"  Test Perplexity: {test_ppl:.2f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Cell 21: Calculate BLEU Score\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8: CALCULATING BLEU SCORE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "bleu_score, predictions, references = calculate_bleu(\n",
        "    model, (test_en, test_vi), en_tokenizer, vi_tokenizer, sample_size=100\n",
        ")\n",
        "\n",
        "print(f\"\\n🎯 BLEU Score: {bleu_score:.2f}\")\n",
        "\n",
        "print(\"\\n📝 Sample Translations:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i in range(min(5, len(predictions))):\n",
        "    # Clean test sentences (remove BOS/EOS for display)\n",
        "    test_en_clean = test_en[i].replace(BOS, '').replace(EOS, '').strip()\n",
        "\n",
        "    print(f\"\\n[Example {i+1}]\")\n",
        "    print(f\"Source:     {test_en_clean}\")\n",
        "    print(f\"Reference:  {references[i]}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKs9WrUJb5gg"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"VISUALIZING RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 1)\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "plt.plot(epochs, train_losses, 'bo-', label='Train Loss', linewidth=2, markersize=6)\n",
        "plt.plot(epochs, val_losses, 'rs-', label='Val Loss', linewidth=2, markersize=6)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Perplexity plot\n",
        "plt.subplot(1, 2, 2)\n",
        "train_ppls = [math.exp(loss) for loss in train_losses]\n",
        "val_ppls = [math.exp(loss) for loss in val_losses]\n",
        "plt.plot(epochs, train_ppls, 'bo-', label='Train Perplexity', linewidth=2, markersize=6)\n",
        "plt.plot(epochs, val_ppls, 'rs-', label='Val Perplexity', linewidth=2, markersize=6)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Perplexity', fontsize=12)\n",
        "plt.title('Training and Validation Perplexity', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Training curves saved to 'training_curves.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0jRXG75b7fj"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'test_loss': test_loss,\n",
        "    'test_perplexity': test_ppl,\n",
        "    'bleu_score': bleu_score,\n",
        "    'config': config,\n",
        "    'vocab_sizes': {\n",
        "        'en': en_tokenizer.get_vocab_size(),\n",
        "        'vi': vi_tokenizer.get_vocab_size()\n",
        "    },\n",
        "    'training_time': total_time,\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses\n",
        "}\n",
        "\n",
        "with open('results.pkl', 'wb') as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "# Save tokenizers\n",
        "en_tokenizer.save('en_tokenizer.pkl')\n",
        "vi_tokenizer.save('vi_tokenizer.pkl')\n",
        "\n",
        "print(\"✓ Results saved to 'results.pkl'\")\n",
        "print(\"✓ Tokenizers saved to 'en_tokenizer.pkl' and 'vi_tokenizer.pkl'\")\n",
        "print(\"✓ Best model saved to 'best_model.pt'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OArymzMUb9vv"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL TRANSLATION QUALITY REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n📊 Model Performance:\")\n",
        "print(f\"  {'Metric':<20} {'Value':<15}\")\n",
        "print(f\"  {'-'*35}\")\n",
        "print(f\"  {'Test Loss':<20} {test_loss:<15.4f}\")\n",
        "print(f\"  {'Test Perplexity':<20} {test_ppl:<15.2f}\")\n",
        "print(f\"  {'BLEU Score':<20} {bleu_score:<15.2f}\")\n",
        "\n",
        "print(f\"\\n📐 Model Size:\")\n",
        "print(f\"  {'EN Vocabulary':<20} {en_tokenizer.get_vocab_size():<15,}\")\n",
        "print(f\"  {'VI Vocabulary':<20} {vi_tokenizer.get_vocab_size():<15,}\")\n",
        "print(f\"  {'Parameters':<20} {total_params:<15,}\")\n",
        "\n",
        "print(f\"\\n⏱️  Training Time:\")\n",
        "print(f\"  {'Total Time':<20} {total_time/60:<15.2f} minutes\")\n",
        "print(f\"  {'Avg per Epoch':<20} {total_time/config['num_epochs']/60:<15.2f} minutes\")\n",
        "\n",
        "print(\"\\n✅ Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uv3R6caEcDFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8a023a-2ab9-4ae5-c578-2780c576523b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRANSLATION TEST\n",
            "======================================================================\n",
            "\n",
            "Translating 10 sentences...\n",
            "\n",
            "Processing [1/10]... Done\n",
            "Processing [2/10]... Done\n",
            "Processing [3/10]... Done\n",
            "Processing [4/10]... Done\n",
            "Processing [5/10]... Done\n",
            "Processing [6/10]... Done\n",
            "Processing [7/10]... Done\n",
            "Processing [8/10]... Done\n",
            "Processing [9/10]... Done\n",
            "Processing [10/10]... Done\n",
            "\n",
            "======================================================================\n",
            "RESULTS\n",
            "======================================================================\n",
            "\n",
            "✅ Test 1:\n",
            "   🇬🇧 EN: Hello, how are you?\n",
            "   🇻🇳 VI: xin chào , làm sao con là gì ?\n",
            "\n",
            "✅ Test 2:\n",
            "   🇬🇧 EN: I love learning natural language processing.\n",
            "   🇻🇳 VI: tôi yêu thích sự tự nhiên bằng ngôn ngữ tự nhiên .\n",
            "\n",
            "✅ Test 3:\n",
            "   🇬🇧 EN: The weather is beautiful today.\n",
            "   🇻🇳 VI: thời tiết thời là tuyệt vời ngày nay rất đẹp .\n",
            "\n",
            "✅ Test 4:\n",
            "   🇬🇧 EN: Machine learning is changing the world.\n",
            "   🇻🇳 VI: máy học đang học đang thay đổi thế giới .\n",
            "\n",
            "✅ Test 5:\n",
            "   🇬🇧 EN: What is your name?\n",
            "   🇻🇳 VI: cái tên của bạn là gì ? ? ? ? ? ? ?\n",
            "\n",
            "✅ Test 6:\n",
            "   🇬🇧 EN: I am studying artificial intelligence.\n",
            "   🇻🇳 VI: tôi đang học đại học nhân tình báo nhân của tình báo .\n",
            "\n",
            "✅ Test 7:\n",
            "   🇬🇧 EN: She goes to school every day.\n",
            "   🇻🇳 VI: cô ấy đi học mỗi ngày mỗi ngày .\n",
            "\n",
            "✅ Test 8:\n",
            "   🇬🇧 EN: This book is very interesting.\n",
            "   🇻🇳 VI: quyển sách này rất thú vị rất thú vị .\n",
            "\n",
            "✅ Test 9:\n",
            "   🇬🇧 EN: Can you help me with this problem?\n",
            "   🇻🇳 VI: bạn có thể giúp tôi làm cho tôi vấn đề này ?\n",
            "\n",
            "✅ Test 10:\n",
            "   🇬🇧 EN: Thank you for your time.\n",
            "   🇻🇳 VI: cám ơn bạn cho thời gian của bạn . . .\n",
            "\n",
            "======================================================================\n",
            "✅ Completed: 10/10\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRANSLATION TEST\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Danh sách các câu test\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I love learning natural language processing.\",\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"Machine learning is changing the world.\",\n",
        "    \"What is your name?\",\n",
        "    \"I am studying artificial intelligence.\",\n",
        "    \"She goes to school every day.\",\n",
        "    \"This book is very interesting.\",\n",
        "    \"Can you help me with this problem?\",\n",
        "    \"Thank you for your time.\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"\\nTranslating {len(test_sentences)} sentences...\\n\")\n",
        "\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    print(f\"Processing [{i}/{len(test_sentences)}]...\", end=\" \")\n",
        "\n",
        "    try:\n",
        "        translation = translate_sentence(model, sentence, en_tokenizer, vi_tokenizer)\n",
        "        results.append((sentence, translation, \"✅\"))\n",
        "        print(\"Done\")\n",
        "    except Exception as e:\n",
        "        results.append((sentence, f\"Error: {e}\", \"❌\"))\n",
        "        print(\"Failed\")\n",
        "\n",
        "# In kết quả\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "for i, (en, vi, status) in enumerate(results, 1):\n",
        "    print(f\"{status} Test {i}:\")\n",
        "    print(f\"   🇬🇧 EN: {en}\")\n",
        "    print(f\"   🇻🇳 VI: {vi}\\n\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"✅ Completed: {sum(1 for r in results if r[2] == '✅')}/{len(results)}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZR73uv0iL0G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}