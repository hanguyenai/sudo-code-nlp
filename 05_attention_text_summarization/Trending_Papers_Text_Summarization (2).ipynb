{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoDI_OWGoHDZ"
      },
      "source": [
        "# 1 Setup & Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixYBt2cfuEwW",
        "outputId": "f2c064e5-6bde-4989-b04b-b53959560501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import HTML, display\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from threading import Lock\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFIUkVPb79cQ"
      },
      "source": [
        "# 2 Crawl papers from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vRNFttKyuSTm"
      },
      "outputs": [],
      "source": [
        "class HuggingFaceScraper:\n",
        "    \"\"\"\n",
        "    Advanced HuggingFace Papers Scraper with Full Abstract Support\n",
        "    Scrapes papers from https://huggingface.co/papers with complete abstracts\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hf_token=None):\n",
        "        self.base_url = \"https://huggingface.co\"\n",
        "        self.session = requests.Session()\n",
        "        self.user_agents = [\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
        "        ]\n",
        "\n",
        "        # Base headers\n",
        "        self.session.headers.update({\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1'\n",
        "        })\n",
        "\n",
        "        # Optional authenticated access\n",
        "        if hf_token or os.getenv(\"HF_TOKEN\"):\n",
        "            token = hf_token or os.getenv(\"HF_TOKEN\")\n",
        "            self.session.headers.update({\n",
        "                \"Authorization\": f\"Bearer {token}\"\n",
        "            })\n",
        "            print(\"ğŸ” Using authenticated Hugging Face session.\")\n",
        "        else:\n",
        "            print(\"âš ï¸  No HF token provided â€” scraping as anonymous (limited rate).\")\n",
        "\n",
        "\n",
        "    def scrape_date_range(self, start_date, end_date, delay=2.5, fetch_full_abstract=True):\n",
        "        \"\"\"\n",
        "        Scrape papers from date range\n",
        "\n",
        "        Args:\n",
        "            start_date (str): Start date in YYYY-MM-DD format\n",
        "            end_date (str): End date in YYYY-MM-DD format\n",
        "            delay (float): Delay between requests in seconds\n",
        "            fetch_full_abstract (bool): If True, visit each paper page to get full abstract\n",
        "\n",
        "        Returns:\n",
        "            list: List of paper dictionaries\n",
        "        \"\"\"\n",
        "        current = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "        end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "        all_papers = []\n",
        "        days = 0\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ğŸš€ SCRAPING HUGGINGFACE PAPERS\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"ğŸ“… Date range: {start_date} to {end_date}\")\n",
        "        print(f\"âš™ï¸  Full abstract mode: {'ENABLED âœ“' if fetch_full_abstract else 'DISABLED âœ—'}\")\n",
        "        if fetch_full_abstract:\n",
        "            print(f\"âš ï¸  Note: Full abstract mode is SLOWER but gets complete abstracts\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        while current <= end:\n",
        "            date_str = current.strftime(\"%Y-%m-%d\")\n",
        "            papers = self._scrape_date(date_str, fetch_full_abstract)\n",
        "\n",
        "            if papers:\n",
        "                all_papers.extend(papers)\n",
        "                print(f\"âœ“ {date_str}: {len(papers)} papers\")\n",
        "            else:\n",
        "                print(f\"â—‹ {date_str}: no papers\")\n",
        "\n",
        "            days += 1\n",
        "\n",
        "            # Progress update every 10 days\n",
        "            if days % 10 == 0:\n",
        "                print(f\"\\n{'â”€'*80}\")\n",
        "                print(f\"ğŸ“Š Progress: {days} days, {len(all_papers)} papers total\")\n",
        "                print(f\"{'â”€'*80}\\n\")\n",
        "                time.sleep(random.uniform(5, 10))  # Longer break\n",
        "\n",
        "            current += timedelta(days=1)\n",
        "            time.sleep(delay + random.uniform(0, 2))\n",
        "\n",
        "        # Deduplicate\n",
        "        unique = self._deduplicate(all_papers)\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"âœ… SCRAPING COMPLETE!\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"ğŸ“Š Total unique papers: {len(unique)}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return unique\n",
        "\n",
        "    def _scrape_date(self, date_str, fetch_full_abstract=True):\n",
        "        \"\"\"\n",
        "        Scrape papers from a specific date\n",
        "\n",
        "        Args:\n",
        "            date_str (str): Date in YYYY-MM-DD format\n",
        "            fetch_full_abstract (bool): Whether to fetch full abstracts\n",
        "\n",
        "        Returns:\n",
        "            list: List of papers for that date\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/papers/date/{date_str}\"\n",
        "\n",
        "        try:\n",
        "            headers = {'User-Agent': random.choice(self.user_agents)}\n",
        "            response = self.session.get(url, headers=headers, timeout=15)\n",
        "\n",
        "            if response.status_code == 404:\n",
        "                return []\n",
        "\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all paper articles (limit to 50 per day)\n",
        "            articles = soup.find_all('article')[:50]\n",
        "\n",
        "            papers = []\n",
        "            for i, article in enumerate(articles, 1):\n",
        "                paper = self._parse_article(article, date_str, fetch_full_abstract)\n",
        "                if paper:\n",
        "                    papers.append(paper)\n",
        "                    if fetch_full_abstract and i % 5 == 0:\n",
        "                        print(f\"      Progress: {i}/{len(articles)} papers processed\")\n",
        "\n",
        "            return papers\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"   âŒ Request error for {date_str}: {e}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Unexpected error for {date_str}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _parse_article(self, article, date_str, fetch_full_abstract=True):\n",
        "        \"\"\"\n",
        "        Parse article element to extract paper information\n",
        "\n",
        "        Args:\n",
        "            article: BeautifulSoup element\n",
        "            date_str (str): Date string\n",
        "            fetch_full_abstract (bool): Whether to fetch full abstract\n",
        "\n",
        "        Returns:\n",
        "            dict: Paper information or None if parsing fails\n",
        "        \"\"\"\n",
        "        paper = {\n",
        "            'date': date_str,\n",
        "            'scraped_at': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Extract title\n",
        "        title_elem = article.find('h3') or article.find('h2')\n",
        "        if not title_elem:\n",
        "            return None\n",
        "        paper['title'] = title_elem.get_text(strip=True)\n",
        "\n",
        "        # Extract link and paper ID\n",
        "        link_elem = article.find('a', href=re.compile(r'/papers/\\d+\\.\\d+'))\n",
        "        if link_elem:\n",
        "            paper['link'] = self.base_url + link_elem['href']\n",
        "\n",
        "            # Extract arXiv ID (format: YYMM.NNNNN)\n",
        "            match = re.search(r'(\\d{4})\\.(\\d{4,5})', link_elem['href'])\n",
        "            if match:\n",
        "                paper['paper_id'] = f\"{match.group(1)}.{match.group(2)}\"\n",
        "                # Extract year from arXiv ID\n",
        "                year_code = match.group(1)[:2]\n",
        "                paper['year'] = 2000 + int(year_code)\n",
        "\n",
        "        # Extract metadata from listing page\n",
        "        text_content = article.get_text()\n",
        "\n",
        "        # Extract number of authors\n",
        "        author_match = re.search(r'(\\d+)\\s+authors?', text_content, re.IGNORECASE)\n",
        "        if author_match:\n",
        "            paper['authors_count'] = int(author_match.group(1))\n",
        "\n",
        "        # Extract upvotes - Multiple strategies\n",
        "        # Strategy 1: Look for div with class \"leading-none\" (common for upvote count)\n",
        "        upvote_div = article.find('div', class_='leading-none')\n",
        "        if upvote_div:\n",
        "            upvote_text = upvote_div.get_text(strip=True)\n",
        "            try:\n",
        "                paper['upvotes'] = int(upvote_text)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        # Strategy 2: Text pattern matching (fallback)\n",
        "        if 'upvotes' not in paper:\n",
        "            upvote_match = re.search(r'(\\d+)\\s+(?:upvotes?|likes?)', text_content, re.IGNORECASE)\n",
        "            if upvote_match:\n",
        "                paper['upvotes'] = int(upvote_match.group(1))\n",
        "\n",
        "        # Extract abstract\n",
        "        if fetch_full_abstract and paper.get('link'):\n",
        "            # Fetch FULL abstract AND additional metadata from detail page\n",
        "            print(f\"      ğŸ“– {paper['title'][:60]}...\")\n",
        "            detail_data = self._fetch_detail_page(paper['link'])\n",
        "\n",
        "            if detail_data:\n",
        "                # Update with full abstract\n",
        "                if detail_data.get('abstract'):\n",
        "                    paper['abstract'] = detail_data['abstract']\n",
        "                    paper['abstract_type'] = 'full'\n",
        "                    word_count = len(detail_data['abstract'].split())\n",
        "                    print(f\"         âœ“ Full abstract: {word_count} words\")\n",
        "\n",
        "                # Update upvotes from detail page if not found earlier\n",
        "                if detail_data.get('upvotes') and not paper.get('upvotes'):\n",
        "                    paper['upvotes'] = detail_data['upvotes']\n",
        "\n",
        "                # Add any other metadata from detail page\n",
        "                if detail_data.get('authors'):\n",
        "                    paper['authors'] = detail_data['authors']\n",
        "            else:\n",
        "                # Fallback to preview\n",
        "                print(f\"         âš ï¸ Detail page failed, using preview\")\n",
        "                preview = self._extract_preview_abstract(article)\n",
        "                if preview:\n",
        "                    paper['abstract'] = preview\n",
        "                    paper['abstract_type'] = 'preview'\n",
        "\n",
        "            # Add delay between detail page requests\n",
        "            time.sleep(random.uniform(1.5, 3))\n",
        "        else:\n",
        "            # Get preview abstract from listing page\n",
        "            preview = self._extract_preview_abstract(article)\n",
        "            if preview:\n",
        "                paper['abstract'] = preview\n",
        "                paper['abstract_type'] = 'preview'\n",
        "\n",
        "        # Only return paper if it has an abstract\n",
        "        return paper if paper.get('abstract') else None\n",
        "\n",
        "    def _extract_preview_abstract(self, article):\n",
        "        \"\"\"\n",
        "        Extract preview abstract from listing page\n",
        "\n",
        "        Args:\n",
        "            article: BeautifulSoup element\n",
        "\n",
        "        Returns:\n",
        "            str: Preview abstract or None\n",
        "        \"\"\"\n",
        "        paragraphs = article.find_all('p')\n",
        "        abstracts = [p.get_text(strip=True) for p in paragraphs\n",
        "                    if len(p.get_text(strip=True)) > 50]\n",
        "\n",
        "        if abstracts:\n",
        "            return ' '.join(abstracts)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _fetch_detail_page(self, paper_url):\n",
        "        \"\"\"\n",
        "        Fetch complete information from paper detail page\n",
        "        Including: full abstract, upvotes, authors, etc.\n",
        "\n",
        "        Args:\n",
        "            paper_url (str): URL of the paper detail page\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary with abstract, upvotes, authors, etc. or None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            headers = {'User-Agent': random.choice(self.user_agents)}\n",
        "            response = self.session.get(paper_url, headers=headers, timeout=15)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            detail_data = {}\n",
        "\n",
        "            # ===== EXTRACT UPVOTES =====\n",
        "            # Look for div with class \"leading-none\" (common pattern for upvote display)\n",
        "            upvote_div = soup.find('div', class_='leading-none')\n",
        "            if upvote_div:\n",
        "                upvote_text = upvote_div.get_text(strip=True)\n",
        "                try:\n",
        "                    detail_data['upvotes'] = int(upvote_text)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "            # ===== EXTRACT AUTHORS =====\n",
        "            # Look for author information (may need adjustment based on actual HTML)\n",
        "            author_links = soup.find_all('a', href=re.compile(r'/papers\\?author='))\n",
        "            if author_links:\n",
        "                detail_data['authors'] = [a.get_text(strip=True) for a in author_links]\n",
        "\n",
        "            # ===== EXTRACT FULL ABSTRACT =====\n",
        "            abstract = self._extract_abstract_from_soup(soup)\n",
        "            if abstract:\n",
        "                detail_data['abstract'] = abstract\n",
        "\n",
        "            return detail_data if detail_data else None\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"         âŒ Request error: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"         âŒ Parse error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_abstract_from_soup(self, soup):\n",
        "        \"\"\"\n",
        "        Extract abstract from BeautifulSoup object using multiple strategies\n",
        "\n",
        "        Args:\n",
        "            soup: BeautifulSoup object of the detail page\n",
        "\n",
        "        Returns:\n",
        "            str: Full abstract or None\n",
        "        \"\"\"\n",
        "        # ===== STRATEGY 1: Find Abstract heading and extract following content =====\n",
        "        headings = soup.find_all(['h2', 'h3'])\n",
        "        for heading in headings:\n",
        "            heading_text = heading.get_text(strip=True).lower()\n",
        "            if heading_text == 'abstract':\n",
        "                # Look for next sibling div or container\n",
        "                next_container = heading.find_next_sibling()\n",
        "                if next_container:\n",
        "                    # Extract all paragraphs with class text-gray\n",
        "                    paragraphs = next_container.find_all('p', class_=re.compile(r'text-gray'))\n",
        "                    if paragraphs:\n",
        "                        abstract_parts = []\n",
        "                        for p in paragraphs:\n",
        "                            text = p.get_text(separator=' ', strip=True)\n",
        "                            if len(text) > 20:\n",
        "                                abstract_parts.append(text)\n",
        "                        if abstract_parts:\n",
        "                            return ' '.join(abstract_parts)\n",
        "\n",
        "        # ===== STRATEGY 2: Direct search for text-gray-600 paragraphs =====\n",
        "        gray_paragraphs = soup.find_all('p', class_=re.compile(r'text-gray-600'))\n",
        "        if gray_paragraphs:\n",
        "            abstract_parts = []\n",
        "            for p in gray_paragraphs:\n",
        "                text = p.get_text(separator=' ', strip=True)\n",
        "                if len(text) > 50:\n",
        "                    abstract_parts.append(text)\n",
        "\n",
        "            if abstract_parts:\n",
        "                full_text = ' '.join(abstract_parts)\n",
        "                return full_text[:5000] if len(full_text) > 5000 else full_text\n",
        "\n",
        "        # ===== STRATEGY 3: Look in prose/content containers =====\n",
        "        content_containers = soup.find_all('div', class_=re.compile(r'prose|content|article'))\n",
        "        for container in content_containers:\n",
        "            paragraphs = container.find_all('p')\n",
        "            long_paras = []\n",
        "            for p in paragraphs:\n",
        "                text = p.get_text(separator=' ', strip=True)\n",
        "                if len(text) > 100:\n",
        "                    long_paras.append(text)\n",
        "\n",
        "            if long_paras:\n",
        "                return ' '.join(long_paras[:5])\n",
        "\n",
        "        # ===== STRATEGY 4: Get all substantial paragraphs (fallback) =====\n",
        "        all_paragraphs = soup.find_all('p')\n",
        "        substantial = []\n",
        "        for p in all_paragraphs:\n",
        "            text = p.get_text(separator=' ', strip=True)\n",
        "            if len(text) > 100:\n",
        "                substantial.append(text)\n",
        "\n",
        "        if substantial:\n",
        "            return ' '.join(substantial[:3])\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _deduplicate(self, papers):\n",
        "        \"\"\"\n",
        "        Remove duplicate papers based on paper_id or title\n",
        "\n",
        "        Args:\n",
        "            papers (list): List of paper dictionaries\n",
        "\n",
        "        Returns:\n",
        "            list: Deduplicated list of papers\n",
        "        \"\"\"\n",
        "        seen = set()\n",
        "        unique = []\n",
        "\n",
        "        for paper in papers:\n",
        "            # Use paper_id as primary key, fall back to title\n",
        "            key = paper.get('paper_id') or paper.get('title')\n",
        "            if key and key not in seen:\n",
        "                seen.add(key)\n",
        "                unique.append(paper)\n",
        "\n",
        "        if len(papers) != len(unique):\n",
        "            print(f\"ğŸ”„ Deduplication: {len(papers)} â†’ {len(unique)} papers\")\n",
        "\n",
        "        return unique\n",
        "\n",
        "    def save(self, papers, filename=\"papers.json\"):\n",
        "        \"\"\"\n",
        "        Save papers to JSON file with statistics\n",
        "\n",
        "        Args:\n",
        "            papers (list): List of paper dictionaries\n",
        "            filename (str): Output filename\n",
        "\n",
        "        Returns:\n",
        "            str: Filename of saved file\n",
        "        \"\"\"\n",
        "        # Calculate statistics\n",
        "        stats = {\n",
        "            'total': len(papers),\n",
        "            'with_abstract': sum(1 for p in papers if p.get('abstract')),\n",
        "            'with_full_abstract': sum(1 for p in papers\n",
        "                                     if p.get('abstract_type') == 'full'),\n",
        "            'with_preview_abstract': sum(1 for p in papers\n",
        "                                        if p.get('abstract_type') == 'preview'),\n",
        "            'with_paper_id': sum(1 for p in papers if p.get('paper_id')),\n",
        "            'with_authors': sum(1 for p in papers if p.get('authors_count')),\n",
        "            'with_author_names': sum(1 for p in papers if p.get('authors')),\n",
        "            'with_upvotes': sum(1 for p in papers if p.get('upvotes')),\n",
        "            'avg_abstract_length': sum(len(p.get('abstract', '').split())\n",
        "                                      for p in papers) / len(papers) if papers else 0,\n",
        "            'avg_upvotes': sum(p.get('upvotes', 0) for p in papers) / sum(1 for p in papers if p.get('upvotes')) if any(p.get('upvotes') for p in papers) else 0\n",
        "        }\n",
        "\n",
        "        # Prepare output\n",
        "        output = {\n",
        "            'scraped_at': datetime.now().isoformat(),\n",
        "            'statistics': stats,\n",
        "            'papers': papers\n",
        "        }\n",
        "\n",
        "        # Save to file\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ğŸ’¾ SAVED TO: {filename}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"ğŸ“Š Statistics:\")\n",
        "        print(f\"   Total papers: {stats['total']}\")\n",
        "        print(f\"   With abstracts: {stats['with_abstract']}\")\n",
        "        print(f\"   - Full abstracts: {stats['with_full_abstract']}\")\n",
        "        print(f\"   - Preview abstracts: {stats['with_preview_abstract']}\")\n",
        "        print(f\"   With upvotes: {stats['with_upvotes']} (avg: {stats['avg_upvotes']:.1f})\")\n",
        "        print(f\"   With author names: {stats['with_author_names']}\")\n",
        "        print(f\"   Average abstract length: {stats['avg_abstract_length']:.1f} words\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def scrape_recent_days(self, num_days=30, fetch_full_abstract=True):\n",
        "        \"\"\"\n",
        "        Scrape papers from recent N days\n",
        "\n",
        "        Args:\n",
        "            num_days (int): Number of recent days to scrape\n",
        "            fetch_full_abstract (bool): Whether to fetch full abstracts\n",
        "\n",
        "        Returns:\n",
        "            list: List of papers\n",
        "        \"\"\"\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=num_days)\n",
        "\n",
        "        return self.scrape_date_range(\n",
        "            start_date=start_date.strftime(\"%Y-%m-%d\"),\n",
        "            end_date=end_date.strftime(\"%Y-%m-%d\"),\n",
        "            fetch_full_abstract=fetch_full_abstract\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UY8_47pT-BH",
        "outputId": "e032f2d2-fca1-468b-aefa-4f45e2af2bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ğŸ”¥ SCRAPING WITH ANTI-BLOCKING PROTECTION\n",
            "================================================================================\n",
            "âš ï¸  No HF token provided â€” scraping as anonymous (limited rate).\n",
            "\n",
            "ğŸ¯ Full scraping mode: Every day from Jan to Oct 2025\n",
            "âš ï¸  This will take 15-20 minutes but gives maximum papers!\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸš€ SCRAPING HUGGINGFACE PAPERS\n",
            "================================================================================\n",
            "ğŸ“… Date range: 2024-10-15 to 2025-10-15\n",
            "âš™ï¸  Full abstract mode: ENABLED âœ“\n",
            "âš ï¸  Note: Full abstract mode is SLOWER but gets complete abstracts\n",
            "================================================================================\n",
            "\n",
            "      ğŸ“– Animate-X: Universal Character Image Animation with Enhanced...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      ğŸ“– LOKI: A Comprehensive Synthetic Data Detection Benchmark usi...\n",
            "         âœ“ Full abstract: 198 words\n",
            "      ğŸ“– MMIE: Massive Multimodal Interleaved Comprehension Benchmark...\n",
            "         âœ“ Full abstract: 226 words\n",
            "      ğŸ“– Toward General Instruction-Following Alignment for Retrieval...\n",
            "         âœ“ Full abstract: 237 words\n",
            "      ğŸ“– MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-W...\n",
            "         âœ“ Full abstract: 171 words\n",
            "      Progress: 5/22 papers processed\n",
            "      ğŸ“– Omni-MATH: A Universal Olympiad Level Mathematic Benchmark F...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– Semantic Image Inversion and Editing using Rectified Stochas...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– VisRAG: Vision-based Retrieval-augmented Generation on Multi...\n",
            "         âœ“ Full abstract: 211 words\n",
            "      ğŸ“– LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Paper...\n",
            "         âœ“ Full abstract: 242 words\n",
            "      ğŸ“– Cavia: Camera-controllable Multi-view Video Diffusion with\n",
            " ...\n",
            "         âœ“ Full abstract: 182 words\n",
            "      Progress: 10/22 papers processed\n",
            "      ğŸ“– Thinking LLMs: General Instruction Following with Thought Ge...\n",
            "         âœ“ Full abstract: 167 words\n",
            "      ğŸ“– TemporalBench: Benchmarking Fine-grained Temporal Understand...\n",
            "         âœ“ Full abstract: 231 words\n",
            "      ğŸ“– Rethinking Data Selection at Scale: Random Selection is Almo...\n",
            "         âœ“ Full abstract: 202 words\n",
            "      ğŸ“– LongMemEval: Benchmarking Chat Assistants on Long-Term Inter...\n",
            "         âœ“ Full abstract: 213 words\n",
            "      ğŸ“– MMCOMPOSITION: Revisiting the Compositionality of Pre-traine...\n",
            "         âœ“ Full abstract: 207 words\n",
            "      Progress: 15/22 papers processed\n",
            "      ğŸ“– Tree of Problems: Improving structured problem solving with\n",
            "...\n",
            "         âœ“ Full abstract: 128 words\n",
            "      ğŸ“– DuoAttention: Efficient Long-Context LLM Inference with Retr...\n",
            "         âœ“ Full abstract: 238 words\n",
            "      ğŸ“– Generalizable Humanoid Manipulation with Improved 3D Diffusi...\n",
            "         âœ“ Full abstract: 140 words\n",
            "      ğŸ“– TVBench: Redesigning Video-Language Evaluation...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      ğŸ“– The Same But Different: Structural Similarities and Differen...\n",
            "         âœ“ Full abstract: 188 words\n",
            "      Progress: 20/22 papers processed\n",
            "      ğŸ“– ReLU's Revival: On the Entropic Overload in Normalization-Fr...\n",
            "         âœ“ Full abstract: 169 words\n",
            "      ğŸ“– Latent Action Pretraining from Videos...\n",
            "         âœ“ Full abstract: 187 words\n",
            "âœ“ 2024-10-15: 22 papers\n",
            "      ğŸ“– Your Mixture-of-Experts LLM Is Secretly an Embedding Model F...\n",
            "         âœ“ Full abstract: 199 words\n",
            "      ğŸ“– Efficiently Democratizing Medical LLMs for 50 Languages via ...\n",
            "         âœ“ Full abstract: 198 words\n",
            "      ğŸ“– LLMtimesMapReduce: Simplified Long-Sequence Processing using...\n",
            "         âœ“ Full abstract: 170 words\n",
            "      ğŸ“– What Matters in Transformers? Not All Attention is Needed...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      ğŸ“– MLLM can see? Dynamic Correction Decoding for Hallucination ...\n",
            "         âœ“ Full abstract: 160 words\n",
            "      Progress: 5/18 papers processed\n",
            "      ğŸ“– Agent-as-a-Judge: Evaluate Agents with Agents...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– LVD-2M: A Long-take Video Dataset with Temporally Dense Capt...\n",
            "         âœ“ Full abstract: 240 words\n",
            "      ğŸ“– MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large ...\n",
            "         âœ“ Full abstract: 190 words\n",
            "      ğŸ“– Efficient Diffusion Models: A Comprehensive Survey from Prin...\n",
            "         âœ“ Full abstract: 161 words\n",
            "      ğŸ“– NesTools: A Dataset for Evaluating Nested Tool Learning Abil...\n",
            "         âœ“ Full abstract: 160 words\n",
            "      Progress: 10/18 papers processed\n",
            "      ğŸ“– EchoPrime: A Multi-Video View-Informed Vision-Language Model...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      ğŸ“– SecCodePLT: A Unified Platform for Evaluating the Security o...\n",
            "         âœ“ Full abstract: 263 words\n",
            "      ğŸ“– GS^3: Efficient Relighting with Triple Gaussian Splatting...\n",
            "         âœ“ Full abstract: 191 words\n",
            "      ğŸ“– SimBa: Simplicity Bias for Scaling Up Parameters in Deep Rei...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– Towards Synergistic, Generalized, and Efficient Dual-System ...\n",
            "         âœ“ Full abstract: 185 words\n",
            "      Progress: 15/18 papers processed\n",
            "      ğŸ“– Towards Natural Image Matting in the Wild via Real-Scenario ...\n",
            "         âœ“ Full abstract: 246 words\n",
            "      ğŸ“– Empirical Study of Mutual Reinforcement Effect and Applicati...\n",
            "         âœ“ Full abstract: 158 words\n",
            "      ğŸ“– MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Ce...\n",
            "         âœ“ Full abstract: 153 words\n",
            "âœ“ 2024-10-16: 18 papers\n",
            "      ğŸ“– VidEgoThink: Assessing Egocentric Video Understanding Capabi...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      ğŸ“– HumanEval-V: Benchmarking High-Level Visual Reasoning with C...\n",
            "         âœ“ Full abstract: 162 words\n",
            "      ğŸ“– DocLayout-YOLO: Enhancing Document Layout Analysis through D...\n",
            "         âœ“ Full abstract: 185 words\n",
            "      ğŸ“– The Curse of Multi-Modalities: Evaluating Hallucinations of ...\n",
            "         âœ“ Full abstract: 173 words\n",
            "      ğŸ“– Revealing the Barriers of Language Agents in Planning...\n",
            "         âœ“ Full abstract: 195 words\n",
            "      Progress: 5/23 papers processed\n",
            "      ğŸ“– Exploring Model Kinship for Merging Large Language Models...\n",
            "         âœ“ Full abstract: 159 words\n",
            "      ğŸ“– Simplifying, Stabilizing and Scaling Continuous-Time Consist...\n",
            "         âœ“ Full abstract: 152 words\n",
            "      ğŸ“– Large Language Model Evaluation via Matrix Nuclear-Norm...\n",
            "         âœ“ Full abstract: 215 words\n",
            "      ğŸ“– Improving Long-Text Alignment for Text-to-Image Diffusion Mo...\n",
            "         âœ“ Full abstract: 235 words\n",
            "      ğŸ“– Controllable Safety Alignment: Inference-Time Adaptation to ...\n",
            "         âœ“ Full abstract: 239 words\n",
            "      Progress: 10/23 papers processed\n",
            "      ğŸ“– DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with...\n",
            "         âœ“ Full abstract: 145 words\n",
            "      ğŸ“– ProSA: Assessing and Understanding the Prompt Sensitivity of...\n",
            "         âœ“ Full abstract: 161 words\n",
            "      ğŸ“– ZipVL: Efficient Large Vision-Language Models with Dynamic T...\n",
            "         âœ“ Full abstract: 262 words\n",
            "      ğŸ“– Stabilize the Latent Space for Image Autoregressive Modeling...\n",
            "         âœ“ Full abstract: 260 words\n",
            "      ğŸ“– ChroKnowledge: Unveiling Chronological Knowledge of Language...\n",
            "         âœ“ Full abstract: 255 words\n",
            "      Progress: 15/23 papers processed\n",
            "      ğŸ“– Neural Metamorphosis...\n",
            "         âœ“ Full abstract: 217 words\n",
            "      ğŸ“– WorldMedQA-V: a multilingual, multimodal medical examination...\n",
            "         âœ“ Full abstract: 159 words\n",
            "      ğŸ“– Tracking Universal Features Through Fine-Tuning and Model Me...\n",
            "         âœ“ Full abstract: 104 words\n",
            "      ğŸ“– Insights from the Inverse: Reconstructing LLM Training Goals...\n",
            "         âœ“ Full abstract: 143 words\n",
            "      ğŸ“– OMCAT: Omni Context Aware Transformer...\n",
            "         âœ“ Full abstract: 198 words\n",
            "      Progress: 20/23 papers processed\n",
            "      ğŸ“– FLARE: Faithful Logic-Aided Reasoning and Exploration...\n",
            "         âœ“ Full abstract: 245 words\n",
            "      ğŸ“– Taming Overconfidence in LLMs: Reward Calibration in RLHF...\n",
            "         âœ“ Full abstract: 244 words\n",
            "      ğŸ“– From Commands to Prompts: LLM-based Semantic File System for...\n",
            "         âœ“ Full abstract: 243 words\n",
            "âœ“ 2024-10-17: 23 papers\n",
            "      ğŸ“– Movie Gen: A Cast of Media Foundation Models...\n",
            "         âœ“ Full abstract: 172 words\n",
            "      ğŸ“– MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtu...\n",
            "         âœ“ Full abstract: 149 words\n",
            "      ğŸ“– JudgeBench: A Benchmark for Evaluating LLM-based Judges...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      ğŸ“– Fluid: Scaling Autoregressive Text-to-image Generative Model...\n",
            "         âœ“ Full abstract: 197 words\n",
            "      ğŸ“– Janus: Decoupling Visual Encoding for Unified Multimodal Und...\n",
            "         âœ“ Full abstract: 154 words\n",
            "      Progress: 5/34 papers processed\n",
            "      ğŸ“– Roadmap towards Superhuman Speech Understanding using Large ...\n",
            "         âœ“ Full abstract: 166 words\n",
            "      ğŸ“– MobA: A Two-Level Agent System for Efficient Mobile Task Aut...\n",
            "         âœ“ Full abstract: 135 words\n",
            "      ğŸ“– WorldCuisines: A Massive-Scale Benchmark for Multilingual an...\n",
            "         âœ“ Full abstract: 151 words\n",
            "      ğŸ“– Harnessing Webpage UIs for Text-Rich Visual Understanding...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– DreamVideo-2: Zero-Shot Subject-Driven Video Customization w...\n",
            "         âœ“ Full abstract: 233 words\n",
            "      Progress: 10/34 papers processed\n",
            "      ğŸ“– MMed-RAG: Versatile Multimodal RAG System for Medical Vision...\n",
            "         âœ“ Full abstract: 211 words\n",
            "      ğŸ“– MoH: Multi-Head Attention as Mixture-of-Head Attention...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      ğŸ“– BenTo: Benchmark Task Reduction with In-Context Transferabil...\n",
            "         âœ“ Full abstract: 133 words\n",
            "      ğŸ“– PopAlign: Diversifying Contrasting Patterns for a More Compr...\n",
            "         âœ“ Full abstract: 154 words\n",
            "      ğŸ“– A Comparative Study on Reasoning Patterns of OpenAI's o1 Mod...\n",
            "         âœ“ Full abstract: 230 words\n",
            "      Progress: 15/34 papers processed\n",
            "      ğŸ“– A Unified View of Delta Parameter Editing in Post-Trained La...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      ğŸ“– FlatQuant: Flatness Matters for LLM Quantization...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      ğŸ“– VidPanos: Generative Panoramic Videos from Casual Panning Vi...\n",
            "         âœ“ Full abstract: 190 words\n",
            "      ğŸ“– Do LLMs Have Political Correctness? Analyzing Ethical Biases...\n",
            "         âœ“ Full abstract: 235 words\n",
            "      ğŸ“– Can MLLMs Understand the Deep Implication Behind Chinese Ima...\n",
            "         âœ“ Full abstract: 270 words\n",
            "      Progress: 20/34 papers processed\n",
            "      ğŸ“– Retrospective Learning from Interactions...\n",
            "         âœ“ Full abstract: 151 words\n",
            "      ğŸ“– Failing Forward: Improving Generative Error Correction for A...\n",
            "         âœ“ Full abstract: 211 words\n",
            "      ğŸ“– Remember, Retrieve and Generate: Understanding Infinite Visu...\n",
            "         âœ“ Full abstract: 220 words\n",
            "      ğŸ“– MuVi: Video-to-Music Generation with Semantic Alignment and ...\n",
            "         âœ“ Full abstract: 168 words\n",
            "      ğŸ“– MedMobile: A mobile-sized language model with expert-level c...\n",
            "         âœ“ Full abstract: 112 words\n",
            "      Progress: 25/34 papers processed\n",
            "      ğŸ“– Î³-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal ...\n",
            "         âœ“ Full abstract: 256 words\n",
            "      ğŸ“– LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decompositio...\n",
            "         âœ“ Full abstract: 184 words\n",
            "      ğŸ“– Open Materials 2024 (OMat24) Inorganic Materials Dataset and...\n",
            "         âœ“ Full abstract: 215 words\n",
            "      ğŸ“– Long-LRM: Long-sequence Large Reconstruction Model for Wide-...\n",
            "         âœ“ Full abstract: 150 words\n",
            "      ğŸ“– Minimum Tuning to Unlock Long Output from LLMs with High Qua...\n",
            "         âœ“ Full abstract: 221 words\n",
            "      Progress: 30/34 papers processed\n",
            "      ğŸ“– Toward Guidance-Free AR Visual Generation via Condition Cont...\n",
            "         âœ“ Full abstract: 196 words\n",
            "      ğŸ“– AERO: Softmax-Only LLMs for Efficient Private Inference...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2410.13060\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– TransAgent: Transfer Vision-Language Foundation Models with\n",
            "...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2410.12183\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– SBI-RAG: Enhancing Math Word Problem Solving for Students th...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2410.13293\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "âœ“ 2024-10-18: 31 papers\n",
            "   âŒ Request error for 2024-10-19: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-10-19\n",
            "â—‹ 2024-10-19: no papers\n",
            "   âŒ Request error for 2024-10-20: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-10-20\n",
            "â—‹ 2024-10-20: no papers\n",
            "      ğŸ“– UCFE: A User-Centric Financial Expertise Benchmark for Large...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– Web Agents with World Models: Learning and Leveraging Enviro...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– NaturalBench: Evaluating Vision-Language Models on Natural A...\n",
            "         âœ“ Full abstract: 282 words\n",
            "      ğŸ“– MagicTailor: Component-Controllable Personalization in Text-...\n",
            "         âœ“ Full abstract: 190 words\n",
            "      ğŸ“– SeerAttention: Learning Intrinsic Sparse Attention in Your L...\n",
            "         âœ“ Full abstract: 239 words\n",
            "      Progress: 5/20 papers processed\n",
            "      ğŸ“– FiTv2: Scalable and Improved Flexible Vision Transformer for...\n",
            "         âœ“ Full abstract: 244 words\n",
            "      ğŸ“– DPLM-2: A Multimodal Diffusion Protein Language Model...\n",
            "         âœ“ Full abstract: 241 words\n",
            "      ğŸ“– Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech a...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      ğŸ“– HART: Efficient Visual Generation with Hybrid Autoregressive...\n",
            "         âœ“ Full abstract: 183 words\n",
            "      ğŸ“– Diffusion Curriculum: Synthetic-to-Real Generative Curriculu...\n",
            "         âœ“ Full abstract: 269 words\n",
            "      Progress: 10/20 papers processed\n",
            "      ğŸ“– Are AI Detectors Good Enough? A Survey on Quality of Dataset...\n",
            "         âœ“ Full abstract: 199 words\n",
            "      ğŸ“– DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      ğŸ“– BiGR: Harnessing Binary Latent Codes for Image Generation an...\n",
            "         âœ“ Full abstract: 147 words\n",
            "      ğŸ“– Looking Inward: Language Models Can Learn About Themselves b...\n",
            "         âœ“ Full abstract: 288 words\n",
            "      ğŸ“– SHAKTI: A 2.5 Billion Parameter Small Language Model Optimiz...\n",
            "         âœ“ Full abstract: 92 words\n",
            "      Progress: 15/20 papers processed\n",
            "      ğŸ“– How Do Training Methods Influence the Utilization of Vision ...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– Context is Key(NMF): Modelling Topical Information Dynamics ...\n",
            "         âœ“ Full abstract: 192 words\n",
            "      ğŸ“– A Common Pitfall of Margin-based Language Model Alignment: G...\n",
            "         âœ“ Full abstract: 260 words\n",
            "      ğŸ“– Teaching Models to Balance Resisting and Accepting Persuasio...\n",
            "         âœ“ Full abstract: 217 words\n",
            "      ğŸ“– Montessori-Instruct: Generate Influential Training Data Tail...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      Progress: 20/20 papers processed\n",
            "âœ“ 2024-10-21: 20 papers\n",
            "      ğŸ“– FrugalNeRF: Fast Convergence for Few-shot Novel View Synthes...\n",
            "         âœ“ Full abstract: 144 words\n",
            "      ğŸ“– SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a...\n",
            "         âœ“ Full abstract: 267 words\n",
            "      ğŸ“– CompassJudger-1: All-in-one Judge Model Helps Model Evaluati...\n",
            "         âœ“ Full abstract: 196 words\n",
            "      ğŸ“– AutoTrain: No-code training for state-of-the-art models...\n",
            "         âœ“ Full abstract: 165 words\n",
            "      ğŸ“– PUMA: Empowering Unified MLLM with Multi-granular Visual Gen...\n",
            "         âœ“ Full abstract: 161 words\n",
            "      Progress: 5/23 papers processed\n",
            "      ğŸ“– Baichuan Alignment Technical Report...\n",
            "         âœ“ Full abstract: 195 words\n",
            "      ğŸ“– SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation...\n",
            "         âœ“ Full abstract: 153 words\n",
            "      ğŸ“– Pangea: A Fully Open Multilingual Multimodal LLM for 39 Lang...\n",
            "         âœ“ Full abstract: 161 words\n",
            "      ğŸ“– RM-Bench: Benchmarking Reward Models of Language Models with...\n",
            "         âœ“ Full abstract: 192 words\n",
            "      ğŸ“– Meta-Chunking: Learning Efficient Text Segmentation via Logi...\n",
            "         âœ“ Full abstract: 201 words\n",
            "      Progress: 10/23 papers processed\n",
            "      ğŸ“– Pre-training Distillation for Large Language Models: A Desig...\n",
            "         âœ“ Full abstract: 181 words\n",
            "      ğŸ“– Alchemy: Amplifying Theorem-Proving Capability through Symbo...\n",
            "         âœ“ Full abstract: 212 words\n",
            "      ğŸ“– Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant...\n",
            "         âœ“ Full abstract: 168 words\n",
            "      ğŸ“– Zero-shot Model-based Reinforcement Learning using Large Lan...\n",
            "         âœ“ Full abstract: 141 words\n",
            "      ğŸ“– Selecting Influential Samples for Long Context Alignment via...\n",
            "         âœ“ Full abstract: 253 words\n",
            "      Progress: 15/23 papers processed\n",
            "      ğŸ“– How Many Van Goghs Does It Take to Van Gogh? Finding the Imi...\n",
            "         âœ“ Full abstract: 251 words\n",
            "      ğŸ“– Agent-to-Sim: Learning Interactive Behavior Models from Casu...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– CBT-Bench: Evaluating Large Language Models on Assisting Cog...\n",
            "         âœ“ Full abstract: 191 words\n",
            "      ğŸ“– Router-Tuning: A Simple and Effective Approach for Enabling\n",
            "...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– DM-Codec: Distilling Multimodal Representations for Speech T...\n",
            "         âœ“ Full abstract: 232 words\n",
            "      Progress: 20/23 papers processed\n",
            "      ğŸ“– In-context learning and Occam's razor...\n",
            "         âœ“ Full abstract: 201 words\n",
            "      ğŸ“– Hallucination Detox: Sensitive Neuron Dropout (SeND) for Lar...\n",
            "         âœ“ Full abstract: 211 words\n",
            "      ğŸ“– Cross-Lingual Auto Evaluation for Assessing Multilingual LLM...\n",
            "         âœ“ Full abstract: 211 words\n",
            "âœ“ 2024-10-22: 23 papers\n",
            "      ğŸ“– PyramidDrop: Accelerating Your Large Vision-Language Models ...\n",
            "         âœ“ Full abstract: 279 words\n",
            "      ğŸ“– SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes...\n",
            "         âœ“ Full abstract: 129 words\n",
            "      ğŸ“– Improve Vision Language Model Chain-of-thought Reasoning...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      ğŸ“– Aligning Large Language Models via Self-Steering Optimizatio...\n",
            "         âœ“ Full abstract: 193 words\n",
            "      ğŸ“– xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Repre...\n",
            "         âœ“ Full abstract: 131 words\n",
            "      Progress: 5/13 papers processed\n",
            "      ğŸ“– Mitigating Object Hallucination via Concentric Causal Attent...\n",
            "         âœ“ Full abstract: 202 words\n",
            "      ğŸ“– MiniPLM: Knowledge Distillation for Pre-Training Language Mo...\n",
            "         âœ“ Full abstract: 219 words\n",
            "      ğŸ“– LLM-based Optimization of Compound AI Systems: A Survey...\n",
            "         âœ“ Full abstract: 147 words\n",
            "      ğŸ“– JMMMU: A Japanese Massive Multi-discipline Multimodal Unders...\n",
            "         âœ“ Full abstract: 200 words\n",
            "      ğŸ“– EvoPress: Towards Optimal Dynamic Model Compression via Evol...\n",
            "         âœ“ Full abstract: 251 words\n",
            "      Progress: 10/13 papers processed\n",
            "      ğŸ“– Math Neurosurgery: Isolating Language Models' Math Reasoning...\n",
            "         âœ“ Full abstract: 190 words\n",
            "      ğŸ“– 3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting wit...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– Frontiers in Intelligent Colonoscopy...\n",
            "         âœ“ Full abstract: 129 words\n",
            "âœ“ 2024-10-23: 13 papers\n",
            "      ğŸ“– MIA-DPO: Multi-Image Augmented Direct Preference Optimizatio...\n",
            "         âœ“ Full abstract: 224 words\n",
            "      ğŸ“– LongVU: Spatiotemporal Adaptive Compression for Long Video-L...\n",
            "         âœ“ Full abstract: 174 words\n",
            "      ğŸ“– WorldSimBench: Towards Video Generation Models as World Simu...\n",
            "         âœ“ Full abstract: 224 words\n",
            "      ğŸ“– Scaling Diffusion Language Models via Adaptation from Autore...\n",
            "         âœ“ Full abstract: 188 words\n",
            "      ğŸ“– Scalable Ranked Preference Optimization for Text-to-Image Ge...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      Progress: 5/13 papers processed\n",
            "      ğŸ“– DynamicCity: Large-Scale LiDAR Generation from Dynamic Scene...\n",
            "         âœ“ Full abstract: 251 words\n",
            "      ğŸ“– M-RewardBench: Evaluating Reward Models in Multilingual Sett...\n",
            "         âœ“ Full abstract: 200 words\n",
            "      ğŸ“– Lightweight Neural App Control...\n",
            "         âœ“ Full abstract: 147 words\n",
            "      ğŸ“– ARKit LabelMaker: A New Scale for Indoor 3D Scene Understand...\n",
            "         âœ“ Full abstract: 169 words\n",
            "      ğŸ“– MedINST: Meta Dataset of Biomedical Instructions...\n",
            "         âœ“ Full abstract: 137 words\n",
            "      Progress: 10/13 papers processed\n",
            "      ğŸ“– TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Cus...\n",
            "         âœ“ Full abstract: 174 words\n",
            "      ğŸ“– LVSM: A Large View Synthesis Model with Minimal 3D Inductive...\n",
            "         âœ“ Full abstract: 188 words\n",
            "      ğŸ“– Steering Your Generalists: Improving Robotic Foundation Mode...\n",
            "         âœ“ Full abstract: 217 words\n",
            "âœ“ 2024-10-24: 13 papers\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Progress: 10 days, 163 papers total\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "      ğŸ“– Breaking the Memory Barrier: Near Infinite Batch Size Scalin...\n",
            "         âœ“ Full abstract: 183 words\n",
            "      ğŸ“– Can Knowledge Editing Really Correct Hallucinations?...\n",
            "         âœ“ Full abstract: 223 words\n",
            "      ğŸ“– LOGO -- Long cOntext aliGnment via efficient preference Opti...\n",
            "         âœ“ Full abstract: 207 words\n",
            "      ğŸ“– Unleashing Reasoning Capability of LLMs via Scalable Questio...\n",
            "         âœ“ Full abstract: 199 words\n",
            "      ğŸ“– Framer: Interactive Frame Interpolation...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      Progress: 5/29 papers processed\n",
            "      ğŸ“– Unbounded: A Generative Infinite Game of Character Life Simu...\n",
            "         âœ“ Full abstract: 209 words\n",
            "      ğŸ“– Distill Visual Chart Reasoning Ability from LLMs to MLLMs...\n",
            "         âœ“ Full abstract: 193 words\n",
            "      ğŸ“– Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs...\n",
            "         âœ“ Full abstract: 110 words\n",
            "      ğŸ“– Steering Knowledge Selection Behaviours in LLMs via SAE-Base...\n",
            "         âœ“ Full abstract: 182 words\n",
            "      ğŸ“– Why Does the Effective Context Length of LLMs Fall Short?...\n",
            "         âœ“ Full abstract: 172 words\n",
            "      Progress: 10/29 papers processed\n",
            "      ğŸ“– Taipan: Efficient and Expressive State Space Language Models...\n",
            "         âœ“ Full abstract: 154 words\n",
            "      ğŸ“– SMITE: Segment Me In TimE...\n",
            "         âœ“ Full abstract: 92 words\n",
            "      ğŸ“– MotionCLR: Motion Generation and Training-free Editing via U...\n",
            "         âœ“ Full abstract: 191 words\n",
            "      ğŸ“– WAFFLE: Multi-Modal Model for Automated Front-End Developmen...\n",
            "         âœ“ Full abstract: 159 words\n",
            "      ğŸ“– CAMEL-Bench: A Comprehensive Arabic LMM Benchmark...\n",
            "         âœ“ Full abstract: 192 words\n",
            "      Progress: 15/29 papers processed\n",
            "      ğŸ“– DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate\n",
            "...\n",
            "         âœ“ Full abstract: 161 words\n",
            "      ğŸ“– CCI3.0-HQ: a large-scale Chinese dataset of high quality des...\n",
            "         âœ“ Full abstract: 101 words\n",
            "      ğŸ“– Stable Consistency Tuning: Understanding and Improving Consi...\n",
            "         âœ“ Full abstract: 168 words\n",
            "      ğŸ“– Robust Watermarking Using Generative Priors Against Image Ed...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      ğŸ“– Value Residual Learning For Alleviating Attention Concentrat...\n",
            "         âœ“ Full abstract: 186 words\n",
            "      Progress: 20/29 papers processed\n",
            "      ğŸ“– ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-L...\n",
            "         âœ“ Full abstract: 257 words\n",
            "      ğŸ“– Language Models are Symbolic Learners in Arithmetic...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      ğŸ“– Should We Really Edit Language Models? On the Evaluation of ...\n",
            "         âœ“ Full abstract: 219 words\n",
            "      ğŸ“– The Nature of Mathematical Modeling and Probabilistic Optimi...\n",
            "         âœ“ Full abstract: 235 words\n",
            "      ğŸ“– Asynchronous RLHF: Faster and More Efficient Off-Policy RL f...\n",
            "         âœ“ Full abstract: 203 words\n",
            "      Progress: 25/29 papers processed\n",
            "      ğŸ“– Data Scaling Laws in Imitation Learning for Robotic Manipula...\n",
            "         âœ“ Full abstract: 222 words\n",
            "      ğŸ“– ZIP-FIT: Embedding-Free Data Selection via Compression-Based...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2410.18194\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Multi-Draft Speculative Sampling: Canonical Architectures an...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2410.18234\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Pantograph: A Machine-to-Machine Interaction Interface for A...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2410.16429\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "âœ“ 2024-10-25: 26 papers\n",
            "   âŒ Request error for 2024-10-26: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-10-26\n",
            "â—‹ 2024-10-26: no papers\n",
            "   âŒ Request error for 2024-10-27: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-10-27\n",
            "â—‹ 2024-10-27: no papers\n",
            "   âŒ Request error for 2024-10-28: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-10-28\n",
            "â—‹ 2024-10-28: no papers\n",
            "   âŒ Request error for 2024-10-29: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-10-29\n",
            "â—‹ 2024-10-29: no papers\n",
            "   âŒ Request error for 2024-10-30: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-10-30\n",
            "â—‹ 2024-10-30: no papers\n",
            "      ğŸ“– CORAL: Benchmarking Multi-turn Conversational Retrieval-Augm...\n",
            "         âœ“ Full abstract: 136 words\n",
            "      ğŸ“– TokenFormer: Rethinking Transformer Scaling with Tokenized M...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– A Large Recurrent Action Model: xLSTM enables Fast Inference...\n",
            "         âœ“ Full abstract: 151 words\n",
            "      ğŸ“– ReferEverything: Towards Segmenting Everything We Can Speak ...\n",
            "         âœ“ Full abstract: 158 words\n",
            "      ğŸ“– On Memorization of Large Language Models in Logical Reasonin...\n",
            "         âœ“ Full abstract: 226 words\n",
            "      Progress: 5/11 papers processed\n",
            "      ğŸ“– Decoding Reading Goals from Eye Movements...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– Stealing User Prompts from Mixture of Experts...\n",
            "         âœ“ Full abstract: 140 words\n",
            "      ğŸ“– Toxicity of the Commons: Curating Open-Source Pre-Training D...\n",
            "         âœ“ Full abstract: 222 words\n",
            "      ğŸ“– SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Vid...\n",
            "         âœ“ Full abstract: 262 words\n",
            "      ğŸ“– Can Models Help Us Create Better Models? Evaluating LLMs as ...\n",
            "         âœ“ Full abstract: 118 words\n",
            "      Progress: 10/11 papers processed\n",
            "      ğŸ“– AutoMIR: Effective Zero-Shot Medical Information Retrieval w...\n",
            "         âœ“ Full abstract: 190 words\n",
            "âœ“ 2024-10-31: 11 papers\n",
            "      ğŸ“– Unpacking SDXL Turbo: Interpreting Text-to-Image Models with...\n",
            "         âœ“ Full abstract: 188 words\n",
            "      ğŸ“– What Happened in LLMs Layers when Trained for Fast vs. Slow ...\n",
            "         âœ“ Full abstract: 234 words\n",
            "      ğŸ“– A Pointer Network-based Approach for Joint Extraction and De...\n",
            "         âœ“ Full abstract: 143 words\n",
            "      ğŸ“– SelfCodeAlign: Self-Alignment for Code Generation...\n",
            "         âœ“ Full abstract: 249 words\n",
            "      ğŸ“– BitStack: Fine-Grained Size Control for Compressed Large Lan...\n",
            "         âœ“ Full abstract: 218 words\n",
            "      Progress: 5/16 papers processed\n",
            "      ğŸ“– Constraint Back-translation Improves Complex Instruction Fol...\n",
            "         âœ“ Full abstract: 178 words\n",
            "      ğŸ“– Language Models can Self-Lengthen to Generate Long Texts...\n",
            "         âœ“ Full abstract: 205 words\n",
            "      ğŸ“– NeuZip: Memory-Efficient Training and Inference with Dynamic...\n",
            "         âœ“ Full abstract: 125 words\n",
            "      ğŸ“– Learning Video Representations without Natural Videos...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– AAAR-1.0: Assessing AI's Potential to Assist Research...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      Progress: 10/16 papers processed\n",
            "      ğŸ“– Navigating the Unknown: A Chat-Based Collaborative Interface...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– BenchX: A Unified Benchmark Framework for Medical Vision-Lan...\n",
            "         âœ“ Full abstract: 215 words\n",
            "      ğŸ“– DELTA: Dense Efficient Long-range 3D Tracking for any video...\n",
            "         âœ“ Full abstract: 145 words\n",
            "      ğŸ“– Teaching Embodied Reinforcement Learning Agents: Informative...\n",
            "         âœ“ Full abstract: 160 words\n",
            "      ğŸ“– Minimum Entropy Coupling with Bottleneck...\n",
            "         âœ“ Full abstract: 212 words\n",
            "      Progress: 15/16 papers processed\n",
            "      ğŸ“– GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeli...\n",
            "         âœ“ Full abstract: 128 words\n",
            "âœ“ 2024-11-01: 16 papers\n",
            "      ğŸ“– Unpacking SDXL Turbo: Interpreting Text-to-Image Models with...\n",
            "         âœ“ Full abstract: 188 words\n",
            "      ğŸ“– What Happened in LLMs Layers when Trained for Fast vs. Slow ...\n",
            "         âœ“ Full abstract: 234 words\n",
            "      ğŸ“– A Pointer Network-based Approach for Joint Extraction and De...\n",
            "         âœ“ Full abstract: 143 words\n",
            "      ğŸ“– SelfCodeAlign: Self-Alignment for Code Generation...\n",
            "         âœ“ Full abstract: 249 words\n",
            "      ğŸ“– BitStack: Fine-Grained Size Control for Compressed Large Lan...\n",
            "         âœ“ Full abstract: 218 words\n",
            "      Progress: 5/16 papers processed\n",
            "      ğŸ“– Constraint Back-translation Improves Complex Instruction Fol...\n",
            "         âœ“ Full abstract: 178 words\n",
            "      ğŸ“– Language Models can Self-Lengthen to Generate Long Texts...\n",
            "         âœ“ Full abstract: 205 words\n",
            "      ğŸ“– NeuZip: Memory-Efficient Training and Inference with Dynamic...\n",
            "         âœ“ Full abstract: 125 words\n",
            "      ğŸ“– Learning Video Representations without Natural Videos...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– AAAR-1.0: Assessing AI's Potential to Assist Research...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      Progress: 10/16 papers processed\n",
            "      ğŸ“– Navigating the Unknown: A Chat-Based Collaborative Interface...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– BenchX: A Unified Benchmark Framework for Medical Vision-Lan...\n",
            "         âœ“ Full abstract: 215 words\n",
            "      ğŸ“– DELTA: Dense Efficient Long-range 3D Tracking for any video...\n",
            "         âœ“ Full abstract: 145 words\n",
            "      ğŸ“– Teaching Embodied Reinforcement Learning Agents: Informative...\n",
            "         âœ“ Full abstract: 160 words\n",
            "      ğŸ“– Minimum Entropy Coupling with Bottleneck...\n",
            "         âœ“ Full abstract: 212 words\n",
            "      Progress: 15/16 papers processed\n",
            "      ğŸ“– GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeli...\n",
            "         âœ“ Full abstract: 128 words\n",
            "âœ“ 2024-11-02: 16 papers\n",
            "      ğŸ“– Unpacking SDXL Turbo: Interpreting Text-to-Image Models with...\n",
            "         âœ“ Full abstract: 188 words\n",
            "      ğŸ“– What Happened in LLMs Layers when Trained for Fast vs. Slow ...\n",
            "         âœ“ Full abstract: 234 words\n",
            "      ğŸ“– A Pointer Network-based Approach for Joint Extraction and De...\n",
            "         âœ“ Full abstract: 143 words\n",
            "      ğŸ“– SelfCodeAlign: Self-Alignment for Code Generation...\n",
            "         âœ“ Full abstract: 249 words\n",
            "      ğŸ“– BitStack: Fine-Grained Size Control for Compressed Large Lan...\n",
            "         âœ“ Full abstract: 218 words\n",
            "      Progress: 5/16 papers processed\n",
            "      ğŸ“– Constraint Back-translation Improves Complex Instruction Fol...\n",
            "         âœ“ Full abstract: 178 words\n",
            "      ğŸ“– Language Models can Self-Lengthen to Generate Long Texts...\n",
            "         âœ“ Full abstract: 205 words\n",
            "      ğŸ“– NeuZip: Memory-Efficient Training and Inference with Dynamic...\n",
            "         âœ“ Full abstract: 125 words\n",
            "      ğŸ“– Learning Video Representations without Natural Videos...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– AAAR-1.0: Assessing AI's Potential to Assist Research...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      Progress: 10/16 papers processed\n",
            "      ğŸ“– Navigating the Unknown: A Chat-Based Collaborative Interface...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– BenchX: A Unified Benchmark Framework for Medical Vision-Lan...\n",
            "         âœ“ Full abstract: 215 words\n",
            "      ğŸ“– DELTA: Dense Efficient Long-range 3D Tracking for any video...\n",
            "         âœ“ Full abstract: 145 words\n",
            "      ğŸ“– Teaching Embodied Reinforcement Learning Agents: Informative...\n",
            "         âœ“ Full abstract: 160 words\n",
            "      ğŸ“– Minimum Entropy Coupling with Bottleneck...\n",
            "         âœ“ Full abstract: 212 words\n",
            "      Progress: 15/16 papers processed\n",
            "      ğŸ“– GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeli...\n",
            "         âœ“ Full abstract: 128 words\n",
            "âœ“ 2024-11-03: 16 papers\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Progress: 20 days, 248 papers total\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "      ğŸ“– OS-ATLAS: A Foundation Action Model for Generalist GUI Agent...\n",
            "         âœ“ Full abstract: 193 words\n",
            "      ğŸ“– Personalization of Large Language Models: A Survey...\n",
            "         âœ“ Full abstract: 197 words\n",
            "      ğŸ“– Constant Acceleration Flow...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– TOMATO: Assessing Visual Temporal Reasoning Capabilities in ...\n",
            "         âœ“ Full abstract: 255 words\n",
            "      ğŸ“– Randomized Autoregressive Visual Generation...\n",
            "         âœ“ Full abstract: 169 words\n",
            "      Progress: 5/20 papers processed\n",
            "      ğŸ“– DynaMath: A Dynamic Visual Benchmark for Evaluating Mathemat...\n",
            "         âœ“ Full abstract: 279 words\n",
            "      ğŸ“– Physics in Next-token Prediction...\n",
            "         âœ“ Full abstract: 96 words\n",
            "      ğŸ“– GPT or BERT: why not both?...\n",
            "         âœ“ Full abstract: 84 words\n",
            "      ğŸ“– Survey of User Interface Design and Interaction Techniques i...\n",
            "         âœ“ Full abstract: 159 words\n",
            "      ğŸ“– Fashion-VDM: Video Diffusion Model for Virtual Try-On...\n",
            "         âœ“ Full abstract: 155 words\n",
            "      Progress: 10/20 papers processed\n",
            "      ğŸ“– In-Context LoRA for Diffusion Transformers...\n",
            "         âœ“ Full abstract: 211 words\n",
            "      ğŸ“– CityGaussianV2: Efficient and Geometrically Accurate Reconst...\n",
            "         âœ“ Full abstract: 182 words\n",
            "      ğŸ“– Zipfian Whitening...\n",
            "         âœ“ Full abstract: 182 words\n",
            "      ğŸ“– Face Anonymization Made Simple...\n",
            "         âœ“ Full abstract: 138 words\n",
            "      ğŸ“– LIBMoE: A Library for comprehensive benchmarking Mixture of ...\n",
            "         âœ“ Full abstract: 169 words\n",
            "      Progress: 15/20 papers processed\n",
            "      ğŸ“– HelloMeme: Integrating Spatial Knitting Attentions to Embed ...\n",
            "         âœ“ Full abstract: 110 words\n",
            "      ğŸ“– GRS-QA -- Graph Reasoning-Structured Question Answering Data...\n",
            "         âœ“ Full abstract: 147 words\n",
            "      ğŸ“– SambaMixer: State of Health Prediction of Li-ion Batteries u...\n",
            "         âœ“ Full abstract: 159 words\n",
            "      ğŸ“– M2rc-Eval: Massively Multilingual Repository-level Code Comp...\n",
            "         âœ“ Full abstract: 161 words\n",
            "      ğŸ“– WikiNER-fr-gold: A Gold-Standard NER Corpus...\n",
            "         âœ“ Full abstract: 131 words\n",
            "      Progress: 20/20 papers processed\n",
            "âœ“ 2024-11-04: 20 papers\n",
            "      ğŸ“– \"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-...\n",
            "         âœ“ Full abstract: 239 words\n",
            "      ğŸ“– AndroidLab: Training and Systematic Benchmarking of Android ...\n",
            "         âœ“ Full abstract: 148 words\n",
            "      ğŸ“– WebRL: Training LLM Web Agents via Self-Evolving Online Curr...\n",
            "         âœ“ Full abstract: 208 words\n",
            "      ğŸ“– DynaSaur: Large Language Agents Beyond Predefined Actions...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      ğŸ“– How Far is Video Generation from World Model: A Physical Law...\n",
            "         âœ“ Full abstract: 245 words\n",
            "      Progress: 5/21 papers processed\n",
            "      ğŸ“– Training-free Regional Prompting for Diffusion Transformers...\n",
            "         âœ“ Full abstract: 124 words\n",
            "      ğŸ“– Hunyuan-Large: An Open-Source MoE Model with 52 Billion Acti...\n",
            "         âœ“ Full abstract: 166 words\n",
            "      ğŸ“– MVPaint: Synchronized Multi-View Diffusion for Painting Anyt...\n",
            "         âœ“ Full abstract: 238 words\n",
            "      ğŸ“– Survey of Cultural Awareness in Language Models: Text and Be...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– Adaptive Caching for Faster Video Generation with Diffusion ...\n",
            "         âœ“ Full abstract: 169 words\n",
            "      Progress: 10/21 papers processed\n",
            "      ğŸ“– GenXD: Generating Any 3D and 4D Scenes...\n",
            "         âœ“ Full abstract: 203 words\n",
            "      ğŸ“– AutoVFX: Physically Realistic Video Editing from Natural Lan...\n",
            "         âœ“ Full abstract: 129 words\n",
            "      ğŸ“– Sparsing Law: Towards Large Language Models with Greater Act...\n",
            "         âœ“ Full abstract: 254 words\n",
            "      ğŸ“– PPLLaVA: Varied Video Sequence Understanding With Prompt Gui...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.02327\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– SALSA: Soup-based Alignment Learning for Stronger Adaptation...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.01798\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– IGOR: Image-GOal Representations are the Atomic Control Unit...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.00785\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Decoding Dark Matter: Specialized Sparse Autoencoders for In...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.00743\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Multi-expert Prompting Improves Reliability, Safety, and Use...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.00492\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Constrained Diffusion Implicit Models...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.00359\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Li...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.01192\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– LoRA-Contextualizing Adaptation of Large Multimodal Models f...\n",
            "         âœ“ Full abstract: 133 words\n",
            "âœ“ 2024-11-05: 14 papers\n",
            "      ğŸ“– HtmlRAG: HTML is Better Than Plain Text for Modeling Retriev...\n",
            "         âœ“ Full abstract: 252 words\n",
            "      ğŸ“– LLaMo: Large Language Model-based Molecular Graph Assistant...\n",
            "         âœ“ Full abstract: 167 words\n",
            "      ğŸ“– Controlling Language and Diffusion Models by Transporting Ac...\n",
            "         âœ“ Full abstract: 166 words\n",
            "      ğŸ“– Adaptive Length Image Tokenization via Recurrent Allocation...\n",
            "         âœ“ Full abstract: 160 words\n",
            "      ğŸ“– DeeR-VLA: Dynamic Inference of Multimodal Large Language Mod...\n",
            "         âœ“ Full abstract: 232 words\n",
            "      Progress: 5/11 papers processed\n",
            "      ğŸ“– Sample-Efficient Alignment for LLMs...\n",
            "         âœ“ Full abstract: 162 words\n",
            "      ğŸ“– DreamPolish: Domain Score Distillation With Progressive Geom...\n",
            "         âœ“ Full abstract: 224 words\n",
            "      ğŸ“– GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a ...\n",
            "         âœ“ Full abstract: 268 words\n",
            "      ğŸ“– Inference Optimal VLMs Need Only One Visual Token but Larger...\n",
            "         âœ“ Full abstract: 240 words\n",
            "      ğŸ“– Zebra-Llama: A Context-Aware Large Language Model for Democr...\n",
            "         âœ“ Full abstract: 249 words\n",
            "      Progress: 10/11 papers processed\n",
            "      ğŸ“– Correlation of Object Detection Performance with Visual Sali...\n",
            "         âœ“ Full abstract: 172 words\n",
            "âœ“ 2024-11-06: 11 papers\n",
            "      ğŸ“– ReCapture: Generative Video Camera Controls for User-Provide...\n",
            "         âœ“ Full abstract: 144 words\n",
            "      ğŸ“– Large Language Models Orchestrating Structured Reasoning Ach...\n",
            "         âœ“ Full abstract: 271 words\n",
            "      ğŸ“– Both Text and Images Leaked! A Systematic Analysis of Multim...\n",
            "         âœ“ Full abstract: 145 words\n",
            "      ğŸ“– Polynomial Composition Activations: Unleashing the Dynamics ...\n",
            "         âœ“ Full abstract: 201 words\n",
            "      ğŸ“– TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset ...\n",
            "         âœ“ Full abstract: 202 words\n",
            "      Progress: 5/7 papers processed\n",
            "      ğŸ“– Self-Consistency Preference Optimization...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– From Medprompt to o1: Exploration of Run-Time Strategies for...\n",
            "         âœ“ Full abstract: 261 words\n",
            "âœ“ 2024-11-07: 7 papers\n",
            "      ğŸ“– OpenCoder: The Open Cookbook for Top-Tier Code Large Languag...\n",
            "         âœ“ Full abstract: 237 words\n",
            "      ğŸ“– BitNet a4.8: 4-bit Activations for 1-bit LLMs...\n",
            "         âœ“ Full abstract: 136 words\n",
            "      ğŸ“– DimensionX: Create Any 3D and 4D Scenes from a Single Image ...\n",
            "         âœ“ Full abstract: 201 words\n",
            "      ğŸ“– Mixture-of-Transformers: A Sparse and Scalable Architecture ...\n",
            "         âœ“ Full abstract: 231 words\n",
            "      ğŸ“– M3DocRAG: Multi-modal Retrieval is What You Need for Multi-p...\n",
            "         âœ“ Full abstract: 262 words\n",
            "      Progress: 5/16 papers processed\n",
            "      ğŸ“– Analyzing The Language of Visual Tokens...\n",
            "         âœ“ Full abstract: 207 words\n",
            "      ğŸ“– VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual ...\n",
            "         âœ“ Full abstract: 184 words\n",
            "      ğŸ“– Thanos: Enhancing Conversational Agents with Skill-of-Mind-I...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      ğŸ“– SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bi...\n",
            "         âœ“ Full abstract: 277 words\n",
            "      ğŸ“– Needle Threading: Can LLMs Follow Threads through Near-Milli...\n",
            "         âœ“ Full abstract: 207 words\n",
            "      Progress: 10/16 papers processed\n",
            "      ğŸ“– DynaMem: Online Dynamic Spatio-Semantic Memory for Open Worl...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      ğŸ“– RetrieveGPT: Merging Prompts and Mathematical Models for Enh...\n",
            "         âœ“ Full abstract: 186 words\n",
            "      ğŸ“– M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmar...\n",
            "         âœ“ Full abstract: 159 words\n",
            "      ğŸ“– SG-I2V: Self-Guided Trajectory Control in Image-to-Video Gen...\n",
            "         âœ“ Full abstract: 141 words\n",
            "      ğŸ“– GazeGen: Gaze-Driven User Interaction for Visual Content Gen...\n",
            "         âœ“ Full abstract: 289 words\n",
            "      Progress: 15/16 papers processed\n",
            "      ğŸ“– Diff-2-in-1: Bridging Generation and Dense Perception with D...\n",
            "         âœ“ Full abstract: 159 words\n",
            "âœ“ 2024-11-08: 16 papers\n",
            "      ğŸ“– OpenCoder: The Open Cookbook for Top-Tier Code Large Languag...\n",
            "         âœ“ Full abstract: 237 words\n",
            "      ğŸ“– BitNet a4.8: 4-bit Activations for 1-bit LLMs...\n",
            "         âœ“ Full abstract: 136 words\n",
            "      ğŸ“– DimensionX: Create Any 3D and 4D Scenes from a Single Image ...\n",
            "         âœ“ Full abstract: 201 words\n",
            "      ğŸ“– Mixture-of-Transformers: A Sparse and Scalable Architecture ...\n",
            "         âœ“ Full abstract: 231 words\n",
            "      ğŸ“– M3DocRAG: Multi-modal Retrieval is What You Need for Multi-p...\n",
            "         âœ“ Full abstract: 262 words\n",
            "      Progress: 5/16 papers processed\n",
            "      ğŸ“– Analyzing The Language of Visual Tokens...\n",
            "         âœ“ Full abstract: 207 words\n",
            "      ğŸ“– VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual ...\n",
            "         âœ“ Full abstract: 184 words\n",
            "      ğŸ“– Thanos: Enhancing Conversational Agents with Skill-of-Mind-I...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      ğŸ“– SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bi...\n",
            "         âœ“ Full abstract: 277 words\n",
            "      ğŸ“– Needle Threading: Can LLMs Follow Threads through Near-Milli...\n",
            "         âœ“ Full abstract: 207 words\n",
            "      Progress: 10/16 papers processed\n",
            "      ğŸ“– DynaMem: Online Dynamic Spatio-Semantic Memory for Open Worl...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      ğŸ“– RetrieveGPT: Merging Prompts and Mathematical Models for Enh...\n",
            "         âœ“ Full abstract: 186 words\n",
            "      ğŸ“– M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmar...\n",
            "         âœ“ Full abstract: 159 words\n",
            "      ğŸ“– SG-I2V: Self-Guided Trajectory Control in Image-to-Video Gen...\n",
            "         âœ“ Full abstract: 141 words\n",
            "      ğŸ“– GazeGen: Gaze-Driven User Interaction for Visual Content Gen...\n",
            "         âœ“ Full abstract: 289 words\n",
            "      Progress: 15/16 papers processed\n",
            "      ğŸ“– Diff-2-in-1: Bridging Generation and Dense Perception with D...\n",
            "         âœ“ Full abstract: 159 words\n",
            "âœ“ 2024-11-09: 16 papers\n",
            "      ğŸ“– StdGEN: Semantic-Decomposed 3D Character Generation from Sin...\n",
            "         âœ“ Full abstract: 186 words\n",
            "âœ“ 2024-11-10: 1 papers\n",
            "      ğŸ“– LLM2CLIP: Powerful Language Model Unlock Richer Visual Repre...\n",
            "         âœ“ Full abstract: 265 words\n",
            "      ğŸ“– Language Models are Hidden Reasoners: Unlocking Latent Reaso...\n",
            "         âœ“ Full abstract: 162 words\n",
            "      ğŸ“– Balancing Pipeline Parallelism with Vocabulary Parallelism...\n",
            "         âœ“ Full abstract: 199 words\n",
            "      ğŸ“– DELIFT: Data Efficient Language model Instruction Fine Tunin...\n",
            "         âœ“ Full abstract: 190 words\n",
            "      ğŸ“– CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation ...\n",
            "         âœ“ Full abstract: 221 words\n",
            "      Progress: 5/10 papers processed\n",
            "      ğŸ“– Parameter-Efficient Fine-Tuning of Large Language Models for...\n",
            "         âœ“ Full abstract: 211 words\n",
            "      ğŸ“– The Semantic Hub Hypothesis: Language Models Share Semantic\n",
            "...\n",
            "         âœ“ Full abstract: 193 words\n",
            "      ğŸ“– Golden Touchstone: A Comprehensive Bilingual Benchmark for E...\n",
            "         âœ“ Full abstract: 233 words\n",
            "      ğŸ“– RaVL: Discovering and Mitigating Spurious Correlations in Fi...\n",
            "         âœ“ Full abstract: 191 words\n",
            "      ğŸ“– Improving the detection of technical debt in Java source cod...\n",
            "         âœ“ Full abstract: 278 words\n",
            "      Progress: 10/10 papers processed\n",
            "âœ“ 2024-11-11: 10 papers\n",
            "      ğŸ“– Add-it: Training-Free Object Insertion in Images With Pretra...\n",
            "         âœ“ Full abstract: 155 words\n",
            "      ğŸ“– OmniEdit: Building Image Editing Generalist Models Through S...\n",
            "         âœ“ Full abstract: 259 words\n",
            "      ğŸ“– M-Longdoc: A Benchmark For Multimodal Super-Long Document Un...\n",
            "         âœ“ Full abstract: 200 words\n",
            "      ğŸ“– Chinese SimpleQA: A Chinese Factuality Evaluation for Large ...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– Edify Image: High-Quality Image Generation with Pixel Space ...\n",
            "         âœ“ Full abstract: 71 words\n",
            "      Progress: 5/15 papers processed\n",
            "      ğŸ“– Watermark Anything with Localized Messages...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– IOPO: Empowering LLMs with Complex Instruction Following via...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– GitChameleon: Unmasking the Version-Switching Capabilities o...\n",
            "         âœ“ Full abstract: 213 words\n",
            "      ğŸ“– Autoregressive Models in Vision: A Survey...\n",
            "         âœ“ Full abstract: 238 words\n",
            "      ğŸ“– Game-theoretic LLM: Agent Workflow for Negotiation Games...\n",
            "         âœ“ Full abstract: 238 words\n",
            "      Progress: 10/15 papers processed\n",
            "      ğŸ“– Counterfactual Generation from Language Models...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– KMM: Key Frame Mask Mamba for Extended Motion Generation...\n",
            "         âœ“ Full abstract: 203 words\n",
            "      ğŸ“– Ablation is Not Enough to Emulate DPO: How Neuron Dynamics D...\n",
            "         âœ“ Full abstract: 160 words\n",
            "      ğŸ“– Energy Efficient Protein Language Models: Leveraging Small L...\n",
            "         âœ“ Full abstract: 278 words\n",
            "      ğŸ“– NeKo: Toward Post Recognition Generative Correction Large La...\n",
            "         âœ“ Full abstract: 178 words\n",
            "      Progress: 15/15 papers processed\n",
            "âœ“ 2024-11-12: 15 papers\n",
            "      ğŸ“– Stronger Models are NOT Stronger Teachers for Instruction Tu...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      ğŸ“– JanusFlow: Harmonizing Autoregression and Rectified Flow for...\n",
            "         âœ“ Full abstract: 130 words\n",
            "      ğŸ“– SAMPart3D: Segment Any Part in 3D Objects...\n",
            "         âœ“ Full abstract: 246 words\n",
            "      ğŸ“– BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions...\n",
            "         âœ“ Full abstract: 100 words\n",
            "      ğŸ“– Scaling Properties of Diffusion Models for Perceptual Tasks...\n",
            "         âœ“ Full abstract: 103 words\n",
            "      Progress: 5/8 papers processed\n",
            "      ğŸ“– Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Genera...\n",
            "         âœ“ Full abstract: 198 words\n",
            "      ğŸ“– Acoustic Volume Rendering for Neural Impulse Response Fields...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– Hardware and Software Platform Inference...\n",
            "         âœ“ Full abstract: 265 words\n",
            "âœ“ 2024-11-13: 8 papers\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Progress: 30 days, 366 papers total\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "      ğŸ“– Large Language Models Can Self-Improve in Long-context Reaso...\n",
            "         âœ“ Full abstract: 169 words\n",
            "      ğŸ“– EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric...\n",
            "         âœ“ Full abstract: 200 words\n",
            "      ğŸ“– Direct Preference Optimization Using Sparse Feature-Level Co...\n",
            "         âœ“ Full abstract: 143 words\n",
            "      ğŸ“– CamemBERT 2.0: A Smarter French Language Model Aged to Perfe...\n",
            "         âœ“ Full abstract: 226 words\n",
            "      ğŸ“– Can sparse autoencoders be used to decompose and interpret s...\n",
            "         âœ“ Full abstract: 112 words\n",
            "      Progress: 5/7 papers processed\n",
            "      ğŸ“– PerceiverS: A Multi-Scale Perceiver with Effective Segmentat...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.08307\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Motion Control for Enhanced Complex Action Video Generation...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.08328\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "âœ“ 2024-11-14: 5 papers\n",
            "   âŒ Request error for 2024-11-15: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-11-15\n",
            "â—‹ 2024-11-15: no papers\n",
            "   âŒ Request error for 2024-11-16: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-11-16\n",
            "â—‹ 2024-11-16: no papers\n",
            "   âŒ Request error for 2024-11-17: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-11-17\n",
            "â—‹ 2024-11-17: no papers\n",
            "      ğŸ“– Region-Aware Text-to-Image Generation via Hard Binding and S...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– The Dawn of GUI Agent: A Preliminary Case Study with Claude ...\n",
            "         âœ“ Full abstract: 172 words\n",
            "      ğŸ“– Number it: Temporal Grounding Videos like Flipping Manga...\n",
            "         âœ“ Full abstract: 170 words\n",
            "      ğŸ“– Xmodel-1.5: An 1B-scale Multilingual LLM...\n",
            "         âœ“ Full abstract: 120 words\n",
            "      ğŸ“– MARS: Unleashing the Power of Variance Reduction for Trainin...\n",
            "         âœ“ Full abstract: 173 words\n",
            "      Progress: 5/5 papers processed\n",
            "âœ“ 2024-11-18: 5 papers\n",
            "      ğŸ“– Generative World Explorer...\n",
            "         âœ“ Full abstract: 196 words\n",
            "      ğŸ“– BlueLM-V-3B: Algorithm and System Co-Design for Multimodal L...\n",
            "         âœ“ Full abstract: 212 words\n",
            "      ğŸ“– AnimateAnything: Consistent and Controllable Animation for V...\n",
            "         âœ“ Full abstract: 121 words\n",
            "      ğŸ“– Search, Verify and Feedback: Towards Next Generation Post-tr...\n",
            "         âœ“ Full abstract: 137 words\n",
            "      ğŸ“– Top-nÏƒ: Not All Logits Are You Need...\n",
            "         âœ“ Full abstract: 141 words\n",
            "      Progress: 5/16 papers processed\n",
            "      ğŸ“– Drowning in Documents: Consequences of Scaling Reranker Infe...\n",
            "         âœ“ Full abstract: 108 words\n",
            "      ğŸ“– FitDiT: Advancing the Authentic Garment Details for High-fid...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      ğŸ“– SlimLM: An Efficient Small Language Model for On-Device Docu...\n",
            "         âœ“ Full abstract: 166 words\n",
            "      ğŸ“– StableV2V: Stablizing Shape Consistency in Video-to-Video Ed...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient ...\n",
            "         âœ“ Full abstract: 153 words\n",
            "      Progress: 10/16 papers processed\n",
            "      ğŸ“– Adaptive Decoding via Latent Preference Optimization...\n",
            "         âœ“ Full abstract: 135 words\n",
            "      ğŸ“– LLÃ¤Mmlein: Compact and Competitive German-Only Language Mode...\n",
            "         âœ“ Full abstract: 139 words\n",
            "      ğŸ“– SmoothCache: A Universal Inference Acceleration Technique fo...\n",
            "         âœ“ Full abstract: 141 words\n",
            "      ğŸ“– VeGaS: Video Gaussian Splatting...\n",
            "         âœ“ Full abstract: 195 words\n",
            "      ğŸ“– Comprehensive and Practical Evaluation of Retrieval-Augmente...\n",
            "         âœ“ Full abstract: 184 words\n",
            "      Progress: 15/16 papers processed\n",
            "      ğŸ“– Evaluating the role of `Constitutions' for learning from AI ...\n",
            "         âœ“ Full abstract: 136 words\n",
            "âœ“ 2024-11-19: 16 papers\n",
            "      ğŸ“– RedPajama: an Open Dataset for Training Large Language Model...\n",
            "         âœ“ Full abstract: 264 words\n",
            "      ğŸ“– FlipSketch: Flipping Static Drawings to Text-Guided Sketch A...\n",
            "         âœ“ Full abstract: 171 words\n",
            "      ğŸ“– SymDPO: Boosting In-Context Learning of Large Multimodal Mod...\n",
            "         âœ“ Full abstract: 184 words\n",
            "      ğŸ“– Continuous Speculative Decoding for Autoregressive Image Gen...\n",
            "         âœ“ Full abstract: 168 words\n",
            "      ğŸ“– ITACLIP: Boosting Training-Free Semantic Segmentation with I...\n",
            "         âœ“ Full abstract: 158 words\n",
            "      Progress: 5/9 papers processed\n",
            "      ğŸ“– Building Trust: Foundations of Security, Safety and Transpar...\n",
            "         âœ“ Full abstract: 113 words\n",
            "      ğŸ“– Soft Robotic Dynamic In-Hand Pen Spinning...\n",
            "         âœ“ Full abstract: 203 words\n",
            "      ğŸ“– SEAGULL: No-reference Image Quality Assessment for Regions o...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      ğŸ“– Evaluating Tokenizer Performance of Large Language Models Ac...\n",
            "         âœ“ Full abstract: 173 words\n",
            "âœ“ 2024-11-20: 9 papers\n",
            "      ğŸ“– SageAttention2 Technical Report: Accurate 4 Bit Attention fo...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– VBench++: Comprehensive and Versatile Benchmark Suite for Vi...\n",
            "         âœ“ Full abstract: 250 words\n",
            "      ğŸ“– Natural Language Reinforcement Learning...\n",
            "         âœ“ Full abstract: 134 words\n",
            "      ğŸ“– VideoAutoArena: An Automated Arena for Evaluating Large Mult...\n",
            "         âœ“ Full abstract: 250 words\n",
            "      ğŸ“– SAMURAI: Adapting Segment Anything Model for Zero-Shot Visua...\n",
            "         âœ“ Full abstract: 200 words\n",
            "      Progress: 5/12 papers processed\n",
            "      ğŸ“– When Precision Meets Position: BFloat16 Breaks Down RoPE in ...\n",
            "         âœ“ Full abstract: 198 words\n",
            "      ğŸ“– Is Your LLM Secretly a World Model of the Internet? Model-Ba...\n",
            "         âœ“ Full abstract: 231 words\n",
            "      ğŸ“– Stylecodes: Encoding Stylistic Information For Image Generat...\n",
            "         âœ“ Full abstract: 142 words\n",
            "      ğŸ“– ViBe: A Text-to-Video Benchmark for Evaluating Hallucination...\n",
            "         âœ“ Full abstract: 181 words\n",
            "      ğŸ“– Loss-to-Loss Prediction: Scaling Laws for All Datasets...\n",
            "         âœ“ Full abstract: 203 words\n",
            "      Progress: 10/12 papers processed\n",
            "      ğŸ“– Generating Compositional Scenes via Text-to-image RGBA Insta...\n",
            "         âœ“ Full abstract: 205 words\n",
            "      ğŸ“– ORID: Organ-Regional Information Driven Framework for Radiol...\n",
            "         âœ“ Full abstract: 172 words\n",
            "âœ“ 2024-11-21: 12 papers\n",
            "      ğŸ“– Enhancing the Reasoning Ability of Multimodal Large Language...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– Marco-o1: Towards Open Reasoning Models for Open-Ended Solut...\n",
            "         âœ“ Full abstract: 105 words\n",
            "      ğŸ“– Multimodal Autoregressive Pre-training of Large Vision Encod...\n",
            "         âœ“ Full abstract: 140 words\n",
            "      ğŸ“– Hymba: A Hybrid-head Architecture for Small Language Models...\n",
            "         âœ“ Full abstract: 140 words\n",
            "      ğŸ“– OpenScholar: Synthesizing Scientific Literature with Retriev...\n",
            "         âœ“ Full abstract: 178 words\n",
            "      Progress: 5/14 papers processed\n",
            "      ğŸ“– Insight-V: Exploring Long-Chain Visual Reasoning with Multim...\n",
            "         âœ“ Full abstract: 236 words\n",
            "      ğŸ“– Ultra-Sparse Memory Network...\n",
            "         âœ“ Full abstract: 124 words\n",
            "      ğŸ“– Stable Flow: Vital Layers for Training-Free Image Editing...\n",
            "         âœ“ Full abstract: 172 words\n",
            "      ğŸ“– DINO-X: A Unified Vision Model for Open-World Object Detecti...\n",
            "         âœ“ Full abstract: 249 words\n",
            "      ğŸ“– Do I Know This Entity? Knowledge Awareness and Hallucination...\n",
            "         âœ“ Full abstract: 202 words\n",
            "      Progress: 10/14 papers processed\n",
            "      ğŸ“– MagicDriveDiT: High-Resolution Long Video Generation for Aut...\n",
            "         âœ“ Full abstract: 138 words\n",
            "      ğŸ“– Baking Gaussian Splatting into Diffusion Denoiser for Fast a...\n",
            "         âœ“ Full abstract: 170 words\n",
            "      ğŸ“– UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptat...\n",
            "         âœ“ Full abstract: 127 words\n",
            "      ğŸ“– Patience Is The Key to Large Language Model Reasoning...\n",
            "         âœ“ Full abstract: 143 words\n",
            "âœ“ 2024-11-22: 14 papers\n",
            "      ğŸ“– Enhancing the Reasoning Ability of Multimodal Large Language...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– Marco-o1: Towards Open Reasoning Models for Open-Ended Solut...\n",
            "         âœ“ Full abstract: 105 words\n",
            "      ğŸ“– Multimodal Autoregressive Pre-training of Large Vision Encod...\n",
            "         âœ“ Full abstract: 140 words\n",
            "      ğŸ“– Hymba: A Hybrid-head Architecture for Small Language Models...\n",
            "         âœ“ Full abstract: 140 words\n",
            "      ğŸ“– OpenScholar: Synthesizing Scientific Literature with Retriev...\n",
            "         âœ“ Full abstract: 178 words\n",
            "      Progress: 5/14 papers processed\n",
            "      ğŸ“– Insight-V: Exploring Long-Chain Visual Reasoning with Multim...\n",
            "         âœ“ Full abstract: 236 words\n",
            "      ğŸ“– Ultra-Sparse Memory Network...\n",
            "         âœ“ Full abstract: 124 words\n",
            "      ğŸ“– Stable Flow: Vital Layers for Training-Free Image Editing...\n",
            "         âœ“ Full abstract: 172 words\n",
            "      ğŸ“– DINO-X: A Unified Vision Model for Open-World Object Detecti...\n",
            "         âœ“ Full abstract: 249 words\n",
            "      ğŸ“– Do I Know This Entity? Knowledge Awareness and Hallucination...\n",
            "         âœ“ Full abstract: 202 words\n",
            "      Progress: 10/14 papers processed\n",
            "      ğŸ“– MagicDriveDiT: High-Resolution Long Video Generation for Aut...\n",
            "         âœ“ Full abstract: 138 words\n",
            "      ğŸ“– Baking Gaussian Splatting into Diffusion Denoiser for Fast a...\n",
            "         âœ“ Full abstract: 170 words\n",
            "      ğŸ“– UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptat...\n",
            "         âœ“ Full abstract: 127 words\n",
            "      ğŸ“– Patience Is The Key to Large Language Model Reasoning...\n",
            "         âœ“ Full abstract: 143 words\n",
            "âœ“ 2024-11-23: 14 papers\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Progress: 40 days, 441 papers total\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "      ğŸ“– Style-Friendly SNR Sampler for Style-Driven Generation...\n",
            "         âœ“ Full abstract: 144 words\n",
            "âœ“ 2024-11-24: 1 papers\n",
            "      ğŸ“– TÃœLU 3: Pushing Frontiers in Open Language Model Post-Traini...\n",
            "         âœ“ Full abstract: 244 words\n",
            "      ğŸ“– OminiControl: Minimal and Universal Control for Diffusion Tr...\n",
            "         âœ“ Full abstract: 196 words\n",
            "      ğŸ“– Material Anything: Generating Materials for Any 3D Object vi...\n",
            "         âœ“ Full abstract: 136 words\n",
            "      ğŸ“– Large-Scale Text-to-Image Model with Inpainting is a Zero-Sh...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– MyTimeMachine: Personalized Facial Age Transformation...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      Progress: 5/16 papers processed\n",
            "      ğŸ“– A Flexible Large Language Models Guardrail Development Metho...\n",
            "         âœ“ Full abstract: 171 words\n",
            "      ğŸ“– Large Multi-modal Models Can Interpret Features in Large Mul...\n",
            "         âœ“ Full abstract: 172 words\n",
            "      ğŸ“– BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games...\n",
            "         âœ“ Full abstract: 206 words\n",
            "      ğŸ“– VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fi...\n",
            "         âœ“ Full abstract: 205 words\n",
            "      ğŸ“– Efficient Long Video Tokenization via Coordinated-based Patc...\n",
            "         âœ“ Full abstract: 203 words\n",
            "      Progress: 10/16 papers processed\n",
            "      ğŸ“– Novel View Extrapolation with Video Diffusion Priors...\n",
            "         âœ“ Full abstract: 165 words\n",
            "      ğŸ“– VideoRepair: Improving Text-to-Video Generation via Misalign...\n",
            "         âœ“ Full abstract: 190 words\n",
            "      ğŸ“– WildLMa: Long Horizon Loco-Manipulation in the Wild...\n",
            "         âœ“ Full abstract: 191 words\n",
            "      ğŸ“– The Impossible Test: A 2024 Unsolvable Dataset and A Chance ...\n",
            "         âœ“ Full abstract: 238 words\n",
            "      ğŸ“– Adapting Vision Foundation Models for Robust Cloud Segmentat...\n",
            "         âœ“ Full abstract: 191 words\n",
            "      Progress: 15/16 papers processed\n",
            "      ğŸ“– One to rule them all: natural language to bind communication...\n",
            "         âœ“ Full abstract: 268 words\n",
            "âœ“ 2024-11-25: 16 papers\n",
            "      ğŸ“– Star Attention: Efficient LLM Inference over Long Sequences...\n",
            "         âœ“ Full abstract: 109 words\n",
            "      ğŸ“– O1 Replication Journey -- Part 2: Surpassing O1-preview thro...\n",
            "         âœ“ Full abstract: 251 words\n",
            "      ğŸ“– From Generation to Judgment: Opportunities and Challenges of...\n",
            "         âœ“ Full abstract: 167 words\n",
            "      ğŸ“– GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A\n",
            "...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.14522\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Reflections from the 2024 Large Language Model (LLM) Hackath...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.15221\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– One Diffusion to Generate Them All...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.16318\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– MH-MoE:Multi-Head Mixture-of-Experts...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.16205\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Interactive Medical Image Segmentation: A Benchmark Dataset ...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.12814\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– SegBook: A Simple Baseline and Cookbook for Volumetric Medic...\n",
            "         âœ“ Full abstract: 239 words\n",
            "      ğŸ“– DreamRunner: Fine-Grained Storytelling Video Generation with...\n",
            "         âœ“ Full abstract: 229 words\n",
            "      Progress: 10/24 papers processed\n",
            "      ğŸ“– Cautious Optimizers: Improving Training with One Line of Cod...\n",
            "         âœ“ Full abstract: 115 words\n",
            "      ğŸ“– Factorized Visual Tokenization and Generation...\n",
            "         âœ“ Full abstract: 200 words\n",
            "      ğŸ“– VisualLens: Personalization through Visual History...\n",
            "         âœ“ Full abstract: 154 words\n",
            "      ğŸ“– TEXGen: a Generative Diffusion Model for Mesh Textures...\n",
            "         âœ“ Full abstract: 163 words\n",
            "      ğŸ“– Knowledge Transfer Across Modalities with Natural Language S...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      Progress: 15/24 papers processed\n",
            "      ğŸ“– From CISC to RISC: language-model guided assembly transpilat...\n",
            "         âœ“ Full abstract: 187 words\n",
            "      ğŸ“– All Languages Matter: Evaluating LMMs on Culturally Diverse ...\n",
            "         âœ“ Full abstract: 234 words\n",
            "      ğŸ“– SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian S...\n",
            "         âœ“ Full abstract: 198 words\n",
            "      ğŸ“– LLMs Do Not Think Step-by-step In Implicit Reasoning...\n",
            "         âœ“ Full abstract: 151 words\n",
            "      ğŸ“– Predicting Emergent Capabilities by Finetuning...\n",
            "         âœ“ Full abstract: 220 words\n",
            "      Progress: 20/24 papers processed\n",
            "      ğŸ“– Best of Both Worlds: Advantages of Hybrid Graph Sequence Mod...\n",
            "         âœ“ Full abstract: 271 words\n",
            "      ğŸ“– DreamMix: Decoupling Object Attributes for Enhanced Editabil...\n",
            "         âœ“ Full abstract: 153 words\n",
            "      ğŸ“– Find Any Part in 3D...\n",
            "         âœ“ Full abstract: 156 words\n",
            "      ğŸ“– Edge Weight Prediction For Category-Agnostic Pose Estimation...\n",
            "         âœ“ Full abstract: 164 words\n",
            "âœ“ 2024-11-26: 19 papers\n",
            "      ğŸ“– ShowUI: One Vision-Language-Action Model for GUI Visual Agen...\n",
            "         âœ“ Full abstract: 226 words\n",
            "      ğŸ“– ROICtrl: Boosting Instance Control for Visual Generation...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– Identity-Preserving Text-to-Video Generation by Frequency De...\n",
            "         âœ“ Full abstract: 249 words\n",
            "      ğŸ“– Pathways on the Image Manifold: Image Editing via Video Gene...\n",
            "         âœ“ Full abstract: 138 words\n",
            "      ğŸ“– MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelit...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      Progress: 5/20 papers processed\n",
            "      ğŸ“– Interleaved Scene Graph for Interleaved Text-and-Image Gener...\n",
            "         âœ“ Full abstract: 228 words\n",
            "      ğŸ“– MME-Survey: A Comprehensive Survey on Evaluation of Multimod...\n",
            "         âœ“ Full abstract: 206 words\n",
            "      ğŸ“– Rethinking Token Reduction in MLLMs: Towards a Unified Parad...\n",
            "         âœ“ Full abstract: 155 words\n",
            "      ğŸ“– SketchAgent: Language-Driven Sequential Sketch Generation...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws ...\n",
            "         âœ“ Full abstract: 226 words\n",
            "      Progress: 10/20 papers processed\n",
            "      ğŸ“– SAR3D: Autoregressive 3D Object Generation and Understanding...\n",
            "         âœ“ Full abstract: 160 words\n",
            "      ğŸ“– VLRewardBench: A Challenging Benchmark for Vision-Language G...\n",
            "         âœ“ Full abstract: 211 words\n",
            "      ğŸ“– SALOVA: Segment-Augmented Long Video Assistant for Targeted ...\n",
            "         âœ“ Full abstract: 211 words\n",
            "      ğŸ“– Learning 3D Representations from Procedural 3D Programs...\n",
            "         âœ“ Full abstract: 131 words\n",
            "      ğŸ“– FINECAPTION: Compositional Image Captioning Focusing on Wher...\n",
            "         âœ“ Full abstract: 200 words\n",
            "      Progress: 15/20 papers processed\n",
            "      ğŸ“– AnchorCrafter: Animate CyberAnchors Saling Your Products via...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– EfficientViM: Efficient Vision Mamba with Hidden State Mixer...\n",
            "         âœ“ Full abstract: 199 words\n",
            "      ğŸ“– MolReFlect: Towards In-Context Fine-grained Alignments betwe...\n",
            "         âœ“ Full abstract: 247 words\n",
            "      ğŸ“– Controllable Human Image Generation with Personalized Multi-...\n",
            "         âœ“ Full abstract: 196 words\n",
            "      ğŸ“– Visual Counter Turing Test (VCT^2): Discovering the Challeng...\n",
            "         âœ“ Full abstract: 242 words\n",
            "      Progress: 20/20 papers processed\n",
            "âœ“ 2024-11-27: 20 papers\n",
            "      ğŸ“– CAT4D: Create Anything in 4D with Multi-View Video Diffusion...\n",
            "         âœ“ Full abstract: 108 words\n",
            "      ğŸ“– Large Language Model-Brained GUI Agents: A Survey...\n",
            "         âœ“ Full abstract: 271 words\n",
            "      ğŸ“– Diffusion Self-Distillation for Zero-Shot Customized Image G...\n",
            "         âœ“ Full abstract: 163 words\n",
            "      ğŸ“– 3D Convex Splatting: Radiance Field Rendering with 3D Smooth...\n",
            "         âœ“ Full abstract: 219 words\n",
            "      ğŸ“– DiffusionDrive: Truncated Diffusion Model for End-to-End Aut...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      Progress: 5/19 papers processed\n",
            "      ğŸ“– Make-It-Animatable: An Efficient Framework for Authoring Ani...\n",
            "         âœ“ Full abstract: 184 words\n",
            "      ğŸ“– UniPose: A Unified Multimodal Framework for Human Pose Compr...\n",
            "         âœ“ Full abstract: 177 words\n",
            "      ğŸ“– Collaborative Decoding Makes Visual Auto-Regressive Modeling...\n",
            "         âœ“ Full abstract: 230 words\n",
            "      ğŸ“– DreamCache: Finetuning-Free Lightweight Personalized Image G...\n",
            "         âœ“ Full abstract: 125 words\n",
            "      ğŸ“– ChatRex: Taming Multimodal LLM for Joint Perception and Unde...\n",
            "         âœ“ Full abstract: 204 words\n",
            "      Progress: 10/19 papers processed\n",
            "      ğŸ“– Video-Guided Foley Sound Generation with Multimodal Controls...\n",
            "         âœ“ Full abstract: 163 words\n",
            "      ğŸ“– Omegance: A Single Parameter for Various Granularities in\n",
            "  ...\n",
            "         âœ“ Full abstract: 153 words\n",
            "      ğŸ“– Draft Model Knows When to Stop: A Self-Verification Length P...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video...\n",
            "         âœ“ Full abstract: 264 words\n",
            "      ğŸ“– Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      Progress: 15/19 papers processed\n",
            "      ğŸ“– Adaptive Blind All-in-One Image Restoration...\n",
            "         âœ“ Full abstract: 208 words\n",
            "      ğŸ“– Training and Evaluating Language Models with Template-based ...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      ğŸ“– Edit Away and My Face Will not Stay: Personal Biometric Defe...\n",
            "         âœ“ Full abstract: 169 words\n",
            "      ğŸ“– Morph: A Motion-free Physics Optimization Framework for Huma...\n",
            "         âœ“ Full abstract: 159 words\n",
            "âœ“ 2024-11-28: 19 papers\n",
            "      ğŸ“– Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Re...\n",
            "         âœ“ Full abstract: 254 words\n",
            "      ğŸ“– TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Recons...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– ChatGen: Automatic Text-to-Image Generation From FreeStyle C...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaus...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      ğŸ“– Free^2Guide: Gradient-Free Path Integral Control for Enhanci...\n",
            "         âœ“ Full abstract: 154 words\n",
            "      Progress: 5/7 papers processed\n",
            "      ğŸ“– LongKey: Keyphrase Extraction for Long Documents...\n",
            "         âœ“ Full abstract: 130 words\n",
            "      ğŸ“– AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question...\n",
            "         âœ“ Full abstract: 180 words\n",
            "âœ“ 2024-11-29: 7 papers\n",
            "      ğŸ“– Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Re...\n",
            "         âœ“ Full abstract: 254 words\n",
            "      ğŸ“– TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Recons...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– ChatGen: Automatic Text-to-Image Generation From FreeStyle C...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaus...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      ğŸ“– Free^2Guide: Gradient-Free Path Integral Control for Enhanci...\n",
            "         âœ“ Full abstract: 154 words\n",
            "      Progress: 5/7 papers processed\n",
            "      ğŸ“– LongKey: Keyphrase Extraction for Long Documents...\n",
            "         âœ“ Full abstract: 130 words\n",
            "      ğŸ“– AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question...\n",
            "         âœ“ Full abstract: 180 words\n",
            "âœ“ 2024-11-30: 7 papers\n",
            "      ğŸ“– Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Re...\n",
            "         âœ“ Full abstract: 254 words\n",
            "      ğŸ“– TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Recons...\n",
            "         âœ“ Full abstract: 164 words\n",
            "      ğŸ“– ChatGen: Automatic Text-to-Image Generation From FreeStyle C...\n",
            "         âœ“ Full abstract: 176 words\n",
            "      ğŸ“– SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaus...\n",
            "         âœ“ Full abstract: 179 words\n",
            "      ğŸ“– Free^2Guide: Gradient-Free Path Integral Control for Enhanci...\n",
            "         âœ“ Full abstract: 154 words\n",
            "      Progress: 5/7 papers processed\n",
            "      ğŸ“– LongKey: Keyphrase Extraction for Long Documents...\n",
            "         âœ“ Full abstract: 130 words\n",
            "      ğŸ“– AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question...\n",
            "         âœ“ Full abstract: 180 words\n",
            "âœ“ 2024-12-01: 7 papers\n",
            "      ğŸ“– GRAPE: Generalizing Robot Policy via Preference Alignment...\n",
            "         âœ“ Full abstract: 241 words\n",
            "      ğŸ“– Video Depth without Video Models...\n",
            "         âœ“ Full abstract: 230 words\n",
            "      ğŸ“– Beyond Examples: High-level Automated Reasoning Paradigm in ...\n",
            "         âœ“ Full abstract: 158 words\n",
            "      ğŸ“– On Domain-Specific Post-Training for Multimodal Large Langua...\n",
            "         âœ“ Full abstract: 178 words\n",
            "      ğŸ“– Yi-Lightning Technical Report...\n",
            "         âœ“ Full abstract: 197 words\n",
            "      Progress: 5/21 papers processed\n",
            "      ğŸ“– Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sa...\n",
            "         âœ“ Full abstract: 162 words\n",
            "      ğŸ“– Reverse Thinking Makes LLMs Stronger Reasoners...\n",
            "         âœ“ Full abstract: 224 words\n",
            "      ğŸ“– Timestep Embedding Tells: It's Time to Cache for Video Diffu...\n",
            "         âœ“ Full abstract: 191 words\n",
            "      ğŸ“– FAM Diffusion: Frequency and Attention Modulation for High-R...\n",
            "         âœ“ Full abstract: 177 words\n",
            "      ğŸ“– Puzzle: Distillation-Based NAS for Inference-Optimized LLMs...\n",
            "         âœ“ Full abstract: 246 words\n",
            "      Progress: 10/21 papers processed\n",
            "      ğŸ“– Trajectory Attention for Fine-grained Video Motion Control...\n",
            "         âœ“ Full abstract: 181 words\n",
            "      ğŸ“– Scaling Transformers for Low-Bitrate High-Quality Speech Cod...\n",
            "         âœ“ Full abstract: 105 words\n",
            "      ğŸ“– DisCoRD: Discrete Tokens to Continuous Motion via Rectified ...\n",
            "         âœ“ Full abstract: 174 words\n",
            "      ğŸ“– Look Every Frame All at Once: Video-Ma^2mba for Efficient Lo...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– MATATA: a weak-supervised MAthematical Tool-Assisted reasoni...\n",
            "         âœ“ Full abstract: 135 words\n",
            "      Progress: 15/21 papers processed\n",
            "      ğŸ“– AC3D: Analyzing and Improving 3D Camera Control in Video Dif...\n",
            "         âœ“ Full abstract: 224 words\n",
            "      ğŸ“– AlphaTablets: A Generic Plane Representation for 3D Planar\n",
            " ...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.19950\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– DeMo: Decoupled Momentum Optimization...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.19870\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– LLM Teacher-Student Framework for Text Classification With N...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.19638\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– SpotLight: Shadow-Guided Object Relighting via Diffusion...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.18665\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "      ğŸ“– Training Noise Token Pruning...\n",
            "         âŒ Request error: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/2411.18092\n",
            "         âš ï¸ Detail page failed, using preview\n",
            "âœ“ 2024-12-02: 16 papers\n",
            "   âŒ Request error for 2024-12-03: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-12-03\n",
            "â—‹ 2024-12-03: no papers\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Progress: 50 days, 553 papers total\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "   âŒ Request error for 2024-12-04: 429 Client Error: Too Many Requests for url: https://huggingface.co/papers/date/2024-12-04\n",
            "â—‹ 2024-12-04: no papers\n",
            "      ğŸ“– PaliGemma 2: A Family of Versatile VLMs for Transfer...\n",
            "         âœ“ Full abstract: 167 words\n",
            "      ğŸ“– SNOOPI: Supercharged One-step Diffusion Distillation with Pr...\n",
            "         âœ“ Full abstract: 217 words\n",
            "      ğŸ“– TokenFlow: Unified Image Tokenizer for Multimodal Understand...\n",
            "         âœ“ Full abstract: 181 words\n",
            "      ğŸ“– Imagine360: Immersive 360 Video Generation from Perspective ...\n",
            "         âœ“ Full abstract: 199 words\n",
            "      ğŸ“– Distilling Diffusion Models to Efficient 3D LiDAR Scene Comp...\n",
            "         âœ“ Full abstract: 168 words\n",
            "      Progress: 5/19 papers processed\n",
            "      ğŸ“– One Shot, One Talk: Whole-body Talking Avatar from a Single ...\n",
            "         âœ“ Full abstract: 143 words\n",
            "      ğŸ“– VideoICL: Confidence-based Iterative In-context Learning for...\n",
            "         âœ“ Full abstract: 210 words\n",
            "      ğŸ“– VARCO-VISION: Expanding Frontiers in Korean Vision-Language ...\n",
            "         âœ“ Full abstract: 120 words\n",
            "      ğŸ“– MIDI: Multi-Instance Diffusion for Single Image to 3D Scene ...\n",
            "         âœ“ Full abstract: 167 words\n",
            "      ğŸ“– NVComposer: Boosting Generative Novel View Synthesis with Mu...\n",
            "         âœ“ Full abstract: 172 words\n",
            "      Progress: 10/19 papers processed\n",
            "      ğŸ“– NitroFusion: High-Fidelity Single-Step Diffusion through Dyn...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      ğŸ“– U-MATH: A University-Level Benchmark for Evaluating Mathemat...\n",
            "         âœ“ Full abstract: 170 words\n",
            "      ğŸ“– Video-3D LLM: Learning Position-Aware Video Representation f...\n",
            "         âœ“ Full abstract: 175 words\n",
            "      ğŸ“– Surveying the Effects of Quality, Diversity, and Complexity ...\n",
            "         âœ“ Full abstract: 275 words\n",
            "      ğŸ“– CleanDIFT: Diffusion Features without Noise...\n",
            "         âœ“ Full abstract: 151 words\n",
            "      Progress: 15/19 papers processed\n",
            "      ğŸ“– Weighted-Reward Preference Optimization for Implicit Model F...\n",
            "         âœ“ Full abstract: 189 words\n",
            "      ğŸ“– Mimir: Improving Video Diffusion Models for Precise Text Und...\n",
            "         âœ“ Full abstract: 190 words\n",
            "      ğŸ“– Inst-IT: Boosting Multimodal Instance Understanding via Expl...\n",
            "         âœ“ Full abstract: 213 words\n",
            "      ğŸ“– LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor...\n",
            "         âœ“ Full abstract: 179 words\n",
            "âœ“ 2024-12-05: 19 papers\n",
            "      ğŸ“– VisionZip: Longer is Better but Not Necessary in Vision Lang...\n",
            "         âœ“ Full abstract: 198 words\n",
            "      ğŸ“– Structured 3D Latents for Scalable and Versatile 3D Generati...\n",
            "         âœ“ Full abstract: 153 words\n",
            "      ğŸ“– Aguvis: Unified Pure Vision Agents for Autonomous GUI Intera...\n",
            "         âœ“ Full abstract: 206 words\n",
            "      ğŸ“– Florence-VL: Enhancing Vision-Language Models with Generativ...\n",
            "         âœ“ Full abstract: 222 words\n",
            "      ğŸ“– NVILA: Efficient Frontier Visual Language Models...\n",
            "         âœ“ Full abstract: 161 words\n",
            "      Progress: 5/31 papers processed\n",
            "      ğŸ“– Evaluating Language Models as Synthetic Data Generators...\n",
            "         âœ“ Full abstract: 180 words\n",
            "      ğŸ“– Code-as-Monitor: Constraint-aware Visual Programming for Rea...\n",
            "         âœ“ Full abstract: 182 words\n",
            "      ğŸ“– A Noise is Worth Diffusion Guidance...\n",
            "         âœ“ Full abstract: 162 words\n",
            "      ğŸ“– MV-Adapter: Multi-view Consistent Image Generation Made Easy...\n",
            "         âœ“ Full abstract: 227 words\n",
            "      ğŸ“– AnyDressing: Customizable Multi-Garment Virtual Dressing via...\n",
            "         âœ“ Full abstract: 232 words\n",
            "      Progress: 10/31 papers processed\n",
            "      ğŸ“– Negative Token Merging: Image-based Adversarial Feature Guid...\n",
            "         âœ“ Full abstract: 207 words\n",
            "      ğŸ“– Global MMLU: Understanding and Addressing Cultural and Lingu...\n",
            "         âœ“ Full abstract: 252 words\n",
            "      ğŸ“– Densing Law of LLMs...\n",
            "         âœ“ Full abstract: 250 words\n",
            "      ğŸ“– Infinity: Scaling Bitwise AutoRegressive Modeling for High-R...\n",
            "         âœ“ Full abstract: 165 words\n",
            "      ğŸ“– HumanEdit: A High-Quality Human-Rewarded Dataset for Instruc...\n",
            "         âœ“ Full abstract: 199 words\n",
            "      Progress: 15/31 papers processed\n",
            "      ğŸ“– Monet: Mixture of Monosemantic Experts for Transformers...\n",
            "         âœ“ Full abstract: 190 words\n",
            "      ğŸ“– Personalized Multimodal Large Language Models: A Survey...\n",
            "         âœ“ Full abstract: 168 words\n",
            "      ğŸ“– Towards Universal Soccer Video Understanding...\n",
            "         âœ“ Full abstract: 147 words\n",
            "      ğŸ“– OmniFlow: Any-to-Any Generation with Multi-Modal Rectified F...\n",
            "         âœ“ Full abstract: 171 words\n",
            "      ğŸ“– Discriminative Fine-tuning of LVLMs...\n",
            "         âœ“ Full abstract: 199 words\n",
            "      Progress: 20/31 papers processed\n",
            "      ğŸ“– Marco-LLM: Bridging Languages via Massive Multilingual Train...\n",
            "         âœ“ Full abstract: 194 words\n",
            "      ğŸ“– MEMO: Memory-Guided Diffusion for Expressive Talking Video G...\n",
            "         âœ“ Full abstract: 172 words\n",
            "      ğŸ“– ZipAR: Accelerating Autoregressive Image Generation through ...\n",
            "         âœ“ Full abstract: 139 words\n",
            "      ğŸ“– KV Shifting Attention Enhances Language Modeling...\n",
            "         âœ“ Full abstract: 131 words\n",
            "      ğŸ“– 4Real-Video: Learning Generalizable Photo-Realistic 4D Video...\n",
            "         âœ“ Full abstract: 134 words\n",
            "      Progress: 25/31 papers processed\n",
            "      ğŸ“– p-MoD: Building Mixture-of-Depths MLLMs via Progressive Rati...\n",
            "         âœ“ Full abstract: 214 words\n",
            "      ğŸ“– Scaling Inference-Time Search with Vision Value Model for Im...\n",
            "         âœ“ Full abstract: 193 words\n",
            "      ğŸ“– MRGen: Diffusion-based Controllable Data Engine for MRI Segm...\n",
            "         âœ“ Full abstract: 161 words\n",
            "      ğŸ“– SynFinTabs: A Dataset of Synthetic Financial Tables for Info...\n",
            "         âœ“ Full abstract: 202 words\n",
            "      ğŸ“– Challenges in Trustworthy Human Evaluation of Chatbots...\n",
            "         âœ“ Full abstract: 130 words\n",
            "      Progress: 30/31 papers processed\n",
            "      ğŸ“– Establishing Task Scaling Laws via Compute-Efficient Model L...\n",
            "         âœ“ Full abstract: 211 words\n",
            "âœ“ 2024-12-06: 31 papers\n"
          ]
        }
      ],
      "source": [
        "HF_TOKEN = \"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\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ”¥ SCRAPING WITH ANTI-BLOCKING PROTECTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "scraper = HuggingFaceScraper()\n",
        "\n",
        "print(\"\\nğŸ¯ Full scraping mode: Every day from Jan to Oct 2025\")\n",
        "print(\"âš ï¸  This will take 15-20 minutes but gives maximum papers!\\n\")\n",
        "\n",
        "papers = scraper.scrape_recent_days(num_days=365, fetch_full_abstract=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCBQtgxs0yy6",
        "outputId": "6056c225-b0f6-4b3c-aa65-0864315cb710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Total papers after merge: 4251\n",
            "\n",
            "================================================================================\n",
            "ğŸ’¾ SAVED TO: papers_jan_preview.json\n",
            "================================================================================\n",
            "ğŸ“Š Statistics:\n",
            "   Total papers: 4251\n",
            "   With abstracts: 4251\n",
            "   - Full abstracts: 4251\n",
            "   - Preview abstracts: 0\n",
            "   With upvotes: 1 (avg: 2.0)\n",
            "   With author names: 0\n",
            "   Average abstract length: 188.4 words\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "âœ… RESUME COMPLETE\n",
            "================================================================================\n",
            "Total unique: 4251 papers\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Deduplicate\n",
        "seen = set()\n",
        "unique_papers = []\n",
        "for p in papers:\n",
        "    key = p.get('paper_id') or p.get('title')\n",
        "    if key and key not in seen:\n",
        "        seen.add(key)\n",
        "        unique_papers.append(p)\n",
        "\n",
        "print(f\"\\nâœ… Total papers after merge: {len(unique_papers)}\")\n",
        "\n",
        "# Save combined dataset\n",
        "scraper.save(papers, \"papers_jan_preview.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ… RESUME COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total unique: {len(unique_papers)} papers\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAHtrGdG1s5o",
        "outputId": "42034927-afee-44a5-e74b-b7c7ecd53499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ“„ SAMPLE PAPERS FROM 2025\n",
            "================================================================================\n",
            "\n",
            "1. WALL-E: World Alignment by Rule Learning Improves World Model-based LLM\n",
            "  Agents\n",
            "   Paper ID: 2410.07484\n",
            "   Date: 2024-10-13\n",
            "   Year: 2024\n",
            "   Authors: 7\n",
            "   Abstract: Can large language models (LLMs) directly serve as powerful world models for model-based agents ? While the gaps between...\n",
            "   Link: https://huggingface.co/papers/2410.07484\n",
            "\n",
            "2. MathCoder2: Better Math Reasoning from Continued Pretraining on\n",
            "  Model-translated Mathematical Code\n",
            "   Paper ID: 2410.08196\n",
            "   Date: 2024-10-13\n",
            "   Year: 2024\n",
            "   Authors: 8\n",
            "   Abstract: Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to it...\n",
            "   Link: https://huggingface.co/papers/2410.08196\n",
            "\n",
            "3. MLLM as Retriever: Interactively Learning Multimodal Retrieval for\n",
            "  Embodied Agents\n",
            "   Paper ID: 2410.03450\n",
            "   Date: 2024-10-13\n",
            "   Year: 2024\n",
            "   Authors: 4\n",
            "   Abstract: MLLM agents demonstrate potential for complex embodied tasks by retrieving\n",
            "multimodal task-relevant trajectory data. How...\n",
            "   Link: https://huggingface.co/papers/2410.03450\n",
            "\n",
            "4. PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n",
            "  in LLMs\n",
            "   Paper ID: 2410.05265\n",
            "   Date: 2024-10-13\n",
            "   Year: 2024\n",
            "   Authors: 6\n",
            "   Abstract: Quantization is essential for deploying Large Language Models (LLMs) by\n",
            "enhancing memory efficiency and inference speed ...\n",
            "   Link: https://huggingface.co/papers/2410.05265\n",
            "\n",
            "5. Benchmarking Agentic Workflow Generation\n",
            "   Paper ID: 2410.07869\n",
            "   Date: 2024-10-13\n",
            "   Year: 2024\n",
            "   Authors: 9\n",
            "   Abstract: Large Language Models (LLMs) , with their exceptional ability to handle a wide\n",
            "range of tasks, have driven significant a...\n",
            "   Link: https://huggingface.co/papers/2410.07869\n"
          ]
        }
      ],
      "source": [
        "# Display sample papers with all available info\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ“„ SAMPLE PAPERS FROM 2025\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, paper in enumerate(papers[:5], 1):\n",
        "    print(f\"\\n{i}. {paper['title']}\")\n",
        "    print(f\"   Paper ID: {paper.get('paper_id', 'N/A')}\")\n",
        "    print(f\"   Date: {paper.get('date', 'N/A')}\")\n",
        "    print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
        "    print(f\"   Authors: {paper.get('authors_count', 'N/A')}\")\n",
        "    print(f\"   Abstract: {paper.get('abstract', 'N/A')[:120]}...\")\n",
        "    if paper.get('link'):\n",
        "        print(f\"   Link: {paper['link']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PxoASM23smO"
      },
      "source": [
        "# 3 Attention Mechanism Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r6xQuwju76Kf"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"Bahdanau (Additive) Attention Mechanism\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_encoder = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_decoder = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.V = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "\n",
        "        decoder_hidden_expanded = decoder_hidden.unsqueeze(1).expand(-1, seq_len, -1)\n",
        "        energy = torch.tanh(\n",
        "            self.W_encoder(encoder_outputs) +\n",
        "            self.W_decoder(decoder_hidden_expanded)\n",
        "        )\n",
        "\n",
        "        attention_scores = self.V(energy).squeeze(-1)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "        context = context.squeeze(1)\n",
        "\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b-zoRzBw9xBI"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Bidirectional LSTM Encoder\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.3):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_size, hidden_size, num_layers=num_layers,\n",
        "            batch_first=True, bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        embedded = self.embedding(x)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(packed)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "        forward_hidden = hidden[-2, :, :]\n",
        "        backward_hidden = hidden[-1, :, :]\n",
        "        hidden_combined = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
        "        hidden_combined = torch.tanh(self.fc(hidden_combined))\n",
        "\n",
        "        outputs = torch.tanh(self.fc(outputs))\n",
        "        return outputs, hidden_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qeHcSQeX_kw3"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    \"\"\"LSTM Decoder with Bahdanau Attention\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.3):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_size + hidden_size, hidden_size,\n",
        "            num_layers=num_layers, batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_size * 2 + embed_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs, mask=None):\n",
        "        embedded = self.embedding(input_token)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        context, attention_weights = self.attention(\n",
        "            hidden[-1], encoder_outputs, mask\n",
        "        )\n",
        "\n",
        "        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)\n",
        "        lstm_output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "        output = torch.cat([\n",
        "            lstm_output.squeeze(1), context, embedded.squeeze(1)\n",
        "        ], dim=1)\n",
        "\n",
        "        output = self.fc_out(output)\n",
        "        return output, hidden, cell, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6E3L34hBFUy",
        "outputId": "8785e71f-b4c6-4167-875e-7c07b3bc9630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model architecture defined!\n",
            "\n",
            "Architecture:\n",
            "  â€¢ Encoder: Bidirectional LSTM\n",
            "  â€¢ Attention: Bahdanau (Additive)\n",
            "  â€¢ Decoder: LSTM with Attention\n"
          ]
        }
      ],
      "source": [
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    \"\"\"Complete Seq2Seq model with Attention\"\"\"\n",
        "    def __init__(self, encoder_vocab_size, decoder_vocab_size, embed_size,\n",
        "                 hidden_size, num_layers=1, dropout=0.3):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(encoder_vocab_size, embed_size, hidden_size,\n",
        "                               num_layers, dropout)\n",
        "        self.decoder = AttentionDecoder(decoder_vocab_size, embed_size,\n",
        "                                       hidden_size, num_layers, dropout)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        tgt_len = tgt.size(1)\n",
        "        vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        encoder_outputs, encoder_hidden = self.encoder(src, src_lengths)\n",
        "\n",
        "        # Create mask based on actual encoder output length (after padding)\n",
        "        encoder_seq_len = encoder_outputs.size(1)\n",
        "        mask = (src[:, :encoder_seq_len] != 0).long()\n",
        "\n",
        "        decoder_hidden = encoder_hidden.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
        "        decoder_cell = torch.zeros_like(decoder_hidden)\n",
        "        decoder_input = tgt[:, 0].unsqueeze(1)\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(src.device)\n",
        "        attention_weights_all = []\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, decoder_hidden, decoder_cell, attention_weights = self.decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs, mask\n",
        "            )\n",
        "\n",
        "            outputs[:, t, :] = output\n",
        "            attention_weights_all.append(attention_weights)\n",
        "\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1).unsqueeze(1)\n",
        "            decoder_input = tgt[:, t].unsqueeze(1) if use_teacher_forcing else top1\n",
        "\n",
        "        return outputs, attention_weights_all\n",
        "\n",
        "print(\"âœ… Model architecture defined!\")\n",
        "print(\"\\nArchitecture:\")\n",
        "print(\"  â€¢ Encoder: Bidirectional LSTM\")\n",
        "print(\"  â€¢ Attention: Bahdanau (Additive)\")\n",
        "print(\"  â€¢ Decoder: LSTM with Attention\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okTPulv_CaYV"
      },
      "source": [
        "## 4 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Aje92PIPCgUd"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MlXIAqqGCmyb"
      },
      "outputs": [],
      "source": [
        "def build_vocab(texts, min_freq=2):\n",
        "  \"\"\"Build vocabulary\"\"\"\n",
        "  vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "  word_freq = {}\n",
        "\n",
        "  for text in texts:\n",
        "    for word in text.split():\n",
        "      word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "  idx = 4\n",
        "  for word, freq in sorted(word_freq.items()):\n",
        "    if freq >= min_freq:\n",
        "      vocab[word] = idx\n",
        "      idx += 1\n",
        "\n",
        "  return vocab, {v: k for k, v in vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2yna7IywDIrg"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(sentence, vocab, max_len):\n",
        "  \"\"\"Encode sentence to indices\"\"\"\n",
        "  tokens = [vocab.get(word, vocab[\"<UNK>\"]) for word in sentence.split()]\n",
        "  tokens = [vocab[\"<SOS>\"]] + tokens + [vocab[\"<EOS>\"]]\n",
        "  actual_len = min(len(tokens), max_len)\n",
        "\n",
        "  if len(tokens) < max_len:\n",
        "    tokens += [vocab[\"<PAD>\"]] * (max_len - len(tokens))\n",
        "  else:\n",
        "    tokens = tokens[:max_len-1] + [vocab[\"<EOS>\"]]\n",
        "\n",
        "  return tokens, actual_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNCUtyhGDvm_",
        "outputId": "8a5035fd-8013-4dfe-b527-12156fd99c91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ğŸ”§ PREPROCESSING DATA\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Dataset Statistics:\n",
            "   Papers: 4251\n",
            "   Avg abstract length: 183.5 words\n",
            "   Avg title length: 9.3 words\n",
            "   Vocabulary size: 17301 words\n",
            "\n",
            "âœ… Data preprocessing complete!\n"
          ]
        }
      ],
      "source": [
        "# Preprocess scraped papers\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ”§ PREPROCESSING DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "abstracts = [preprocess_text(p[\"abstract\"]) for p in papers]\n",
        "titles = [preprocess_text(p[\"title\"]) for p in papers]\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
        "print(f\"   Papers: {len(papers)}\")\n",
        "print(f\"   Avg abstract length: {np.mean([len(a.split()) for a in abstracts]):.1f} words\")\n",
        "print(f\"   Avg title length: {np.mean([len(t.split()) for t in titles]):.1f} words\")\n",
        "\n",
        "# Build vocabulary\n",
        "all_texts = abstracts + titles\n",
        "vocab, idx2word = build_vocab(all_texts, min_freq=2)\n",
        "print(f\"   Vocabulary size: {len(vocab)} words\")\n",
        "\n",
        "# Encode data\n",
        "max_src_len = 120\n",
        "max_tgt_len = 20\n",
        "\n",
        "src_encoded = [encode_sentence(abstract, vocab, max_src_len) for abstract in abstracts]\n",
        "tgt_encoded = [encode_sentence(title, vocab, max_tgt_len) for title in titles]\n",
        "\n",
        "print(f\"\\nâœ… Data preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aucKGyvD1HV"
      },
      "source": [
        "# 5 Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GUunYAeEDsA",
        "outputId": "863f68f9-a590-44cd-cdfb-3657e9596cfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ğŸ—ï¸  MODEL INITIALIZED\n",
            "================================================================================\n",
            "\n",
            "   Embedding size: 256\n",
            "   Hidden size: 512\n",
            "   Num layers: 2\n",
            "   Total parameters: 46,250,645\n",
            "   Device: cpu\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¥ STARTING TRAINING\n",
            "================================================================================\n",
            "\n",
            "Epochs: 150\n",
            "Learning rate: 0.001\n",
            "Training samples: 4251\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "EMBED_SIZE = 256\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 150\n",
        "PRINT_EVERY = 30\n",
        "\n",
        "# Initialize model\n",
        "model = Seq2SeqWithAttention(\n",
        "    encoder_vocab_size=len(vocab),\n",
        "    decoder_vocab_size=len(vocab),\n",
        "    embed_size=EMBED_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ—ï¸  MODEL INITIALIZED\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n   Embedding size: {EMBED_SIZE}\")\n",
        "print(f\"   Hidden size: {HIDDEN_SIZE}\")\n",
        "print(f\"   Num layers: {NUM_LAYERS}\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Training setup\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ”¥ STARTING TRAINING\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nEpochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Training samples: {len(papers)}\")\n",
        "print(\"\\n\" + \"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfolkS8jEISa",
        "outputId": "f08ebb46-b23b-4d34-dfaf-c8bcfb68355d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Epoch [  1/150] | Avg Loss: 7.0695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Epoch [  2/150] | Avg Loss: 6.0614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Epoch [  3/150] | Avg Loss: 6.1270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Epoch [  4/150] | Avg Loss: 6.1125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Epoch [  5/150] | Avg Loss: 6.0816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/150:  10%|â–‰         | 423/4251 [21:54<3:20:44,  3.15s/it, loss=5.8081]"
          ]
        }
      ],
      "source": [
        "loss_history = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(range(len(papers)), desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False)\n",
        "\n",
        "    for i in progress_bar:\n",
        "        src_tokens, src_len = src_encoded[i]\n",
        "        tgt_tokens, tgt_len = tgt_encoded[i]\n",
        "\n",
        "        src = torch.LongTensor([src_tokens]).to(device)\n",
        "        tgt = torch.LongTensor([tgt_tokens]).to(device)\n",
        "        src_lengths = torch.LongTensor([src_len])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs, _ = model(src, src_lengths, tgt, teacher_forcing_ratio=0.5)\n",
        "\n",
        "        # reshape Ä‘á»ƒ tÃ­nh loss\n",
        "        outputs = outputs[:, 1:, :].reshape(-1, len(vocab))\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(outputs, tgt)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        avg_loss_so_far = total_loss / (i + 1)\n",
        "        progress_bar.set_postfix({\"loss\": f\"{avg_loss_so_far:.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / len(papers)\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f\"âœ… Epoch [{epoch+1:3d}/{NUM_EPOCHS}] | Avg Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JFQ1js2Fmem"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GMM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
