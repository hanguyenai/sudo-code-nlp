{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPiSW8CP3rv/SMCP6LxrUFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanguyenai/sudo-code-nlp/blob/main/10_prompting_guide/GeminiAPI_Prompting_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sending simple text prompts"
      ],
      "metadata": {
        "id": "yfYkvRCtZYfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv9mwgeOYpwm",
        "outputId": "a9b1a329-fd84-4608-e28a-c928afb956b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/261.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.2/261.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U -q 'google-genai>=1.51.0' # 1.51 is needed for Gemini 3 pro thinking levels support"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "rmtwC3dXZMPY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYiKVg5sZMKK",
        "outputId": "f03b3141-4f82-49d9-dbc4-c39e9c94a5ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI learns patterns from data to make smart decisions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to print nicely\n",
        "def run_prompt(test_name, contents, config=None):\n",
        "    print(f\"--- üß™ TESTING: {test_name} ---\")\n",
        "    print(f\"üì• INPUT:\\n{contents}\\n\")\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=contents,\n",
        "            config=config\n",
        "        )\n",
        "        print(\"out üì§ OUTPUT:\")\n",
        "        display(Markdown(response.text))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "    print(\"-\" * 40 + \"\\n\")"
      ],
      "metadata": {
        "id": "8HZWORVdZyHZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Zero-shot Prompting"
      ],
      "metadata": {
        "id": "8TKdo5VcZmmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "from google.genai import types\n",
        "\n",
        "prompt_without_zero = \"\"\"\n",
        "What is overfitting in machine learning?\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without Zero-shot\", prompt_without_zero)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S-KThMW5a8Im",
        "outputId": "b1639931-90f5-4db7-e92a-f2719e537302"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without Zero-shot ---\n",
            "üì• INPUT:\n",
            "\n",
            "What is overfitting in machine learning?\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Overfitting** in machine learning is a phenomenon where a model learns the training data **too well**, including its noise and specific irrelevant details, to the detriment of its ability to generalize to new, unseen data.\n\nIn simpler terms:\n\n*   **The model performs exceptionally well on the data it was trained on.** It can predict the outcomes for those specific examples with very high accuracy.\n*   **However, when presented with new, previously unseen data, its performance drops significantly.** It struggles to make accurate predictions because it has essentially \"memorized\" the training examples rather than learning the underlying, general patterns.\n\n---\n\n### Analogy: The Overfitting Student\n\nImagine a student studying for an exam:\n\n*   **Training Data:** The specific practice questions and examples given in class or a textbook.\n*   **Overfitting Student:** This student memorizes the exact answers to every single practice question. If the exam asks *precisely* those same questions, they'll ace it.\n*   **New, Unseen Data (The Real Exam):** The exam contains questions that are slightly different, rephrased, or require applying the *concepts* rather than just recalling specific answers.\n*   **Result:** The overfitting student struggles because they didn't learn the general concepts; they just memorized specific instances. They can't generalize their knowledge to new problems.\n\n---\n\n### Key Characteristics of Overfitting:\n\n1.  **High Training Accuracy/Low Training Error:** The model fits the training data almost perfectly.\n2.  **Low Validation/Test Accuracy or High Validation/Test Error:** The model performs poorly on data it hasn't seen before.\n3.  **Lack of Generalization:** The model cannot effectively predict outcomes for new, real-world data.\n\n---\n\n### Why Does Overfitting Happen? (Common Causes)\n\n1.  **Model Complexity:**\n    *   **Too many parameters:** The model has too much \"capacity\" or flexibility (e.g., a very deep neural network with many layers and neurons, or a polynomial regression model with a very high degree). This allows it to fit even the noise in the training data.\n2.  **Insufficient Training Data:**\n    *   If the training dataset is too small, the model doesn't have enough diverse examples to learn the true underlying patterns. It ends up memorizing the limited examples it has.\n3.  **Noisy Data:**\n    *   If the training data contains errors, outliers, or irrelevant features, an overfit model will try to learn these \"noise\" elements as if they were significant patterns.\n4.  **Long Training Time (for iterative models like neural networks):**\n    *   Training a model for too many epochs can lead it to start fitting the noise in the data after it has already learned the genuine patterns.\n\n---\n\n### How to Identify Overfitting:\n\nThe most common way is to monitor the model's performance on **separate datasets**:\n\n*   **Training Set:** Used to train the model.\n*   **Validation Set (or Test Set):** Data that the model has never seen before, used to evaluate its generalization ability.\n\nWhen you plot the performance (e.g., error rate) over training iterations/epochs:\n*   The **training error** will typically decrease continuously.\n*   The **validation error** will initially decrease along with the training error, but at some point, it will start to **increase** again while the training error continues to fall. The point where the validation error starts to rise is where overfitting begins.\n\n---\n\n### How to Prevent/Mitigate Overfitting:\n\n1.  **More Data:** The best solution, if feasible. A larger, more diverse dataset helps the model learn the true underlying patterns rather than memorizing specifics.\n2.  **Feature Selection/Engineering:**\n    *   Reduce the number of input features to only the most relevant ones. This simplifies the problem for the model.\n    *   Create better, more informative features.\n3.  **Regularization:**\n    *   **L1 (Lasso) and L2 (Ridge) Regularization:** Add a penalty term to the model's loss function based on the magnitude of its coefficients. This discourages overly complex models with large weights.\n    *   **Dropout (for Neural Networks):** Randomly \"turns off\" a fraction of neurons during each training iteration. This forces the network to learn more robust features and prevents specific neurons from becoming overly reliant on others.\n4.  **Cross-Validation:**\n    *   A technique to more robustly estimate a model's performance and select optimal hyperparameters. It involves splitting the training data into multiple folds and training/validating on different combinations.\n5.  **Early Stopping:**\n    *   Monitor the model's performance on a validation set during training. Stop training when the validation error starts to increase (indicating overfitting), even if the training error is still decreasing.\n6.  **Simplifying Model Architecture:**\n    *   Reduce the complexity of the model (e.g., fewer layers in a neural network, fewer neurons per layer, using a simpler algorithm).\n7.  **Ensemble Methods:**\n    *   Combine multiple weaker models into a single, stronger model (e.g., Random Forests, Gradient Boosting Machines). These methods often reduce variance and improve generalization.\n\nOverfitting is a central challenge in machine learning, and addressing it effectively is crucial for building models that perform well in real-world applications."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "from google.genai import types\n",
        "\n",
        "prompt_zero = \"\"\"\n",
        "You are an AI tutor for beginners.\n",
        "\n",
        "In 3‚Äì4 simple sentences, explain what overfitting is in machine learning\n",
        "to a first-year CS student.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Zero-shot\", prompt_zero)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "p8Dt-4tXZOU8",
        "outputId": "2a8fbffe-39bf-43e0-fb6d-45e76f8b0752"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Zero-shot ---\n",
            "üì• INPUT:\n",
            "\n",
            "You are an AI tutor for beginners.\n",
            "\n",
            "In 3‚Äì4 simple sentences, explain what overfitting is in machine learning\n",
            "to a first-year CS student.\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hey there! Think of it like studying for a test by memorizing every single practice question's answer, word for word.\n\nOverfitting in machine learning means your model learns the training data *too* well, almost memorizing specific examples and even noise, instead of understanding the general patterns. This makes it perform amazingly on the data it's already seen, but very poorly on any new, unseen data. So, it struggles to generalize its knowledge to real-world situations."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Few-shot Prompting"
      ],
      "metadata": {
        "id": "0cJZASIyZ3we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "from google.genai import types\n",
        "\n",
        "prompt_without_few = \"\"\"\n",
        "Classify the following AI paper title into one of the categories:\n",
        "[theory, application, optimization].\n",
        "\n",
        "Title: \"Improving Large Language Models with Curriculum RLHF\"\n",
        "Category:\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without few-shot\", prompt_without_few)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "W845C2TNaDiF",
        "outputId": "376a61d6-8c8f-4391-cdd0-3dd23a0f0a3a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without few-shot ---\n",
            "üì• INPUT:\n",
            "\n",
            "Classify the following AI paper title into one of the categories:\n",
            "[theory, application, optimization].\n",
            "\n",
            "Title: \"Improving Large Language Models with Curriculum RLHF\"\n",
            "Category:\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Category: **optimization**\n\n**Explanation:**\n\n*   \"Improving Large Language Models\" clearly indicates a goal of enhancing performance or effectiveness of an existing system.\n*   \"with Curriculum RLHF\" describes a specific *methodology* or *strategy* (Curriculum learning applied to RLHF) to achieve that improvement. This suggests refining or making the existing RLHF process more efficient, effective, or robust for LLMs, which falls under the umbrella of optimization. It's not introducing a fundamentally new theoretical concept, nor is it merely applying an existing technique to a brand new problem domain without specific enhancements to the process itself."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "from google.genai import types\n",
        "\n",
        "prompt_few = \"\"\"\n",
        "Classify each AI paper title into one of: [theory, application, optimization].\n",
        "\n",
        "Examples:\n",
        "\"Convergence Analysis of Adam with Weight Decay\" -> theory\n",
        "\"Real-Time Face Recognition on Edge Devices\" -> application\n",
        "\"Hyperparameter Tuning with Bayesian Optimization\" -> optimization\n",
        "\n",
        "Now classify this title:\n",
        "\"Improving Large Language Models with Curriculum RLHF\" ->\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Few-shot\", prompt_few)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "wTJBljRQcBxp",
        "outputId": "8227a642-9e90-48a1-b801-bb26ed445bad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Few-shot ---\n",
            "üì• INPUT:\n",
            "\n",
            "Classify each AI paper title into one of: [theory, application, optimization].\n",
            "\n",
            "Examples:\n",
            "\"Convergence Analysis of Adam with Weight Decay\" -> theory\n",
            "\"Real-Time Face Recognition on Edge Devices\" -> application\n",
            "\"Hyperparameter Tuning with Bayesian Optimization\" -> optimization\n",
            "\n",
            "Now classify this title:\n",
            "\"Improving Large Language Models with Curriculum RLHF\" ->\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "optimization"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Chain-of-though (CoT) Prompting"
      ],
      "metadata": {
        "id": "puH5DjE9cGat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_without_cot = \"\"\"\n",
        "I am training a transformer model for 3 epochs on 50,000 samples.\n",
        "The batch size is 64.\n",
        "\n",
        "How many optimizer update steps will there be in total?\n",
        "Just give the final number, no explanation.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without CoT prompting:\", prompt_without_cot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "S1iUL-PpcMpM",
        "outputId": "da2abe21-aeb4-4583-af4e-0383ae5b09fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without CoT prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "I am training a transformer model for 3 epochs on 50,000 samples.\n",
            "The batch size is 64.\n",
            "\n",
            "How many optimizer update steps will there be in total?\n",
            "Just give the final number, no explanation.\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "2346"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_cot = \"\"\"\n",
        "I am training a transformer model for 3 epochs on 50,000 samples.\n",
        "The batch size is 64.\n",
        "\n",
        "How many optimizer update steps will there be in total?\n",
        "Let's reason step by step.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With CoT prompting:\", prompt_cot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "5QmrD0wRcYGp",
        "outputId": "dd23628e-7ccf-40fe-cd0d-dfb7853a4d06"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With CoT prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "I am training a transformer model for 3 epochs on 50,000 samples.\n",
            "The batch size is 64.\n",
            "\n",
            "How many optimizer update steps will there be in total?\n",
            "Let's reason step by step.\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's break it down step by step:\n\n1.  **Calculate the number of batches per epoch:**\n    *   Total samples: 50,000\n    *   Batch size: 64\n    *   Number of batches = `total_samples / batch_size`\n    *   `50,000 / 64 = 781.25`\n\n    Since you can't have a fraction of a batch, and the optimizer will update after processing the last (even if partial) batch, we need to take the ceiling of this number.\n    *   Batches per epoch = `ceil(781.25) = 782`\n\n    *(This means there will be 781 batches of 64 samples, and one final batch with `50,000 - (781 * 64) = 50,000 - 49,984 = 16` samples.)*\n\n2.  **Calculate the total number of optimizer update steps over all epochs:**\n    *   Optimizer updates per epoch = number of batches per epoch = 782\n    *   Number of epochs: 3\n    *   Total optimizer update steps = `updates_per_epoch * number_of_epochs`\n    *   `782 * 3 = 2346`\n\n**Therefore, there will be a total of 2346 optimizer update steps.**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Meta Prompting"
      ],
      "metadata": {
        "id": "QTJBIgh6cfqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_without_meta = \"\"\"\n",
        "Compare RNNs and large language models.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without Meta prompting:\", prompt_without_meta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c59oxFDCcjoG",
        "outputId": "b35fd822-2db7-4e6e-9339-86e218f31acb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without Meta prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "Compare RNNs and large language models.\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "RNNs (Recurrent Neural Networks) and LLMs (Large Language Models) both deal with sequential data, particularly language, but they represent different generations of neural network architectures with vastly different scales, capabilities, and underlying mechanisms.\n\nHere's a comparison:\n\n## Recurrent Neural Networks (RNNs)\n\nRNNs are a class of neural networks designed to process sequential data by maintaining an internal state (memory) that captures information about previous elements in the sequence.\n\n**Key Characteristics:**\n\n1.  **Architecture:**\n    *   **Recurrent Connections:** They have loops that allow information to persist from one step of the network to the next. For each input in a sequence, the RNN processes it and updates its hidden state, which then serves as an input for the next step along with the next item in the sequence.\n    *   **Simple RNNs:** Suffer from vanishing or exploding gradient problems, making it difficult to learn long-range dependencies.\n    *   **LSTMs (Long Short-Term Memory) & GRUs (Gated Recurrent Units):** These are specialized types of RNNs that introduced \"gates\" to control the flow of information, significantly mitigating the vanishing gradient problem and allowing them to capture longer-range dependencies than simple RNNs.\n\n2.  **Memory/Context Handling:**\n    *   **Sequential Processing:** Process tokens one by one, updating a hidden state.\n    *   **Limited \"Memory\":** While they have a hidden state, their effective memory for very long sequences is still limited. Information from early parts of a long sequence can degrade or be forgotten by the time the network reaches the end. LSTMs/GRUs improved this but didn't solve it entirely for extremely long sequences.\n\n3.  **Parallelism:**\n    *   **Inherently Sequential:** Training is inherently sequential because each step depends on the previous hidden state. This makes parallelization difficult and slow, especially for long sequences.\n\n4.  **Scale:**\n    *   **Smaller Models:** Typically have millions, sometimes tens of millions, of parameters.\n    *   **Smaller Datasets:** Trained on relatively smaller datasets compared to LLMs.\n\n5.  **Training:**\n    *   **Backpropagation Through Time (BPTT):** The gradient descent algorithm used to train RNNs, which unrolls the network over time steps.\n    *   **Slower:** Due to sequential processing and BPTT, training can be slow.\n\n6.  **Capabilities (Historic):**\n    *   Machine translation (earlier systems)\n    *   Speech recognition\n    *   Text generation (simple, less coherent)\n    *   Time series prediction\n\n## Large Language Models (LLMs)\n\nLLMs are a type of artificial intelligence model, primarily based on the Transformer architecture, characterized by their massive scale (billions to trillions of parameters) and extensive training on vast amounts of text data.\n\n**Key Characteristics:**\n\n1.  **Architecture:**\n    *   **Transformer Architecture:** The defining feature. It completely *abandoned* recurrent connections.\n    *   **Self-Attention Mechanism:** Allows the model to weigh the importance of different words in the input sequence when processing each word. This enables direct access to any part of the input, regardless of distance, effectively capturing very long-range dependencies.\n    *   **Positional Encoding:** Added to the input embeddings to give the model information about the order of words, as self-attention itself is permutation-invariant.\n    *   **Encoder-Decoder or Decoder-Only:** LLMs often use a decoder-only architecture (e.g., GPT-series) for generative tasks.\n\n2.  **Memory/Context Handling:**\n    *   **Direct Access:** Self-attention allows direct comparison and interaction between any two tokens in a given input sequence (within its fixed context window). This makes them exceptionally good at understanding and generating long, coherent texts.\n    *   **Large Context Windows:** While still finite, their context windows (the maximum number of tokens they can \"see\" at once) are typically thousands or even tens of thousands of tokens long, far exceeding the practical limits of RNNs.\n\n3.  **Parallelism:**\n    *   **Highly Parallelizable:** The self-attention mechanism processes all tokens in the input sequence simultaneously (within the context window). This allows for highly efficient parallel computation during training and inference, which is crucial for their massive scale.\n\n4.  **Scale:**\n    *   **Massive Models:** Billions to trillions of parameters (e.g., GPT-3 has 175 billion, some models are larger).\n    *   **Massive Datasets:** Trained on petabytes of text data (e.g., Common Crawl, web pages, books, code).\n\n5.  **Training:**\n    *   **Pre-training + Fine-tuning:** Typically involves unsupervised pre-training on a massive text corpus to learn general language patterns, followed by supervised fine-tuning on specific tasks or Reinforcement Learning from Human Feedback (RLHF) for alignment.\n    *   **Extremely Resource-Intensive:** Requires vast computational resources (thousands of GPUs) and energy.\n\n6.  **Capabilities (Current State-of-the-Art):**\n    *   Highly coherent and contextually relevant text generation (articles, stories, code)\n    *   Complex question answering\n    *   Summarization\n    *   Machine translation (state-of-the-art)\n    *   Sentiment analysis\n    *   Code generation and debugging\n    *   Reasoning, instruction following, and zero-shot/few-shot learning.\n    *   Exhibiting \"emergent abilities\" not explicitly programmed.\n\n## Summary of Key Differences:\n\n| Feature             | Recurrent Neural Networks (RNNs)                               | Large Language Models (LLMs)                                       |\n| :------------------ | :--------------------------------------------------------------- | :------------------------------------------------------------------ |\n| **Core Architecture** | Recurrent connections (loops), often LSTM/GRU gates              | Transformer (self-attention mechanism), no recurrent connections    |\n| **Memory Handling** | Sequential hidden state updates, limited long-range memory       | Self-attention provides direct access to tokens, large context window |\n| **Parallelism**     | Inherently sequential, difficult to parallelize training         | Highly parallelizable, processes tokens simultaneously             |\n| **Scale (Params)**  | Millions (e.g., ~10-100M for complex LSTMs)                      | Billions to trillions                                              |\n| **Scale (Data)**    | Moderately sized datasets                                        | Petabytes of text and code                                          |\n| **Training**        | Backpropagation Through Time (BPTT), slower                      | Pre-training + Fine-tuning/RLHF, very resource-intensive          |\n| **Primary Limitation**| Vanishing/exploding gradients, limited long-range dependencies | High computational cost, hallucinations, bias, lack of true understanding |\n| **Performance**     | Good for short sequences, earlier NLP tasks                      | State-of-the-art across virtually all NLP and reasoning tasks      |\n| **Era**             | Dominant for sequential data from ~2012-2017                     | Dominant for NLP from ~2018-present                                 |\n\nIn essence, LLMs, by leveraging the Transformer architecture and massive scale, overcame the fundamental limitations of RNNs regarding long-range dependencies, parallelism, and overall coherence, leading to a revolution in artificial intelligence and natural language processing."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_meta = \"\"\"\n",
        "Compare RNNs and large language models (LLMs) for NLP tasks.\n",
        "\n",
        "Use this exact structure:\n",
        "\n",
        "1. Short definition of RNNs\n",
        "2. Short definition of LLMs\n",
        "3. A bullet list with 3 advantages of LLMs over RNNs\n",
        "4. A bullet list with 2 situations where RNNs might still be useful\n",
        "5. A one-sentence conclusion\n",
        "\n",
        "Follow the structure strictly.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With Meta prompting:\", prompt_with_meta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "wlOlk62Zcpu1",
        "outputId": "0e03b8e4-9790-4559-9cde-799f512cda71"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With Meta prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "Compare RNNs and large language models (LLMs) for NLP tasks.\n",
            "\n",
            "Use this exact structure:\n",
            "\n",
            "1. Short definition of RNNs\n",
            "2. Short definition of LLMs\n",
            "3. A bullet list with 3 advantages of LLMs over RNNs\n",
            "4. A bullet list with 2 situations where RNNs might still be useful\n",
            "5. A one-sentence conclusion\n",
            "\n",
            "Follow the structure strictly.\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's a comparison of RNNs and LLMs for NLP tasks:\n\n1.  **Short definition of RNNs**\n    Recurrent Neural Networks (RNNs) are a type of neural network designed to process sequential data, using an internal memory (hidden state) to maintain information from previous steps in the sequence.\n\n2.  **Short definition of LLMs**\n    Large Language Models (LLMs) are deep learning models, primarily based on the transformer architecture, trained on immense datasets of text and code to understand, generate, and process human language at an advanced level.\n\n3.  **A bullet list with 3 advantages of LLMs over RNNs**\n    *   **Superior Long-Range Dependency Handling:** LLMs, leveraging attention mechanisms, effectively capture relationships between distant words in a sequence, a significant challenge for standard RNNs due to vanishing/exploding gradients.\n    *   **Advanced Contextual Understanding and Nuance:** Trained on vast and diverse datasets, LLMs develop a much deeper and more nuanced understanding of context, semantics, and pragmatics, leading to higher quality outputs across various NLP tasks.\n    *   **Generalization and Transfer Learning Capabilities:** LLMs are pre-trained on massive corpora, allowing them to perform well on a wide range of downstream tasks with minimal fine-tuning, whereas RNNs typically require task-specific training from scratch.\n\n4.  **A bullet list with 2 situations where RNNs might still be useful**\n    *   **Resource-constrained environments or simple, short sequence tasks:** For applications with very limited computational power, small datasets, or tasks involving only very short sequences where deep contextual understanding is not critical.\n    *   **Educational purposes or foundational understanding:** As a simpler architectural baseline, RNNs are still valuable for teaching fundamental concepts of sequence modeling before introducing more complex transformer architectures.\n\n5.  **A one-sentence conclusion**\n    While RNNs laid foundational groundwork, LLMs, with their transformative architecture and vast training data, have largely eclipsed them, setting new benchmarks for performance and versatility in nearly all NLP domains."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generate Knowledge Prompting"
      ],
      "metadata": {
        "id": "YM1LHSY1ct6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_without_generate_knowledge = \"\"\"\n",
        "Is reinforcement learning useful for recommender systems? Why?\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without Generate Knowledge prompting:\", prompt_without_generate_knowledge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KlABg2-ycxeq",
        "outputId": "93889f0b-f360-4329-c0da-9acdf9813fe9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without Generate Knowledge prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "Is reinforcement learning useful for recommender systems? Why?\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, reinforcement learning (RL) is increasingly recognized as a powerful and promising approach for recommender systems, particularly in scenarios where the goal is to optimize for **long-term user engagement and satisfaction** rather than just immediate clicks or purchases.\n\n### Why RL is Useful for Recommender Systems:\n\nThe core idea behind using RL for recommender systems stems from treating the recommendation process as a **sequential decision-making problem**, analogous to an agent interacting with an environment.\n\n1.  **Optimizing for Long-Term Value:**\n    *   **Problem with Traditional Recommenders:** Many traditional recommenders (e.g., collaborative filtering, matrix factorization, basic supervised learning) optimize for short-term metrics like click-through rate (CTR) or purchase conversion for a single recommendation round. This can lead to \"greedy\" recommendations that might satisfy a user in the short run but don't foster long-term engagement or diverse exploration.\n    *   **RL Solution:** RL agents can be trained to maximize a cumulative reward over a sequence of interactions. This means the system learns to make recommendations that lead to sustained user engagement, repeated visits, broader content consumption, or even higher lifetime value, rather than just optimizing for the next immediate click.\n\n2.  **Dynamic Adaptation to User Behavior:**\n    *   User preferences change over time, and a user's current context (time of day, device, mood, previous interactions) significantly impacts what they might want to see.\n    *   RL agents are inherently designed to adapt their policies based on ongoing interactions and feedback, allowing the recommender system to personalize recommendations in real-time as user behavior evolves.\n\n3.  **Handling Sequential Interactions and Context:**\n    *   Recommendations are not isolated events; they are part of a continuous dialogue between the user and the system. What was recommended previously, how the user reacted, and the current state of the session all influence the next optimal recommendation.\n    *   RL naturally models this sequential interaction, where the \"state\" of the environment can include user history, current session information, item features, and external context, allowing for more contextual and coherent recommendation sequences.\n\n4.  **Exploration vs. Exploitation:**\n    *   A critical challenge in recommenders is balancing showing users items they are likely to enjoy (exploitation) with introducing them to new items or categories they might discover and love (exploration).\n    *   RL algorithms, especially those based on multi-armed bandits or more complex settings, provide principled ways to manage this trade-off, ensuring that the system occasionally explores new recommendations while primarily exploiting what it knows works. This helps prevent \"filter bubbles\" and enhances user discovery.\n\n5.  **Dealing with Delayed and Sparse Feedback:**\n    *   Sometimes, the true \"reward\" for a good recommendation isn't immediate (e.g., a user might purchase an item hours or days after seeing the recommendation, or subscribe to a service after consistently positive experiences).\n    *   RL techniques, particularly those using value functions, are well-suited to handle these delayed and sparse reward signals, learning to attribute long-term outcomes to previous actions.\n\n### How RL is Applied in Recommender Systems:\n\n*   **Agent:** The recommender system itself, making decisions.\n*   **Environment:** The user and the catalog of items.\n*   **State:** A representation of the user's history, current session, contextual information, and item features. This can be high-dimensional and often uses embeddings from deep learning.\n*   **Action:** The act of recommending one or a list of items.\n*   **Reward:** Feedback from the user (e.g., click, purchase, time spent, rating, adding to cart, completing a video, positive sentiment, negative feedback like skipping). This can be a scalar value or a composite score.\n*   **Policy:** The learned strategy that maps states to actions (recommendations) to maximize cumulative reward.\n\n**Common Architectures/Approaches:**\n\n*   **Contextual Bandits:** A simpler form of RL, often used for recommending a single item or a small set of items in a sequential manner, focusing on the exploration-exploitation dilemma. (e.g., news article recommendation).\n*   **Q-Learning and Deep Q-Networks (DQN):** For discrete action spaces where the agent learns the value of taking a certain action in a given state.\n*   **Actor-Critic Methods:** For more complex, potentially continuous state and action spaces, where an \"actor\" learns the policy and a \"critic\" learns to evaluate the actor's actions.\n*   **Hybrid Approaches:** Often, RL is used in conjunction with traditional recommender techniques. For instance, a supervised model might pre-filter a large item catalog, and then an RL agent ranks the top-k candidates to optimize for long-term engagement.\n\n### Challenges of Using RL for Recommender Systems:\n\nDespite its promise, applying RL in recommender systems comes with significant challenges:\n\n1.  **Defining State and Action Spaces:**\n    *   **State:** Representing the user's complex history and context in a meaningful and manageable way is difficult.\n    *   **Action:** The number of items to recommend can be millions or billions, making a direct action space intractable for most RL algorithms. This usually requires clever item embedding techniques or hierarchical action selection.\n2.  **Reward Design:** Crafting appropriate reward functions that accurately reflect long-term user satisfaction and align with business goals can be complex and requires careful experimentation.\n3.  **Data Efficiency and Sample Complexity:** RL algorithms often require a vast amount of interaction data to learn effective policies, which can be slow and expensive to collect in real-world systems.\n4.  **Off-Policy Learning and Evaluation:** Learning from historical data (which was generated by a different, possibly suboptimal, recommendation policy) is challenging. Evaluating the performance of a new RL policy without costly A/B tests is also difficult.\n5.  **Cold Start Problem:** Like other recommendation methods, RL struggles with new users or new items that have no interaction history.\n6.  **Computational Cost:** Training and deploying complex RL models can be computationally intensive and require significant infrastructure.\n7.  **Explainability:** RL models can be \"black boxes,\" making it hard to explain *why* a particular recommendation was made, which can be important for user trust and debugging.\n\nIn conclusion, RL offers a powerful paradigm shift for recommender systems, moving beyond static, short-term optimization to dynamic, long-term user engagement. While it presents unique challenges, the benefits of adapting to evolving user preferences and optimizing for sustained value make it a rapidly growing and highly promising area of research and application in the field."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_generate_knowledge_step_1 = \"\"\"\n",
        "Input: Is reinforcement learning useful for recommender systems?\n",
        "First, list 3‚Äì4 facts about how RL can be used in recommendation (no final answer yet).\n",
        "Knowledge:\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With Generate Knowledge prompting - Step 1: Knowledge\", prompt_with_generate_knowledge_step_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "leG3KKQoc8mc",
        "outputId": "a14c1e73-484b-4e6b-d513-d69366c55752"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With Generate Knowledge prompting - Step 1: Knowledge ---\n",
            "üì• INPUT:\n",
            "\n",
            "Input: Is reinforcement learning useful for recommender systems?\n",
            "First, list 3‚Äì4 facts about how RL can be used in recommendation (no final answer yet).\n",
            "Knowledge:\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here are 3-4 facts about how Reinforcement Learning can be used in recommender systems:\n\n1.  **Optimizing for Long-term Rewards:** Unlike traditional recommender systems that often optimize for immediate metrics like clicks or purchases, RL can be designed to maximize long-term user satisfaction, engagement, or retention by considering the cumulative rewards over a sequence of interactions.\n2.  **Sequential Decision Making and User State Modeling:** Recommender systems involve a sequence of interactions where each recommendation (action) influences subsequent user behavior. RL naturally models this as a Markov Decision Process, taking into account the evolving user \"state\" (e.g., past interactions, current context, changing preferences) to make adaptive recommendations.\n3.  **Handling Exploration-Exploitation Trade-off:** RL provides a framework to strategically balance recommending familiar items (exploitation) that are likely to be accepted, with introducing novel but potentially valuable items (exploration) to discover new user preferences or expand their interests.\n4.  **Dynamic Adaptation to Changing Preferences and Environments:** RL agents can continuously learn and adapt their recommendation policies in real-time as user preferences shift, new items are introduced, or the overall environment changes, making them highly suitable for dynamic recommendation scenarios."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_generate_knowledge_step_2 = \"\"\"\n",
        "Question: Is reinforcement learning useful for recommender systems? Why?\n",
        "You are given some background knowledge. Use it to write a concise answer.\n",
        "\n",
        "Knowledge:\n",
        "- (paste the facts from step 1 here)\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With Generate Knowledge prompting - Step 2: Answer\", prompt_with_generate_knowledge_step_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "-aW9Vxs1dEyI",
        "outputId": "43fe759a-95cb-4583-a5f4-6b87ccbab3bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With Generate Knowledge prompting - Step 2: Answer ---\n",
            "üì• INPUT:\n",
            "\n",
            "Question: Is reinforcement learning useful for recommender systems? Why?\n",
            "You are given some background knowledge. Use it to write a concise answer.\n",
            "\n",
            "Knowledge:\n",
            "- (paste the facts from step 1 here)\n",
            "\n",
            "Answer:\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, Reinforcement Learning (RL) is highly useful for recommender systems.\n\nThe primary reason is that recommender systems inherently involve **sequential decision-making** and **long-term optimization**. Traditional recommender systems often focus on optimizing immediate, short-term metrics like predicting the next click or purchase. However, a user's satisfaction and engagement are built over a series of interactions.\n\nRL excels at modeling this dynamic process:\n1.  **Long-Term Optimization:** By treating the recommender as an agent, the user as the environment, recommendations as actions, and user feedback (e.g., clicks, purchases, time spent, returning to the platform) as rewards, RL can learn a policy that maximizes **cumulative future reward**. This allows the system to optimize for long-term goals like user retention, overall engagement, or customer lifetime value, rather than just immediate gratification.\n2.  **Dynamic Adaptation:** RL agents can continuously learn and adapt their recommendation strategy as user preferences evolve and the underlying item catalog changes.\n3.  **Exploration-Exploitation:** RL naturally addresses the dilemma of balancing recommending items known to be liked (exploitation) with exploring new, potentially valuable items to discover novel preferences or avoid filter bubbles."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Self-Consistency"
      ],
      "metadata": {
        "id": "8VDVpt3MdOxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_without_self_consistency = \"\"\"\n",
        "A reinforcement learning agent receives a reward of +1 for reaching the goal,\n",
        "and -1 if it hits an obstacle. After taking three steps, the agent got a\n",
        "total reward of +1. What is the most likely sequence of rewards it received?\n",
        "Answer briefly.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without Self-Consistency prompting:\", prompt_without_self_consistency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "BQkjKkwddidx",
        "outputId": "cdc6db10-f55b-4d19-dcd9-ac87e6a61e3b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without Self-Consistency prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "A reinforcement learning agent receives a reward of +1 for reaching the goal,\n",
            "and -1 if it hits an obstacle. After taking three steps, the agent got a\n",
            "total reward of +1. What is the most likely sequence of rewards it received?\n",
            "Answer briefly.\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The agent received two rewards of +1 and one reward of -1.\nThe most likely sequences are any permutation of (+1, +1, -1). For example:\n\n**(+1, +1, -1)**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_self_consistency = \"\"\"\n",
        "A reinforcement learning agent receives a reward of +1 when reaching the goal,\n",
        "and -1 if it hits an obstacle. After taking three steps, its total reward is +1.\n",
        "\n",
        "Generate 5 different reasoning paths showing possible reward sequences.\n",
        "Then pick the final answer that appears the most frequently among them.\n",
        "Format:\n",
        "- Reasoning paths\n",
        "- Final answer\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With Self-Consistency prompting:\", prompt_with_self_consistency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vA-ZhR4YdsaF",
        "outputId": "4cd26d5b-0561-4da4-e297-930e25a064fc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With Self-Consistency prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "A reinforcement learning agent receives a reward of +1 when reaching the goal,\n",
            "and -1 if it hits an obstacle. After taking three steps, its total reward is +1.\n",
            "\n",
            "Generate 5 different reasoning paths showing possible reward sequences.\n",
            "Then pick the final answer that appears the most frequently among them.\n",
            "Format:\n",
            "- Reasoning paths\n",
            "- Final answer\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "An agent's total reward after three steps is +1. Each step gives either a +1 (goal) or -1 (obstacle) reward. Let $r_1, r_2, r_3$ be the rewards for the three steps. We have $r_1 + r_2 + r_3 = +1$, where $r_i \\in \\{+1, -1\\}$.\n\nTo achieve a sum of +1 from three terms of +1 or -1:\n- If there were three +1s, sum = +3.\n- If there were three -1s, sum = -3.\n- If there were one +1 and two -1s, sum = +1 - 1 - 1 = -1.\n- If there were two +1s and one -1, sum = +1 + 1 - 1 = +1. This is the only combination that works.\n\nSo, any valid reward sequence must consist of exactly two +1 rewards and one -1 reward. There are three permutations for this:\n1. (+1, +1, -1)\n2. (+1, -1, +1)\n3. (-1, +1, +1)\n\nWe need to generate 5 different reasoning paths showing possible reward sequences, which means we will likely repeat some of these valid sequences.\n\n---\n\n**Reasoning paths:**\n\n**1. Path: Agent makes progress then an error.**\nThe agent successfully navigates the first two steps, reaching positive outcomes (+1, +1). However, on the third and final step, it encounters an obstacle (-1).\n- Step 1: +1\n- Step 2: +1\n- Step 3: -1\n- Total reward: (+1) + (+1) + (-1) = +1\n- Possible Reward Sequence: **(+1, +1, -1)**\n\n**2. Path: Agent makes an early error but recovers.**\nThe agent starts by hitting an obstacle on the first step (-1). Despite this initial setback, it successfully completes the next two steps (+1, +1), compensating for the early mistake.\n- Step 1: -1\n- Step 2: +1\n- Step 3: +1\n- Total reward: (-1) + (+1) + (+1) = +1\n- Possible Reward Sequence: **(-1, +1, +1)**\n\n**3. Path: Agent performs well, makes a mid-step error, then corrects.**\nThe agent starts strong with a successful step (+1), then makes an error and hits an obstacle in the middle step (-1), but manages to recover and make the final step successful (+1).\n- Step 1: +1\n- Step 2: -1\n- Step 3: +1\n- Total reward: (+1) + (-1) + (+1) = +1\n- Possible Reward Sequence: **(+1, -1, +1)**\n\n**4. Path: Identifying the required reward composition and a specific ordering.**\nTo get a sum of +1 from three rewards of +1 or -1, there must be exactly two +1s and one -1. Consider the case where the agent achieves two successes and then a failure. A natural way for this to occur is two positive rewards followed by a negative reward.\n- Step 1: Positive reward (+1)\n- Step 2: Positive reward (+1)\n- Step 3: Negative reward (-1)\n- Total reward: (+1) + (+1) + (-1) = +1\n- Possible Reward Sequence: **(+1, +1, -1)**\n\n**5. Path: Scenario of strong initial performance followed by a late misstep.**\nImagine the agent is generally performing well and accumulates two positive rewards. If the sole negative reward occurs as the final action, resulting in a net positive outcome. This is a common pattern of \"almost there\" or \"slipped at the finish line\".\n- Step 1: Goal (+1)\n- Step 2: Goal (+1)\n- Step 3: Obstacle (-1)\n- Total reward: (+1) + (+1) + (-1) = +1\n- Possible Reward Sequence: **(+1, +1, -1)**\n\n---\n\n**Frequency Count:**\nFrom the 5 reasoning paths, the reward sequences generated are:\n1. (+1, +1, -1)\n2. (-1, +1, +1)\n3. (+1, -1, +1)\n4. (+1, +1, -1)\n5. (+1, +1, -1)\n\nCounting the occurrences:\n- (+1, +1, -1): Appears 3 times\n- (-1, +1, +1): Appears 1 time\n- (+1, -1, +1): Appears 1 time\n\nThe sequence **(+1, +1, -1)** appears most frequently (3 times).\n\n---\n\n**Final Answer:**\n(+1, +1, -1)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Prompt Chaining"
      ],
      "metadata": {
        "id": "cDHgov6ud1cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_without_prompt_chaining = \"\"\"\n",
        "Explain how the PPO (Proximal Policy Optimization) algorithm works\n",
        "and evaluate its strengths and weaknesses in 5‚Äì6 sentences.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without Prompt Chaining prompting:\", prompt_without_prompt_chaining)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "P6yU_aGwd5tB",
        "outputId": "b278758c-935b-4833-8701-dc90873e4b7c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without Prompt Chaining prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "Explain how the PPO (Proximal Policy Optimization) algorithm works\n",
            "and evaluate its strengths and weaknesses in 5‚Äì6 sentences.\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Proximal Policy Optimization (PPO) is an on-policy reinforcement learning algorithm that iteratively updates a policy by maximizing a \"clipped surrogate objective function.\" This objective uses an advantage estimate and, crucially, employs a clipping mechanism to limit the policy ratio between the new and old policies. This clipping ensures that policy updates are not too drastic, preventing instability and catastrophic performance drops often seen with large policy changes.\n\nIts primary strength lies in its excellent balance of stability, performance, and relative ease of implementation compared to its predecessor, TRPO, making it a robust choice for many continuous control tasks. However, as an on-policy algorithm, PPO can be less sample-efficient than off-policy methods, requiring fresh interactions with the environment for each major update. Its performance can also be sensitive to hyperparameter tuning, such as the clipping threshold and learning rate."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_prompt_chaining_step_1 = \"\"\"\n",
        "Explain the PPO algorithm in 4‚Äì5 sentences.\n",
        "Focus only on how it works (do not mention pros/cons).\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With Prompt Chaining prompting - Step 1\", prompt_with_prompt_chaining_step_1)\n",
        "\n",
        "prompt_with_prompt_chaining_step_2 = \"\"\"\n",
        "Using the explanation above:\n",
        "\n",
        "Write a separate evaluation (not mixed with description) listing:\n",
        "- Two strengths of PPO\n",
        "- Two weaknesses of PPO\n",
        "\n",
        "Use bullet points only..\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With Prompt Chaining prompting - Step 2\", prompt_with_prompt_chaining_step_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "KGRp-DPeeCAM",
        "outputId": "14a3b971-f886-43b0-ce0a-7d28f287ecb9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With Prompt Chaining prompting - Step 1 ---\n",
            "üì• INPUT:\n",
            "\n",
            "Explain the PPO algorithm in 4‚Äì5 sentences.\n",
            "Focus only on how it works (do not mention pros/cons).\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "PPO optimizes a policy by maximizing a \"clipped surrogate objective function\" using an actor-critic approach. It collects a batch of experience by interacting with the environment using the current policy. The core of PPO involves calculating a ratio of the new policy's action probability to the old policy's action probability. This ratio is then \"clipped\" within a small interval (e.g., [0.8, 1.2]) in the objective function. This clipping mechanism prevents the new policy from diverging too drastically from the old policy in a single update, ensuring stable training while allowing for multiple gradient steps on the same batch of collected data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n",
            "--- üß™ TESTING: With Prompt Chaining prompting - Step 2 ---\n",
            "üì• INPUT:\n",
            "\n",
            "Using the explanation above:\n",
            "\n",
            "Write a separate evaluation (not mixed with description) listing:\n",
            "- Two strengths of PPO\n",
            "- Two weaknesses of PPO\n",
            "\n",
            "Use bullet points only..\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is an evaluation of PPO:\n\n### Strengths of PPO\n\n*   **Stable and Reliable Updates:** PPO uses a clipped objective function (or KL regularization) to ensure that policy updates are not too large, which prevents catastrophic performance drops and leads to more stable training.\n*   **Strong Performance and Simplicity:** It achieves state-of-the-art or near state-of-the-art performance across a wide range of continuous control tasks while being significantly simpler to implement and tune than previous stable policy gradient methods like TRPO.\n\n### Weaknesses of PPO\n\n*   **Lower Sample Efficiency (compared to off-policy methods):** As an on-policy algorithm, PPO generally requires fresh interaction data for each significant policy update, making it less sample-efficient than off-policy algorithms that can reuse older data more extensively.\n*   **Hyperparameter Sensitivity:** While robust for many tasks, optimal performance often requires careful tuning of specific hyperparameters, such as the clip ratio, learning rates for the actor and critic, and GAE (Generalized Advantage Estimation) parameters."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Tree of Thoughts (ToT)"
      ],
      "metadata": {
        "id": "fJGZkrFqeNH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_without_tot = \"\"\"\n",
        "How can we improve exploration in reinforcement learning?\n",
        "Give one answer.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without Tree of Thoughts prompting:\", prompt_without_tot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4db8B7XMeShd",
        "outputId": "12868da5-dec9-40d6-d931-7673bf21af14"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without Tree of Thoughts prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "How can we improve exploration in reinforcement learning?\n",
            "Give one answer.\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Incorporate **curiosity-driven exploration**, where the agent receives an intrinsic reward for visiting novel states or for actions that lead to unpredictable outcomes, encouraging it to explore even in the absence of external rewards."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_tot = \"\"\"\n",
        "How can we improve exploration in reinforcement learning?\n",
        "\n",
        "Follow a Tree-of-Thoughts process:\n",
        "\n",
        "1. Generate 3 different independent reasoning paths (3 branches),\n",
        "   each proposing a different exploration strategy.\n",
        "2. Evaluate each path in terms of:\n",
        "   - feasibility\n",
        "   - novelty\n",
        "   - potential performance benefits\n",
        "3. Choose the best strategy and provide a short justification.\n",
        "\n",
        "Format:\n",
        "- Branch 1\n",
        "- Branch 2\n",
        "- Branch 3\n",
        "- Evaluation\n",
        "- Final chosen solution\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With Tree of Thoughts prompting:\", prompt_with_tot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iSURQKMNekyE",
        "outputId": "8dfdffd4-ab17-4e8e-e9d2-49710a6c951e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With Tree of Thoughts prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "How can we improve exploration in reinforcement learning?\n",
            "\n",
            "Follow a Tree-of-Thoughts process:\n",
            "\n",
            "1. Generate 3 different independent reasoning paths (3 branches),\n",
            "   each proposing a different exploration strategy.\n",
            "2. Evaluate each path in terms of:\n",
            "   - feasibility\n",
            "   - novelty\n",
            "   - potential performance benefits\n",
            "3. Choose the best strategy and provide a short justification.\n",
            "\n",
            "Format:\n",
            "- Branch 1\n",
            "- Branch 2\n",
            "- Branch 3\n",
            "- Evaluation\n",
            "- Final chosen solution\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here are three distinct approaches to improve exploration in reinforcement learning, following a Tree-of-Thoughts process:\n\n---\n\n### Branch 1: Information Gain with Predictive Models\n\n**Strategy:** The agent is intrinsically motivated to take actions that maximize the information gained about the environment's dynamics or reward function. This involves training a predictive model (e.g., a world model or a dynamics model) alongside the policy. The intrinsic reward is derived from the expected reduction in the predictive model's uncertainty or the expected increase in the model's confidence about future states/rewards, given a specific action.\n\n**Mechanism:**\n1.  **Predictive Model Training:** A separate neural network learns to predict the next state $s_{t+1}$ and/or reward $r_{t+1}$ given the current state $s_t$ and action $a_t$. This model also estimates its own uncertainty (e.g., using an ensemble of models, dropout, or Bayesian neural networks).\n2.  **Information Gain Metric:** At each step, the agent considers potential actions and evaluates which action is expected to maximally reduce its predictive model's uncertainty about the immediate future (e.g., by leading to states where the model's prediction variance is expected to drop significantly, or where the model learns the most). This can be quantified using metrics like Bayesian surprise or mutual information.\n3.  **Intrinsic Reward:** The information gain metric is used as an intrinsic reward, added to the extrinsic environmental reward. This encourages the agent to prioritize actions that lead to \"more informative\" experiences for its predictive model, thereby improving its understanding of the environment.\n4.  **Policy Learning:** The main policy is trained to maximize the sum of extrinsic and intrinsic rewards.\n\n---\n\n### Branch 2: Hierarchical Goal-Setting with Active Goal Generation and Hindsight\n\n**Strategy:** Implement a hierarchical reinforcement learning (HRL) framework where a high-level policy sets abstract goals, and a low-level policy tries to achieve them. Crucially, the high-level policy *actively generates* challenging but achievable goals, and the learning process efficiently leverages hindsight to learn from \"failed\" attempts.\n\n**Mechanism:**\n1.  **Hierarchical Structure:**\n    *   **High-Level Policy:** Operates on a slower timescale, observes the full state, and outputs a goal $g$ (a desired future state or state feature) for the low-level policy. It receives intrinsic reward based on the novelty or learning potential of the generated goal, and extrinsic reward from the environment when high-level objectives are met.\n    *   **Low-Level Policy:** Operates on a faster timescale, receives the current state $s_t$ and the high-level goal $g$, and outputs primitive actions $a_t$ aiming to reach $g$. It receives an intrinsic reward based on its progress towards goal $g$.\n2.  **Active Goal Generation:** The high-level policy doesn't just sample random goals. Instead, it employs a separate network or heuristic to propose goals that are:\n    *   **Novel/Uncertain:** Goals corresponding to parts of the state space that haven't been thoroughly explored or where the agent's world model (if one exists) is uncertain.\n    *   **Slightly Out of Reach:** Goals that are challenging enough to provide new information but not so hard that they are impossible to achieve, encouraging productive exploration. (e.g., sampling goals from a distribution of past achieved states, but slightly further away).\n3.  **Hindsight Experience Replay (HER):** When a low-level episode finishes (whether the goal $g$ was reached or not), HER is applied. The actual final state achieved $s_{final}$ is re-labeled as a new goal $g'$ for that trajectory. The experience $(s_t, a_t, r_t, s_{t+1})$ is then replayed multiple times with $g'$ as the desired goal. This allows the low-level policy to learn even from \"failed\" attempts by interpreting them as successful attempts at a different, actually achieved goal. This significantly improves learning efficiency and exploration by making every experience valuable.\n\n---\n\n### Branch 3: Decentralized Diverse-Strategy Exploration (DDSE)\n\n**Strategy:** Instead of a single agent with one exploration strategy, deploy a population of agents, each initialized with a *different* exploration strategy (e.g., different intrinsic reward functions, noise parameters, or prior biases). These agents explore in parallel, sharing their experiences into a common replay buffer, and a meta-learning or evolutionary process periodically selects and propagates more effective exploration strategies.\n\n**Mechanism:**\n1.  **Population of Explorers:** Initialize N distinct agents. Each agent $i$ has its own policy $\\pi_i$ and a unique exploration strategy $E_i$. $E_i$ could be defined by:\n    *   Different intrinsic reward functions (e.g., count-based, RND-based, curiosity-based, different weights for these).\n    *   Different noise injection mechanisms (e.g., Gaussian noise with varying std dev, Ornstein-Uhlenbeck process with different parameters).\n    *   Different initializations or prior exploration biases.\n2.  **Shared Experience Replay:** All agents contribute their collected trajectories (state, action, reward, next state, done) to a single, large, shared experience replay buffer.\n3.  **Centralized Policy Learner:** A single \"main\" policy $\\pi_{main}$ is trained by sampling uniformly from this shared replay buffer. This policy benefits from the combined diverse exploration of all agents.\n4.  **Meta-Learning/Evolutionary Strategy Selection:** Periodically (e.g., every K episodes or after a certain number of steps):\n    *   Evaluate the \"performance\" of each agent's exploration strategy. Performance could be measured by the novelty of states visited, the diversity of transitions added to the buffer, or even by a proxy for how much the main policy improved thanks to the data contributed by that agent.\n    *   \"Bad\" exploration strategies are replaced by \"mutated\" versions of \"good\" strategies (e.g., copying the exploration parameters of high-performing agents and adding small perturbations). This creates an evolutionary pressure, favoring strategies that efficiently discover valuable information for the main policy.\n    *   This ensures continuous generation and refinement of effective exploration approaches, without being stuck with a single hand-designed method.\n\n---\n\n### Evaluation\n\n**Branch 1: Information Gain with Predictive Models**\n\n*   **Feasibility:** Moderate. Requires training a robust predictive model alongside the main policy. Estimating uncertainty reliably and computing information gain (or good approximations) can be computationally intensive, often needing ensembles or specialized architectures. Existing frameworks (e.g., RND) are simpler variants, but full information gain is harder.\n*   **Novelty:** Medium. Uncertainty-based and novelty-based intrinsic motivation are well-established. The emphasis here is on the more sophisticated \"information gain\" metric using predictive models rather than simple visit counts or prediction error, pushing towards active learning in RL.\n*   **Potential Performance Benefits:** High. Can be very effective in environments with sparse rewards, as it directly incentivizes learning about the environment. More robust than purely count-based methods in high-dimensional state spaces. Helps avoid getting stuck in locally optimal but poorly understood regions.\n\n**Branch 2: Hierarchical Goal-Setting with Active Goal Generation and Hindsight**\n\n*   **Feasibility:** High. HER is a widely adopted and proven technique. Hierarchical RL is a known paradigm. The \"active goal generation\" adds complexity but builds on existing research in goal-conditioned RL and intrinsic motivation. It requires careful design of goal representation and the goal-generation mechanism.\n*   **Novelty:** Medium-High. While individual components exist, the combination of *active, intelligent goal generation* (beyond just random sampling or fixed goals) within a hierarchical structure, coupled with the efficiency of HER specifically for *exploration*, is a powerful and less common blend. It represents a proactive, goal-oriented approach to exploring.\n*   **Potential Performance Benefits:** Very High. Excellent for tasks requiring long horizons or complex sequences of actions, and particularly effective in sparse reward settings by breaking down hard problems into manageable sub-goals. HER makes every \"failure\" a learning opportunity. Active goal generation can effectively guide exploration towards challenging and informative regions of the state space, leading to faster and more robust learning.\n\n**Branch 3: Decentralized Diverse-Strategy Exploration (DDSE)**\n\n*   **Feasibility:** Moderate. Requires managing a population of agents, ensuring diverse exploration strategies, and implementing a meta-learning or evolutionary loop to select and propagate effective strategies. This adds overhead in terms of computation and system design compared to a single agent.\n*   **Novelty:** High. While multi-agent RL and evolutionary algorithms exist, this approach specifically focuses on the *discovery and evolution of exploration strategies themselves* within a decentralized, shared-experience framework. It's distinct from simple ensemble exploration or multi-agent learning for cooperative tasks, offering a meta-learning approach to exploration.\n*   **Potential Performance Benefits:** High. Can discover novel and robust exploration strategies that a single hand-designed approach might miss. Good for highly complex or multimodal environments where different \"types\" of exploration might be needed in different regions. Robust to getting stuck in local optima because of population diversity. Can scale well if agents can explore in parallel.\n\n---\n\n### Final Chosen Solution\n\nThe best strategy is **Branch 2: Hierarchical Goal-Setting with Active Goal Generation and Hindsight**.\n\n**Justification:**\n\nThis strategy offers the most compelling balance of feasibility, novelty, and high potential performance benefits for tackling challenging reinforcement learning problems, especially those with **sparse rewards** and **long-horizon objectives**.\n\n1.  **Directly Addresses Hard Problems:** Sparse rewards and long horizons are two of the biggest hurdles in RL. Hierarchy breaks down the problem, and active goal generation specifically targets informative intermediate states. HER then ensures that every interaction, successful or not, contributes significantly to learning, making the most of limited data.\n2.  **Intelligent and Proactive Exploration:** Unlike passive curiosity or count-based methods, this approach is more intelligent. The high-level policy *actively generates* goals that are challenging yet within reach, guiding exploration more efficiently than random noise or simple novelty.\n3.  **High Learning Efficiency:** HER is incredibly powerful for learning from sparse rewards by re-contextualizing failed attempts. When combined with a hierarchical structure that sets sub-goals, it creates a highly efficient learning loop for both primitive actions and higher-level planning.\n4.  **Strong Foundation:** It builds upon well-established techniques (HRL, HER) while adding a crucial layer of active, intelligent goal generation, making it practical to implement and highly impactful.\n\nWhile Branch 3 is highly novel in its meta-learning approach to exploration, it might be more complex to implement and fine-tune for general problems, requiring robust mechanisms for strategy evaluation and evolution. Branch 1 is solid but may lack the structural benefits of hierarchy and the data efficiency of hindsight for truly long-horizon tasks. Branch 2 offers a robust and intelligent path forward for improved exploration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. ReAct (Reason + Act)"
      ],
      "metadata": {
        "id": "3_kti78uepEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_without_react = \"\"\"\n",
        "What is QLoRA in large language model fine-tuning, and why is it useful?\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without ReAct (Reason + Act) prompting:\", prompt_without_react)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DJhx2_83e7Hr",
        "outputId": "b6eb2275-87f3-4e2c-bb69-25a47b7f9e09"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without ReAct (Reason + Act) prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "What is QLoRA in large language model fine-tuning, and why is it useful?\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "QLoRA (Quantized Low-Rank Adaptation) is a highly efficient fine-tuning technique for large language models (LLMs) that significantly reduces the memory requirements while maintaining performance close to full fine-tuning. It builds upon the LoRA (Low-Rank Adaptation) method by adding quantization.\n\nTo understand QLoRA, let's break down its components:\n\n### 1. Understanding LoRA (Low-Rank Adaptation)\n\nBefore QLoRA, we need to understand LoRA, which addresses the high computational cost and memory footprint of fine-tuning large models.\n\n*   **The Problem:** Full fine-tuning of an LLM involves updating all millions or billions of parameters of the pre-trained model. This requires immense GPU memory (to store model weights, gradients, and optimizer states) and computational power.\n*   **LoRA's Solution:** Instead of updating all parameters, LoRA injects small, trainable \"adapter\" matrices into specific layers of the pre-trained model (typically the linear projection layers in the attention mechanism).\n    *   For a pre-trained weight matrix $W_0$, LoRA adds a low-rank decomposition of a weight update $\\Delta W = BA$, where $A$ is a $d \\times r$ matrix and $B$ is an $r \\times k$ matrix.\n    *   The \"rank\" $r$ is much smaller than the dimensions $d$ or $k$.\n    *   During fine-tuning, the original pre-trained weights $W_0$ are frozen, and *only* the small adapter matrices $A$ and $B$ are trained.\n*   **Benefits of LoRA:**\n    *   **Significantly fewer trainable parameters:** Instead of training billions of parameters, you train only a few million (or even less).\n    *   **Reduced memory footprint:** Less memory needed for gradients and optimizer states.\n    *   **Faster training:** Fewer parameters to update.\n    *   **Modular adapters:** Different tasks can have their own small LoRA adapters applied to the same frozen base model.\n\n### 2. What is QLoRA?\n\nQLoRA takes LoRA a step further by addressing the memory footprint of the *frozen base model* itself.\n\nQLoRA builds on LoRA with three key innovations:\n\n1.  **4-bit NormalFloat (NF4) Quantization:** QLoRA quantizes the entire pre-trained LLM to 4-bit precision (specifically, a new data type called 4-bit NormalFloat). This drastically reduces the memory needed to store the large, frozen base model weights.\n    *   Standard quantization often uses 8-bit or lower, but NF4 is specifically designed to be optimal for weights that follow a normal distribution, which is common in neural networks.\n2.  **Double Quantization:** To save even more memory, QLoRA quantizes the *quantization constants* themselves (the scales and offsets used in the initial 4-bit quantization). While this saves only a small amount of memory per constant, it adds up over billions of parameters.\n3.  **Paged Optimizers:** This feature helps manage memory spikes during training by offloading optimizer states to CPU RAM when GPU memory runs low, similar to how operating systems handle virtual memory paging. This prevents out-of-memory errors that can occur during the forward/backward pass with large batches or longer sequences.\n\n**How QLoRA Works in Practice:**\n\n1.  The large pre-trained LLM is loaded and its weights are quantized to 4-bit (using NF4 and double quantization). These quantized weights are then frozen.\n2.  LoRA adapters are added to the desired layers of this *quantized* base model.\n3.  During training, only the parameters of these small LoRA adapters are updated. Crucially, these adapters are typically trained in higher precision (e.g., 16-bit BF16 or FP16) to preserve training stability and performance.\n4.  For computations, the 4-bit base model weights are dequantized *on the fly* to a higher precision (e.g., 16-bit) just before being used in matrix multiplications, and then the LoRA delta is applied.\n5.  After fine-tuning, only the small LoRA adapters (and optionally the quantization metadata) need to be saved, allowing for very small fine-tuned models.\n\n### Why is QLoRA Useful?\n\nQLoRA offers significant advantages, primarily related to accessibility and efficiency:\n\n1.  **Massive Memory Reduction:** This is the most critical benefit. QLoRA allows fine-tuning LLMs with billions of parameters (e.g., 65B models) on a single GPU (like an 80GB A100), or even smaller models (e.g., 7B, 13B) on consumer-grade GPUs with 24GB or 12GB of VRAM.\n2.  **Democratization of LLM Fine-tuning:** By drastically lowering the hardware barrier, QLoRA enables researchers, hobbyists, and smaller companies to experiment with and fine-tune powerful LLMs that would otherwise require prohibitively expensive multi-GPU setups or cloud instances.\n3.  **Cost-Effectiveness:** Less powerful hardware requirements translate directly to lower financial costs for fine-tuning.\n4.  **Near State-of-the-Art Performance:** Despite the aggressive 4-bit quantization of the base model, QLoRA has been shown to achieve performance comparable to, and sometimes even surpass, full 16-bit fine-tuning on various benchmarks. This indicates that the information loss from quantization is minimal, especially when combined with the higher-precision LoRA adapters.\n5.  **Faster Iteration:** While the training process itself isn't necessarily faster than standard LoRA (which is already fast), the ability to fit much larger models onto available hardware allows for quicker experimentation and iteration on models that would otherwise be impossible to fine-tune within reasonable timeframes or budget.\n\nIn summary, QLoRA is a groundbreaking technique that makes fine-tuning large language models much more accessible and efficient by combining 4-bit quantization of the base model with the parameter-efficient LoRA adapters, all while maintaining excellent performance."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_react = \"\"\"\n",
        "You are an AI assistant using a ReAct-style reasoning process.\n",
        "\n",
        "For the question below, follow this loop:\n",
        "- Thought: explain what you are thinking.\n",
        "- Action: one of [RECALL, CHECK_KNOWLEDGE, STOP]\n",
        "- Observation: what you concluded from that action.\n",
        "\n",
        "Repeat the Thought/Action/Observation cycle 2‚Äì3 times,\n",
        "then produce a final short answer.\n",
        "\n",
        "Question: What is QLoRA in large language model fine-tuning, and why is it useful?\n",
        "\n",
        "Format your answer exactly like this:\n",
        "\n",
        "Thought 1:\n",
        "Action 1:\n",
        "Observation 1:\n",
        "\n",
        "Thought 2:\n",
        "Action 2:\n",
        "Observation 2:\n",
        "\n",
        "Final answer:\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With ReAct (Reason + Act) prompting:\", prompt_with_react)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "mfKM7mwhfCe2",
        "outputId": "b1018ca6-a540-423a-905c-47d739e28723"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With ReAct (Reason + Act) prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "You are an AI assistant using a ReAct-style reasoning process.\n",
            "\n",
            "For the question below, follow this loop:\n",
            "- Thought: explain what you are thinking.\n",
            "- Action: one of [RECALL, CHECK_KNOWLEDGE, STOP]\n",
            "- Observation: what you concluded from that action.\n",
            "\n",
            "Repeat the Thought/Action/Observation cycle 2‚Äì3 times,\n",
            "then produce a final short answer.\n",
            "\n",
            "Question: What is QLoRA in large language model fine-tuning, and why is it useful?\n",
            "\n",
            "Format your answer exactly like this:\n",
            "\n",
            "Thought 1:\n",
            "Action 1:\n",
            "Observation 1:\n",
            "\n",
            "Thought 2:\n",
            "Action 2:\n",
            "Observation 2:\n",
            "\n",
            "Final answer:\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Thought 1: The user is asking for an explanation of QLoRA and its utility in large language model fine-tuning. This is a concept I should be able to recall directly from my training data regarding LLM optimization techniques. I will start by defining what QLoRA is.\n\nAction 1: RECALL\n\nObservation 1: QLoRA (Quantized Low-Rank Adapters) is a memory-efficient fine-tuning approach that combines quantization with LoRA (Low-Rank Adaptation). It quantizes a pre-trained large language model (LLM) to 4-bit precision (e.g., NF4) and then freezes these quantized weights. During fine-tuning, only a small set of trainable LoRA adapters are added to the model, and only the weights of these adapters are updated. This means the 4-bit quantized base model weights are used for computation but not updated, significantly reducing memory footprint while allowing full 16-bit precision for the LoRA adapter weights and computations.\n\nThought 2: Now that I've defined what QLoRA is, I need to explain why it is useful. The key benefit stems from its memory efficiency and ability to fine-tune very large models on consumer-grade GPUs.\n\nAction 2: RECALL\n\nObservation 2: QLoRA is extremely useful because it drastically reduces the memory required to fine-tune large language models. This allows models with billions of parameters (e.g., 65B or even 100B+) to be fine-tuned on single consumer GPUs or smaller clusters, which would otherwise require massive computational resources and specialized hardware. By using 4-bit quantization for the frozen base model and only training a small fraction of parameters (the LoRA adapters), it makes state-of-the-art LLM fine-tuning accessible to a much broader range of researchers and practitioners, democratizing access to powerful customization capabilities for LLMs. It achieves this significant memory saving with minimal to no performance degradation compared to full 16-bit fine-tuning.\n\nFinal answer: QLoRA (Quantized Low-Rank Adapters) is a memory-efficient fine-tuning method for large language models (LLMs). It quantizes the pre-trained LLM weights to 4-bit precision (e.g., NF4) and freezes them. During fine-tuning, only a small number of LoRA (Low-Rank Adaptation) adapter weights are added and trained, allowing the 4-bit base model to be used for computation while only updating the small, 16-bit LoRA adapters. It is useful because it dramatically reduces the memory footprint required for fine-tuning, enabling individuals and smaller organizations to fine-tune very large LLMs (e.g., 65B parameters) on consumer-grade GPUs, thus democratizing access to advanced LLM customization with minimal performance loss."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Reflexion Prompting"
      ],
      "metadata": {
        "id": "S727NGFcfHE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_without_reflexion = \"\"\"\n",
        "Explain what overfitting is in machine learning.\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"Without Reflexion (Reason + Act) prompting:\", prompt_without_reflexion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_ErcaG1fgFn",
        "outputId": "3ee7b3b5-cac1-46ba-de84-9e1ce24a682f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: Without Reflexion (Reason + Act) prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "Explain what overfitting is in machine learning.\n",
            "\n",
            "\n",
            "‚ùå Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_reflexion = \"\"\"\n",
        "You will follow a three-stage Reflexion process: Answer ‚Üí Critique ‚Üí Refine.\n",
        "\n",
        "Your task topic: Explain **what overfitting is in machine learning**.\n",
        "\n",
        "Stage 1 ‚Äî Answer:\n",
        "Give your best explanation of overfitting to a beginner who knows nothing about ML.\n",
        "Keep it simple (3‚Äì5 sentences).\n",
        "\n",
        "Stage 2 ‚Äî Critique:\n",
        "Critically analyze your own explanation.\n",
        "Identify at least:\n",
        "- one missing detail,\n",
        "- one unclear or potentially misleading point.\n",
        "\n",
        "Stage 3 ‚Äî Refine:\n",
        "Rewrite a clearer and more accurate explanation,\n",
        "fixing the issues you identified. Keep it under 5‚Äì6 sentences.\n",
        "\n",
        "Format strictly as:\n",
        "\n",
        "Answer:\n",
        "[your initial answer]\n",
        "\n",
        "Critique:\n",
        "[your self-critique]\n",
        "\n",
        "Refined answer:\n",
        "[your improved explanation]\n",
        "\"\"\"\n",
        "\n",
        "run_prompt(\"With Reflexion (Reason + Act) prompting:\", prompt_with_reflexion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "4zNIlY4Xfs1m",
        "outputId": "7b7ec7f9-cf90-4e4a-e11d-d37f14760b16"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üß™ TESTING: With Reflexion (Reason + Act) prompting: ---\n",
            "üì• INPUT:\n",
            "\n",
            "You will follow a three-stage Reflexion process: Answer ‚Üí Critique ‚Üí Refine.\n",
            "\n",
            "Your task topic: Explain **what overfitting is in machine learning**.\n",
            "\n",
            "Stage 1 ‚Äî Answer:\n",
            "Give your best explanation of overfitting to a beginner who knows nothing about ML.\n",
            "Keep it simple (3‚Äì5 sentences).\n",
            "\n",
            "Stage 2 ‚Äî Critique:\n",
            "Critically analyze your own explanation.\n",
            "Identify at least:\n",
            "- one missing detail,\n",
            "- one unclear or potentially misleading point.\n",
            "\n",
            "Stage 3 ‚Äî Refine:\n",
            "Rewrite a clearer and more accurate explanation,\n",
            "fixing the issues you identified. Keep it under 5‚Äì6 sentences.\n",
            "\n",
            "Format strictly as:\n",
            "\n",
            "Answer:\n",
            "[your initial answer]\n",
            "\n",
            "Critique:\n",
            "[your self-critique]\n",
            "\n",
            "Refined answer:\n",
            "[your improved explanation]\n",
            "\n",
            "\n",
            "out üì§ OUTPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Answer:\nImagine you're studying for a test by memorizing every single question and answer from a practice exam, instead of understanding the concepts. You'd ace *that exact* practice exam, but struggle on a new test with slightly different questions because you only learned specifics, not general rules. Overfitting in machine learning is similar: a model learns the training data (like the practice exam) too perfectly, including any random noise. This makes it perform poorly on new, real-world data because it can't apply its knowledge generally.\n\nCritique:\n- **Missing detail:** The explanation implicitly states that doing poorly on new data is bad, but doesn't explicitly mention that the *goal* of a machine learning model is specifically to perform well on *unseen* data, not just the training data.\n- **Unclear/potentially misleading point:** The phrase \"including any random noise\" might be a bit abstract for a beginner. They might not understand what \"noise\" means in this context and how a model \"learns\" it. It could be clarified as learning specific, irrelevant details or quirks of the training data.\n\nRefined answer:\nImagine you're studying for a test by memorizing every single question and answer from a practice exam, instead of understanding the core concepts. You'd ace *that specific* practice exam, but struggle on a new test with slightly different questions. Overfitting in machine learning is similar: a model learns the training data too perfectly, including its unique quirks or random details, rather than the true underlying patterns. This means it fails to generalize; it performs well on the data it was trained on, but poorly on new, real-world data. The main goal of such models is to predict *new* data accurately, not just memorize old data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whBuyXCffyNz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}